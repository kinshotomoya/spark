Spark Executor Command: "/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/bin/java" "-cp" "/Users/kinsho/workspace/spark-3.0.1/conf/:/Users/kinsho/workspace/spark-3.0.1/assembly/target/scala-2.12/jars/*" "-Xmx1024M" "-Dspark.driver.port=51030" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.11.7:51030" "--executor-id" "9" "--hostname" "192.168.11.7" "--cores" "2" "--app-id" "app-20210131134107-0030" "--worker-url" "spark://Worker@192.168.11.7:63954"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/31 13:41:09 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 27473@ST000000035
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for TERM
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for HUP
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for INT
21/01/31 13:41:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/01/31 13:41:10 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 135 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO DiskBlockManager: Created local directory at /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-3ab0568a-ea0f-4ede-9044-9b9b119a7a74/executor-e9e0037b-0efc-4e3f-91ff-bd6cf9862b77/blockmgr-0b052284-807d-45f7-948c-570a3deb47b6
21/01/31 13:41:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/01/31 13:41:11 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.11.7:51030
21/01/31 13:41:11 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.11.7:63954
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:63954 after 4 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO ResourceUtils: ==============================================================
21/01/31 13:41:11 INFO ResourceUtils: Resources for spark.executor:

21/01/31 13:41:11 INFO ResourceUtils: ==============================================================
21/01/31 13:41:11 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
21/01/31 13:41:11 INFO Executor: Starting executor ID 9 on host 192.168.11.7
21/01/31 13:41:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51089.
21/01/31 13:41:12 INFO NettyBlockTransferService: Server created on 192.168.11.7:51089
21/01/31 13:41:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/31 13:41:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(9, 192.168.11.7, 51089, None)
21/01/31 13:41:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(9, 192.168.11.7, 51089, None)
21/01/31 13:41:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(9, 192.168.11.7, 51089, None)
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 7
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 17
21/01/31 13:41:16 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
21/01/31 13:41:16 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
21/01/31 13:41:16 INFO Executor: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar with timestamp 1612068067341
21/01/31 13:41:16 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 4 ms (0 ms spent in bootstraps)
21/01/31 13:41:16 INFO Utils: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar to /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-3ab0568a-ea0f-4ede-9044-9b9b119a7a74/executor-e9e0037b-0efc-4e3f-91ff-bd6cf9862b77/spark-389d66dc-f54d-4f36-aaa4-b5c20f562048/fetchFileTemp9027455158982661901.tmp
21/01/31 13:41:16 INFO Utils: Copying /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-3ab0568a-ea0f-4ede-9044-9b9b119a7a74/executor-e9e0037b-0efc-4e3f-91ff-bd6cf9862b77/spark-389d66dc-f54d-4f36-aaa4-b5c20f562048/16839229251612068067341_cache to /Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/9/./simple-project_2.12-1.0.jar
21/01/31 13:41:16 INFO Executor: Adding file:/Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/9/./simple-project_2.12-1.0.jar to class loader
21/01/31 13:41:16 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:17 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51036 after 45 ms (0 ms spent in bootstraps)
21/01/31 13:41:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 366.3 MiB)
21/01/31 13:41:17 INFO TorrentBroadcast: Reading broadcast variable 4 took 389 ms
21/01/31 13:41:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 43.3 KiB, free 366.2 MiB)
21/01/31 13:41:19 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 2147483648-2281701376, partition values: [empty row]
21/01/31 13:41:19 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 805306368-939524096, partition values: [empty row]
21/01/31 13:41:21 INFO CodeGenerator: Code generated in 994.634291 ms
21/01/31 13:41:21 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:21 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51085 after 3 ms (0 ms spent in bootstraps)
21/01/31 13:41:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.2 MiB)
21/01/31 13:41:21 INFO TorrentBroadcast: Reading broadcast variable 3 took 19 ms
21/01/31 13:41:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 336.8 KiB, free 365.9 MiB)
21/01/31 13:41:50 INFO MemoryStore: Will not store rdd_12_6
21/01/31 13:41:50 WARN MemoryStore: Not enough space to cache rdd_12_6 in memory! (computed 138.0 MiB so far)
21/01/31 13:41:50 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 306.7 MiB (scratch space shared across 2 tasks(s)) = 307.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:41:50 WARN BlockManager: Persisting block rdd_12_6 to disk instead.
21/01/31 13:41:57 INFO MemoryStore: Will not store rdd_12_6
21/01/31 13:41:57 WARN MemoryStore: Not enough space to cache rdd_12_6 in memory! (computed 138.0 MiB so far)
21/01/31 13:41:57 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 306.7 MiB (scratch space shared across 2 tasks(s)) = 307.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:41:57 INFO CodeGenerator: Code generated in 10.93214 ms
21/01/31 13:41:57 INFO CodeGenerator: Code generated in 51.26468 ms
21/01/31 13:41:57 INFO CodeGenerator: Code generated in 40.091707 ms
21/01/31 13:41:57 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2108 bytes result sent to driver
21/01/31 13:41:57 INFO CoarseGrainedExecutorBackend: Got assigned task 28
21/01/31 13:41:57 INFO Executor: Running task 27.0 in stage 1.0 (TID 28)
21/01/31 13:41:57 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 3623878656-3758096384, partition values: [empty row]
21/01/31 13:42:01 INFO MemoryStore: Block rdd_12_16 stored as values in memory (estimated size 187.4 MiB, free 178.5 MiB)
21/01/31 13:42:01 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2108 bytes result sent to driver
21/01/31 13:42:01 INFO CoarseGrainedExecutorBackend: Got assigned task 38
21/01/31 13:42:01 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
21/01/31 13:42:01 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 4966055936-5100273664, partition values: [empty row]
21/01/31 13:42:14 INFO MemoryStore: Will not store rdd_12_37
21/01/31 13:42:14 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 69.7 MiB so far)
21/01/31 13:42:14 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 157.1 MiB (scratch space shared across 2 tasks(s)) = 344.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:14 WARN BlockManager: Persisting block rdd_12_37 to disk instead.
21/01/31 13:42:23 INFO MemoryStore: Will not store rdd_12_27
21/01/31 13:42:23 WARN MemoryStore: Not enough space to cache rdd_12_27 in memory! (computed 136.2 MiB so far)
21/01/31 13:42:23 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 103.3 MiB (scratch space shared across 1 tasks(s)) = 291.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:23 WARN BlockManager: Persisting block rdd_12_27 to disk instead.
21/01/31 13:42:33 INFO MemoryStore: Will not store rdd_12_27
21/01/31 13:42:33 WARN MemoryStore: Not enough space to cache rdd_12_27 in memory! (computed 136.2 MiB so far)
21/01/31 13:42:33 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 103.3 MiB (scratch space shared across 1 tasks(s)) = 291.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:33 INFO Executor: Finished task 27.0 in stage 1.0 (TID 28). 2108 bytes result sent to driver
21/01/31 13:42:33 INFO CoarseGrainedExecutorBackend: Got assigned task 49
21/01/31 13:42:33 INFO Executor: Running task 48.0 in stage 1.0 (TID 49)
21/01/31 13:42:33 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 6442450944-6576668672, partition values: [empty row]
21/01/31 13:42:35 INFO MemoryStore: Will not store rdd_12_37
21/01/31 13:42:35 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 135.8 MiB so far)
21/01/31 13:42:35 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 107.7 MiB (scratch space shared across 2 tasks(s)) = 295.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:35 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2108 bytes result sent to driver
21/01/31 13:42:35 INFO CoarseGrainedExecutorBackend: Got assigned task 57
21/01/31 13:42:35 INFO Executor: Running task 56.0 in stage 1.0 (TID 57)
21/01/31 13:42:35 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 7516192768-7650410496, partition values: [empty row]
21/01/31 13:42:48 INFO MemoryStore: Will not store rdd_12_56
21/01/31 13:42:48 WARN MemoryStore: Not enough space to cache rdd_12_56 in memory! (computed 69.6 MiB so far)
21/01/31 13:42:48 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 158.4 MiB (scratch space shared across 2 tasks(s)) = 346.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:48 WARN BlockManager: Persisting block rdd_12_56 to disk instead.
21/01/31 13:42:58 INFO MemoryStore: Will not store rdd_12_48
21/01/31 13:42:58 WARN MemoryStore: Not enough space to cache rdd_12_48 in memory! (computed 136.4 MiB so far)
21/01/31 13:42:58 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 104.3 MiB (scratch space shared across 1 tasks(s)) = 292.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:58 WARN BlockManager: Persisting block rdd_12_48 to disk instead.
21/01/31 13:43:05 INFO MemoryStore: Will not store rdd_12_48
21/01/31 13:43:05 WARN MemoryStore: Not enough space to cache rdd_12_48 in memory! (computed 136.4 MiB so far)
21/01/31 13:43:05 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 104.3 MiB (scratch space shared across 1 tasks(s)) = 292.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:05 INFO Executor: Finished task 48.0 in stage 1.0 (TID 49). 2108 bytes result sent to driver
21/01/31 13:43:05 INFO CoarseGrainedExecutorBackend: Got assigned task 72
21/01/31 13:43:05 INFO Executor: Running task 71.0 in stage 1.0 (TID 72)
21/01/31 13:43:05 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 9529458688-9663676416, partition values: [empty row]
21/01/31 13:43:07 INFO MemoryStore: Will not store rdd_12_56
21/01/31 13:43:07 WARN MemoryStore: Not enough space to cache rdd_12_56 in memory! (computed 135.9 MiB so far)
21/01/31 13:43:07 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 107.5 MiB (scratch space shared across 2 tasks(s)) = 295.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:07 INFO Executor: Finished task 56.0 in stage 1.0 (TID 57). 2108 bytes result sent to driver
21/01/31 13:43:07 INFO CoarseGrainedExecutorBackend: Got assigned task 79
21/01/31 13:43:07 INFO Executor: Running task 78.0 in stage 1.0 (TID 79)
21/01/31 13:43:07 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 10468982784-10603200512, partition values: [empty row]
21/01/31 13:43:20 INFO MemoryStore: Will not store rdd_12_78
21/01/31 13:43:20 WARN MemoryStore: Not enough space to cache rdd_12_78 in memory! (computed 68.9 MiB so far)
21/01/31 13:43:20 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 157.7 MiB (scratch space shared across 2 tasks(s)) = 345.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:20 WARN BlockManager: Persisting block rdd_12_78 to disk instead.
21/01/31 13:43:31 INFO MemoryStore: Will not store rdd_12_71
21/01/31 13:43:31 WARN MemoryStore: Not enough space to cache rdd_12_71 in memory! (computed 136.0 MiB so far)
21/01/31 13:43:31 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 104.0 MiB (scratch space shared across 1 tasks(s)) = 291.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:31 WARN BlockManager: Persisting block rdd_12_71 to disk instead.
21/01/31 13:43:37 INFO MemoryStore: Will not store rdd_12_71
21/01/31 13:43:37 WARN MemoryStore: Not enough space to cache rdd_12_71 in memory! (computed 136.0 MiB so far)
21/01/31 13:43:37 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 104.0 MiB (scratch space shared across 1 tasks(s)) = 291.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:37 INFO Executor: Finished task 71.0 in stage 1.0 (TID 72). 2108 bytes result sent to driver
21/01/31 13:43:37 INFO CoarseGrainedExecutorBackend: Got assigned task 94
21/01/31 13:43:37 INFO Executor: Running task 93.0 in stage 1.0 (TID 94)
21/01/31 13:43:37 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 12482248704-12616466432, partition values: [empty row]
21/01/31 13:43:40 INFO MemoryStore: Will not store rdd_12_78
21/01/31 13:43:40 WARN MemoryStore: Not enough space to cache rdd_12_78 in memory! (computed 135.5 MiB so far)
21/01/31 13:43:40 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 106.5 MiB (scratch space shared across 2 tasks(s)) = 294.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:40 INFO Executor: Finished task 78.0 in stage 1.0 (TID 79). 2108 bytes result sent to driver
21/01/31 13:43:40 INFO CoarseGrainedExecutorBackend: Got assigned task 99
21/01/31 13:43:40 INFO Executor: Running task 98.0 in stage 1.0 (TID 99)
21/01/31 13:43:40 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 13153337344-13287555072, partition values: [empty row]
21/01/31 13:43:53 INFO MemoryStore: Will not store rdd_12_98
21/01/31 13:43:53 WARN MemoryStore: Not enough space to cache rdd_12_98 in memory! (computed 66.0 MiB so far)
21/01/31 13:43:53 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 154.8 MiB (scratch space shared across 2 tasks(s)) = 342.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:53 WARN BlockManager: Persisting block rdd_12_98 to disk instead.
21/01/31 13:44:02 INFO MemoryStore: Will not store rdd_12_93
21/01/31 13:44:02 WARN MemoryStore: Not enough space to cache rdd_12_93 in memory! (computed 132.6 MiB so far)
21/01/31 13:44:02 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 102.5 MiB (scratch space shared across 1 tasks(s)) = 290.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:02 WARN BlockManager: Persisting block rdd_12_93 to disk instead.
21/01/31 13:44:08 INFO MemoryStore: Will not store rdd_12_93
21/01/31 13:44:08 WARN MemoryStore: Not enough space to cache rdd_12_93 in memory! (computed 132.6 MiB so far)
21/01/31 13:44:08 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 102.5 MiB (scratch space shared across 1 tasks(s)) = 290.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:08 INFO Executor: Finished task 93.0 in stage 1.0 (TID 94). 2108 bytes result sent to driver
21/01/31 13:44:08 INFO CoarseGrainedExecutorBackend: Got assigned task 114
21/01/31 13:44:08 INFO Executor: Running task 113.0 in stage 1.0 (TID 114)
21/01/31 13:44:08 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 15166603264-15300820992, partition values: [empty row]
21/01/31 13:44:12 INFO MemoryStore: Block rdd_12_98 stored as values in memory (estimated size 155.0 MiB, free 23.5 MiB)
21/01/31 13:44:12 INFO Executor: Finished task 98.0 in stage 1.0 (TID 99). 2108 bytes result sent to driver
21/01/31 13:44:12 INFO CoarseGrainedExecutorBackend: Got assigned task 119
21/01/31 13:44:12 INFO Executor: Running task 118.0 in stage 1.0 (TID 119)
21/01/31 13:44:12 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 15837691904-15971909632, partition values: [empty row]
21/01/31 13:44:15 INFO MemoryStore: Will not store rdd_12_113
21/01/31 13:44:15 WARN MemoryStore: Not enough space to cache rdd_12_113 in memory! (computed 36.0 MiB so far)
21/01/31 13:44:15 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 349.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:15 WARN BlockManager: Persisting block rdd_12_113 to disk instead.
21/01/31 13:44:18 INFO MemoryStore: Will not store rdd_12_118
21/01/31 13:44:18 WARN MemoryStore: Not enough space to cache rdd_12_118 in memory! (computed 36.4 MiB so far)
21/01/31 13:44:18 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 346.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:18 WARN BlockManager: Persisting block rdd_12_118 to disk instead.
21/01/31 13:44:40 INFO MemoryStore: Will not store rdd_12_113
21/01/31 13:44:40 WARN MemoryStore: Not enough space to cache rdd_12_113 in memory! (computed 36.0 MiB so far)
21/01/31 13:44:40 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 346.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:40 INFO Executor: Finished task 113.0 in stage 1.0 (TID 114). 2108 bytes result sent to driver
21/01/31 13:44:40 INFO CoarseGrainedExecutorBackend: Got assigned task 135
21/01/31 13:44:40 INFO Executor: Running task 134.0 in stage 1.0 (TID 135)
21/01/31 13:44:40 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 17985175552-18119393280, partition values: [empty row]
21/01/31 13:44:42 INFO MemoryStore: Will not store rdd_12_118
21/01/31 13:44:42 WARN MemoryStore: Not enough space to cache rdd_12_118 in memory! (computed 36.4 MiB so far)
21/01/31 13:44:42 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 349.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:43 INFO Executor: Finished task 118.0 in stage 1.0 (TID 119). 2108 bytes result sent to driver
21/01/31 13:44:43 INFO CoarseGrainedExecutorBackend: Got assigned task 138
21/01/31 13:44:43 INFO Executor: Running task 137.0 in stage 1.0 (TID 138)
21/01/31 13:44:43 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 18387828736-18522046464, partition values: [empty row]
21/01/31 13:44:47 INFO MemoryStore: Will not store rdd_12_134
21/01/31 13:44:47 WARN MemoryStore: Not enough space to cache rdd_12_134 in memory! (computed 35.2 MiB so far)
21/01/31 13:44:47 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 349.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:47 WARN BlockManager: Persisting block rdd_12_134 to disk instead.
21/01/31 13:44:49 INFO MemoryStore: Will not store rdd_12_137
21/01/31 13:44:49 WARN MemoryStore: Not enough space to cache rdd_12_137 in memory! (computed 36.5 MiB so far)
21/01/31 13:44:49 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 345.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:49 WARN BlockManager: Persisting block rdd_12_137 to disk instead.
21/01/31 13:45:12 INFO MemoryStore: Will not store rdd_12_134
21/01/31 13:45:12 WARN MemoryStore: Not enough space to cache rdd_12_134 in memory! (computed 35.2 MiB so far)
21/01/31 13:45:12 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.0 MiB (scratch space shared across 1 tasks(s)) = 345.8 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:12 INFO Executor: Finished task 134.0 in stage 1.0 (TID 135). 2108 bytes result sent to driver
21/01/31 13:45:12 INFO CoarseGrainedExecutorBackend: Got assigned task 156
21/01/31 13:45:12 INFO Executor: Running task 155.0 in stage 1.0 (TID 156)
21/01/31 13:45:12 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 20803747840-20937965568, partition values: [empty row]
21/01/31 13:45:14 INFO MemoryStore: Will not store rdd_12_137
21/01/31 13:45:14 WARN MemoryStore: Not enough space to cache rdd_12_137 in memory! (computed 36.5 MiB so far)
21/01/31 13:45:14 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 349.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:14 INFO Executor: Finished task 137.0 in stage 1.0 (TID 138). 2108 bytes result sent to driver
21/01/31 13:45:14 INFO CoarseGrainedExecutorBackend: Got assigned task 158
21/01/31 13:45:14 INFO Executor: Running task 157.0 in stage 1.0 (TID 158)
21/01/31 13:45:14 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 21072183296-21206401024, partition values: [empty row]
21/01/31 13:45:19 INFO MemoryStore: Will not store rdd_12_155
21/01/31 13:45:19 WARN MemoryStore: Not enough space to cache rdd_12_155 in memory! (computed 36.7 MiB so far)
21/01/31 13:45:19 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 349.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:19 WARN BlockManager: Persisting block rdd_12_155 to disk instead.
21/01/31 13:45:21 INFO MemoryStore: Will not store rdd_12_157
21/01/31 13:45:21 WARN MemoryStore: Not enough space to cache rdd_12_157 in memory! (computed 37.3 MiB so far)
21/01/31 13:45:21 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 346.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:21 WARN BlockManager: Persisting block rdd_12_157 to disk instead.
21/01/31 13:45:42 INFO MemoryStore: Will not store rdd_12_155
21/01/31 13:45:42 WARN MemoryStore: Not enough space to cache rdd_12_155 in memory! (computed 36.7 MiB so far)
21/01/31 13:45:42 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 346.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:42 INFO Executor: Finished task 155.0 in stage 1.0 (TID 156). 2108 bytes result sent to driver
21/01/31 13:45:42 INFO CoarseGrainedExecutorBackend: Got assigned task 176
21/01/31 13:45:42 INFO Executor: Running task 175.0 in stage 1.0 (TID 176)
21/01/31 13:45:42 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 23488102400-23517392614, partition values: [empty row]
21/01/31 13:45:44 INFO MemoryStore: Will not store rdd_12_157
21/01/31 13:45:44 WARN MemoryStore: Not enough space to cache rdd_12_157 in memory! (computed 37.3 MiB so far)
21/01/31 13:45:44 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 349.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:44 INFO Executor: Finished task 157.0 in stage 1.0 (TID 158). 2108 bytes result sent to driver
21/01/31 13:45:47 INFO MemoryStore: Will not store rdd_12_175
21/01/31 13:45:47 WARN MemoryStore: Not enough space to cache rdd_12_175 in memory! (computed 31.5 MiB so far)
21/01/31 13:45:47 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 346.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:47 WARN BlockManager: Persisting block rdd_12_175 to disk instead.
21/01/31 13:45:48 INFO MemoryStore: Will not store rdd_12_175
21/01/31 13:45:48 WARN MemoryStore: Not enough space to cache rdd_12_175 in memory! (computed 31.5 MiB so far)
21/01/31 13:45:48 INFO MemoryStore: Memory use = 342.8 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 346.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:48 INFO Executor: Finished task 175.0 in stage 1.0 (TID 176). 2108 bytes result sent to driver
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 178
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 188
21/01/31 13:46:04 INFO Executor: Running task 6.0 in stage 3.0 (TID 178)
21/01/31 13:46:04 INFO Executor: Running task 16.0 in stage 3.0 (TID 188)
21/01/31 13:46:04 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
21/01/31 13:46:04 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:46:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 69.4 KiB, free 23.4 MiB)
21/01/31 13:46:04 INFO TorrentBroadcast: Reading broadcast variable 6 took 17 ms
21/01/31 13:46:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 194.6 KiB, free 23.2 MiB)
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_16 locally
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 196.458085 ms
21/01/31 13:46:05 INFO MemoryStore: Will not store rdd_12_6
21/01/31 13:46:05 WARN MemoryStore: Not enough space to cache rdd_12_6 in memory! (computed 36.2 MiB so far)
21/01/31 13:46:05 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 346.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_6 locally
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 199.770556 ms
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:06 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:06 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:06 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:06 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:06 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:06 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:06 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:06 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:06 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38170915
21/01/31 13:46:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000006_178' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000006
21/01/31 13:46:18 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000006_178: Committed
21/01/31 13:46:18 INFO Executor: Finished task 6.0 in stage 3.0 (TID 178). 2504 bytes result sent to driver
21/01/31 13:46:18 INFO CoarseGrainedExecutorBackend: Got assigned task 207
21/01/31 13:46:18 INFO Executor: Running task 27.0 in stage 3.0 (TID 207)
21/01/31 13:46:19 INFO MemoryStore: Will not store rdd_12_27
21/01/31 13:46:19 WARN MemoryStore: Not enough space to cache rdd_12_27 in memory! (computed 35.6 MiB so far)
21/01/31 13:46:19 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 346.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:19 INFO BlockManager: Found block rdd_12_27 locally
21/01/31 13:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:19 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:19 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:19 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:19 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:19 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:19 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:19 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:19 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 42854867
21/01/31 13:46:19 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000016_188' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000016
21/01/31 13:46:19 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000016_188: Committed
21/01/31 13:46:19 INFO Executor: Finished task 16.0 in stage 3.0 (TID 188). 2461 bytes result sent to driver
21/01/31 13:46:19 INFO CoarseGrainedExecutorBackend: Got assigned task 210
21/01/31 13:46:19 INFO Executor: Running task 37.0 in stage 3.0 (TID 210)
21/01/31 13:46:19 INFO MemoryStore: Will not store rdd_12_37
21/01/31 13:46:19 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 35.9 MiB so far)
21/01/31 13:46:19 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 349.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:19 INFO BlockManager: Found block rdd_12_37 locally
21/01/31 13:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:20 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:20 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:47 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.unsafe.types.UTF8String.fromBytes(UTF8String.java:126)
	at org.apache.spark.sql.execution.columnar.STRING$.extract(ColumnType.scala:447)
	at org.apache.spark.sql.execution.columnar.STRING$.extract(ColumnType.scala:431)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder.$anonfun$new$3(compressionSchemes.scala:478)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder$$Lambda$972/1617014164.apply$mcVI$sp(Unknown Source)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder.<init>(compressionSchemes.scala:477)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:369)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:361)
	at org.apache.spark.sql.execution.columnar.compression.CompressibleColumnAccessor.initialize(CompressibleColumnAccessor.scala:32)
	at org.apache.spark.sql.execution.columnar.compression.CompressibleColumnAccessor.initialize$(CompressibleColumnAccessor.scala:30)
	at org.apache.spark.sql.execution.columnar.NativeColumnAccessor.initialize(ColumnAccessor.scala:73)
	at org.apache.spark.sql.execution.columnar.ColumnAccessor.$init$(ColumnAccessor.scala:38)
	at org.apache.spark.sql.execution.columnar.BasicColumnAccessor.<init>(ColumnAccessor.scala:52)
	at org.apache.spark.sql.execution.columnar.NativeColumnAccessor.<init>(ColumnAccessor.scala:76)
	at org.apache.spark.sql.execution.columnar.StringColumnAccessor.<init>(ColumnAccessor.scala:102)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1028/1746147445.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$963/496892366.apply(Unknown Source)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$428/999276936.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
21/01/31 13:46:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39075773
21/01/31 13:46:47 ERROR Executor: Exception in task 37.0 in stage 3.0 (TID 210)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.BootstrapMethodError: call site initialization exception
	at java.lang.invoke.CallSite.makeSite(CallSite.java:341)
	at java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:307)
	at java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:297)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1427)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	... 9 more
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:46:47 INFO CoarseGrainedExecutorBackend: Got assigned task 232
21/01/31 13:46:47 INFO Executor: Running task 48.0 in stage 3.0 (TID 232)
21/01/31 13:46:47 INFO MemoryStore: Will not store rdd_12_48
21/01/31 13:46:47 WARN MemoryStore: Not enough space to cache rdd_12_48 in memory! (computed 35.6 MiB so far)
21/01/31 13:46:47 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 346.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:47 INFO BlockManager: Found block rdd_12_48 locally
21/01/31 13:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:47 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:47 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:47 ERROR FileFormatWriter: Job job_20210131134604_0003 aborted.
21/01/31 13:46:47 ERROR Executor: Exception in task 27.0 in stage 3.0 (TID 207)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.unsafe.types.UTF8String.fromBytes(UTF8String.java:126)
	at org.apache.spark.sql.execution.columnar.STRING$.extract(ColumnType.scala:447)
	at org.apache.spark.sql.execution.columnar.STRING$.extract(ColumnType.scala:431)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder.$anonfun$new$3(compressionSchemes.scala:478)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder$$Lambda$972/1617014164.apply$mcVI$sp(Unknown Source)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder.<init>(compressionSchemes.scala:477)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:369)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:361)
	at org.apache.spark.sql.execution.columnar.compression.CompressibleColumnAccessor.initialize(CompressibleColumnAccessor.scala:32)
	at org.apache.spark.sql.execution.columnar.compression.CompressibleColumnAccessor.initialize$(CompressibleColumnAccessor.scala:30)
	at org.apache.spark.sql.execution.columnar.NativeColumnAccessor.initialize(ColumnAccessor.scala:73)
	at org.apache.spark.sql.execution.columnar.ColumnAccessor.$init$(ColumnAccessor.scala:38)
	at org.apache.spark.sql.execution.columnar.BasicColumnAccessor.<init>(ColumnAccessor.scala:52)
	at org.apache.spark.sql.execution.columnar.NativeColumnAccessor.<init>(ColumnAccessor.scala:76)
	at org.apache.spark.sql.execution.columnar.StringColumnAccessor.<init>(ColumnAccessor.scala:102)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1028/1746147445.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$963/496892366.apply(Unknown Source)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$428/999276936.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
21/01/31 13:46:47 INFO CoarseGrainedExecutorBackend: Got assigned task 233
21/01/31 13:46:47 INFO Executor: Running task 37.1 in stage 3.0 (TID 233)
21/01/31 13:46:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:48 INFO MemoryStore: Will not store rdd_12_37
21/01/31 13:46:48 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 35.9 MiB so far)
21/01/31 13:46:48 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 349.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:48 INFO BlockManager: Found block rdd_12_37 locally
21/01/31 13:46:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:48 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:48 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:48 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:48 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:48 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:47:08 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34301840
21/01/31 13:47:26 ERROR Utils: Aborting task
21/01/31 13:47:29 ERROR Executor: Exception in task 37.1 in stage 3.0 (TID 233)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:29 ERROR Executor: Exception in task 48.0 in stage 3.0 (TID 232)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:29 INFO CoarseGrainedExecutorBackend: Got assigned task 272
21/01/31 13:47:29 INFO Executor: Running task 27.1 in stage 3.0 (TID 272)
21/01/31 13:47:29 INFO CoarseGrainedExecutorBackend: Got assigned task 273
21/01/31 13:47:29 INFO Executor: Running task 37.2 in stage 3.0 (TID 273)
21/01/31 13:47:29 INFO MemoryStore: Will not store rdd_12_27
21/01/31 13:47:29 WARN MemoryStore: Not enough space to cache rdd_12_27 in memory! (computed 35.6 MiB so far)
21/01/31 13:47:29 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 349.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:29 INFO BlockManager: Found block rdd_12_27 locally
21/01/31 13:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:29 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:29 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:29 INFO MemoryStore: Will not store rdd_12_37
21/01/31 13:47:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:29 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 35.9 MiB so far)
21/01/31 13:47:29 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 349.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:29 INFO BlockManager: Found block rdd_12_37 locally
21/01/31 13:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:29 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:29 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:29 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:29 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:29 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:29 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:29 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:47:44 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28492324
21/01/31 13:47:45 WARN Utils: Suppressing exception in catch: You cannot call toBytes() more than once without calling reset()
java.lang.IllegalArgumentException: You cannot call toBytes() more than once without calling reset()
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:53)
	at org.apache.parquet.column.values.rle.RunLengthBitPackingHybridEncoder.toBytes(RunLengthBitPackingHybridEncoder.java:254)
	at org.apache.parquet.column.values.rle.RunLengthBitPackingHybridValuesWriter.getBytes(RunLengthBitPackingHybridValuesWriter.java:65)
	at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:148)
	at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:235)
	at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:122)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:172)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)
	at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:84)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:278)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1422)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:47:45 ERROR Executor: Exception in task 37.2 in stage 3.0 (TID 273)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:45 INFO CoarseGrainedExecutorBackend: Got assigned task 292
21/01/31 13:47:45 INFO Executor: Running task 48.1 in stage 3.0 (TID 292)
21/01/31 13:47:46 INFO MemoryStore: Will not store rdd_12_48
21/01/31 13:47:46 WARN MemoryStore: Not enough space to cache rdd_12_48 in memory! (computed 35.6 MiB so far)
21/01/31 13:47:46 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 346.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:46 INFO BlockManager: Found block rdd_12_48 locally
21/01/31 13:47:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:46 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:46 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:46 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:46 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 40920172
21/01/31 13:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000027_272' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000027
21/01/31 13:47:50 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000027_272: Committed
21/01/31 13:47:50 INFO Executor: Finished task 27.1 in stage 3.0 (TID 272). 2504 bytes result sent to driver
21/01/31 13:47:50 INFO CoarseGrainedExecutorBackend: Got assigned task 296
21/01/31 13:47:50 INFO Executor: Running task 37.3 in stage 3.0 (TID 296)
21/01/31 13:47:50 INFO MemoryStore: Will not store rdd_12_37
21/01/31 13:47:50 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 35.9 MiB so far)
21/01/31 13:47:50 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 346.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:50 INFO BlockManager: Found block rdd_12_37 locally
21/01/31 13:47:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:50 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:50 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:50 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:50 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:04 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:48:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36320897
21/01/31 13:48:05 ERROR Executor: Exception in task 48.1 in stage 3.0 (TID 292)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:48:05 INFO CoarseGrainedExecutorBackend: Got assigned task 318
21/01/31 13:48:05 INFO Executor: Running task 56.0 in stage 3.0 (TID 318)
21/01/31 13:48:05 INFO MemoryStore: Will not store rdd_12_56
21/01/31 13:48:05 WARN MemoryStore: Not enough space to cache rdd_12_56 in memory! (computed 36.1 MiB so far)
21/01/31 13:48:05 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 346.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:05 INFO BlockManager: Found block rdd_12_56 locally
21/01/31 13:48:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:05 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38899243
21/01/31 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000037_296' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000037
21/01/31 13:48:11 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000037_296: Committed
21/01/31 13:48:11 INFO Executor: Finished task 37.3 in stage 3.0 (TID 296). 2504 bytes result sent to driver
21/01/31 13:48:11 INFO CoarseGrainedExecutorBackend: Got assigned task 326
21/01/31 13:48:11 INFO Executor: Running task 78.0 in stage 3.0 (TID 326)
21/01/31 13:48:11 INFO MemoryStore: Will not store rdd_12_78
21/01/31 13:48:11 WARN MemoryStore: Not enough space to cache rdd_12_78 in memory! (computed 35.8 MiB so far)
21/01/31 13:48:11 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 346.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:11 INFO BlockManager: Found block rdd_12_78 locally
21/01/31 13:48:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:11 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:11 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:11 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:11 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37840119
21/01/31 13:48:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000056_318' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000056
21/01/31 13:48:21 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000056_318: Committed
21/01/31 13:48:21 INFO Executor: Finished task 56.0 in stage 3.0 (TID 318). 2504 bytes result sent to driver
21/01/31 13:48:21 INFO CoarseGrainedExecutorBackend: Got assigned task 335
21/01/31 13:48:21 INFO Executor: Running task 98.0 in stage 3.0 (TID 335)
21/01/31 13:48:21 INFO BlockManager: Found block rdd_12_98 locally
21/01/31 13:48:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:21 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:21 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:21 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:21 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36291589
21/01/31 13:48:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000078_326' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000078
21/01/31 13:48:27 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000078_326: Committed
21/01/31 13:48:28 INFO Executor: Finished task 78.0 in stage 3.0 (TID 326). 2504 bytes result sent to driver
21/01/31 13:48:28 INFO CoarseGrainedExecutorBackend: Got assigned task 340
21/01/31 13:48:28 INFO Executor: Running task 113.0 in stage 3.0 (TID 340)
21/01/31 13:48:28 INFO MemoryStore: Will not store rdd_12_113
21/01/31 13:48:28 WARN MemoryStore: Not enough space to cache rdd_12_113 in memory! (computed 36.0 MiB so far)
21/01/31 13:48:28 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 346.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:28 INFO BlockManager: Found block rdd_12_113 locally
21/01/31 13:48:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:28 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:28 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:28 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:28 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:28 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:28 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:28 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:28 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:28 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:28 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:28 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35385960
21/01/31 13:48:34 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000098_335' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000098
21/01/31 13:48:34 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000098_335: Committed
21/01/31 13:48:34 INFO Executor: Finished task 98.0 in stage 3.0 (TID 335). 2504 bytes result sent to driver
21/01/31 13:48:34 INFO CoarseGrainedExecutorBackend: Got assigned task 349
21/01/31 13:48:34 INFO Executor: Running task 134.0 in stage 3.0 (TID 349)
21/01/31 13:48:34 INFO MemoryStore: Will not store rdd_12_134
21/01/31 13:48:34 WARN MemoryStore: Not enough space to cache rdd_12_134 in memory! (computed 35.2 MiB so far)
21/01/31 13:48:34 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.0 MiB (scratch space shared across 1 tasks(s)) = 346.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:34 INFO BlockManager: Found block rdd_12_134 locally
21/01/31 13:48:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:34 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:34 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:34 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:34 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:34 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:34 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:34 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:34 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:34 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35418967
21/01/31 13:48:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000113_340' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000113
21/01/31 13:48:40 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000113_340: Committed
21/01/31 13:48:40 INFO Executor: Finished task 113.0 in stage 3.0 (TID 340). 2504 bytes result sent to driver
21/01/31 13:48:40 INFO CoarseGrainedExecutorBackend: Got assigned task 356
21/01/31 13:48:40 INFO Executor: Running task 155.0 in stage 3.0 (TID 356)
21/01/31 13:48:41 INFO MemoryStore: Will not store rdd_12_155
21/01/31 13:48:41 WARN MemoryStore: Not enough space to cache rdd_12_155 in memory! (computed 36.7 MiB so far)
21/01/31 13:48:41 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 346.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:41 INFO BlockManager: Found block rdd_12_155 locally
21/01/31 13:48:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:41 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:41 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35251259
21/01/31 13:48:47 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000134_349' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000134
21/01/31 13:48:47 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000134_349: Committed
21/01/31 13:48:47 INFO Executor: Finished task 134.0 in stage 3.0 (TID 349). 2504 bytes result sent to driver
21/01/31 13:48:47 INFO CoarseGrainedExecutorBackend: Got assigned task 365
21/01/31 13:48:47 INFO Executor: Running task 175.0 in stage 3.0 (TID 365)
21/01/31 13:48:47 INFO MemoryStore: Will not store rdd_12_175
21/01/31 13:48:47 WARN MemoryStore: Not enough space to cache rdd_12_175 in memory! (computed 31.5 MiB so far)
21/01/31 13:48:47 INFO MemoryStore: Memory use = 343.0 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 346.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:47 INFO BlockManager: Found block rdd_12_175 locally
21/01/31 13:48:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:47 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:47 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8274951
21/01/31 13:48:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000175_365' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000175
21/01/31 13:48:50 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000175_365: Committed
21/01/31 13:48:50 INFO Executor: Finished task 175.0 in stage 3.0 (TID 365). 2504 bytes result sent to driver
21/01/31 13:48:50 INFO CoarseGrainedExecutorBackend: Got assigned task 368
21/01/31 13:48:50 INFO Executor: Running task 171.0 in stage 3.0 (TID 368)
21/01/31 13:48:50 INFO BlockManager: Read rdd_12_171 from the disk of a same host executor is successful.
21/01/31 13:48:50 INFO BlockManager: Found block rdd_12_171 remotely
21/01/31 13:48:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:50 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:50 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:50 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:50 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36196093
21/01/31 13:48:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000155_356' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000155
21/01/31 13:48:53 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000155_356: Committed
21/01/31 13:48:53 INFO Executor: Finished task 155.0 in stage 3.0 (TID 356). 2504 bytes result sent to driver
21/01/31 13:48:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35090772
21/01/31 13:48:59 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000171_368' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000171
21/01/31 13:48:59 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000171_368: Committed
21/01/31 13:48:59 INFO Executor: Finished task 171.0 in stage 3.0 (TID 368). 2504 bytes result sent to driver
