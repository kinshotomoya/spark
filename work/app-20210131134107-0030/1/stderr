Spark Executor Command: "/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/bin/java" "-cp" "/Users/kinsho/workspace/spark-3.0.1/conf/:/Users/kinsho/workspace/spark-3.0.1/assembly/target/scala-2.12/jars/*" "-Xmx1024M" "-Dspark.driver.port=51030" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.11.7:51030" "--executor-id" "1" "--hostname" "192.168.11.7" "--cores" "2" "--app-id" "app-20210131134107-0030" "--worker-url" "spark://Worker@192.168.11.7:63916"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/31 13:41:09 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 27465@ST000000035
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for TERM
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for HUP
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for INT
21/01/31 13:41:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/01/31 13:41:10 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 125 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO DiskBlockManager: Created local directory at /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-775ca6ad-503a-4ccd-a65d-f6d5ff6c06ec/executor-92a4b876-6867-40ae-ba01-1d7e6707ff4d/blockmgr-a626da0f-5c4c-4f2e-aadf-6b81bc7cc8a0
21/01/31 13:41:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/01/31 13:41:11 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.11.7:51030
21/01/31 13:41:11 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.11.7:63916
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:63916 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.11.7:63916
21/01/31 13:41:11 INFO ResourceUtils: ==============================================================
21/01/31 13:41:11 INFO ResourceUtils: Resources for spark.executor:

21/01/31 13:41:11 INFO ResourceUtils: ==============================================================
21/01/31 13:41:11 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
21/01/31 13:41:11 INFO Executor: Starting executor ID 1 on host 192.168.11.7
21/01/31 13:41:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51078.
21/01/31 13:41:12 INFO NettyBlockTransferService: Server created on 192.168.11.7:51078
21/01/31 13:41:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/31 13:41:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(1, 192.168.11.7, 51078, None)
21/01/31 13:41:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(1, 192.168.11.7, 51078, None)
21/01/31 13:41:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(1, 192.168.11.7, 51078, None)
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 9
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 19
21/01/31 13:41:16 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
21/01/31 13:41:16 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
21/01/31 13:41:16 INFO Executor: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar with timestamp 1612068067341
21/01/31 13:41:16 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 3 ms (0 ms spent in bootstraps)
21/01/31 13:41:16 INFO Utils: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar to /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-775ca6ad-503a-4ccd-a65d-f6d5ff6c06ec/executor-92a4b876-6867-40ae-ba01-1d7e6707ff4d/spark-b38c582d-688a-42de-a53f-0348c4eeea9a/fetchFileTemp7343064431496363488.tmp
21/01/31 13:41:16 INFO Utils: Copying /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-775ca6ad-503a-4ccd-a65d-f6d5ff6c06ec/executor-92a4b876-6867-40ae-ba01-1d7e6707ff4d/spark-b38c582d-688a-42de-a53f-0348c4eeea9a/16839229251612068067341_cache to /Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/1/./simple-project_2.12-1.0.jar
21/01/31 13:41:19 INFO Executor: Adding file:/Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/1/./simple-project_2.12-1.0.jar to class loader
21/01/31 13:41:19 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:20 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51089 after 21 ms (0 ms spent in bootstraps)
21/01/31 13:41:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 366.3 MiB)
21/01/31 13:41:20 INFO TorrentBroadcast: Reading broadcast variable 4 took 432 ms
21/01/31 13:41:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 43.3 KiB, free 366.2 MiB)
21/01/31 13:41:22 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 2415919104-2550136832, partition values: [empty row]
21/01/31 13:41:22 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 1073741824-1207959552, partition values: [empty row]
21/01/31 13:41:24 INFO CodeGenerator: Code generated in 1311.00588 ms
21/01/31 13:41:24 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:24 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51094 after 1 ms (0 ms spent in bootstraps)
21/01/31 13:41:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.2 MiB)
21/01/31 13:41:24 INFO TorrentBroadcast: Reading broadcast variable 3 took 75 ms
21/01/31 13:41:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 336.8 KiB, free 365.9 MiB)
21/01/31 13:41:51 INFO MemoryStore: Will not store rdd_12_8
21/01/31 13:41:51 WARN MemoryStore: Not enough space to cache rdd_12_8 in memory! (computed 136.3 MiB so far)
21/01/31 13:41:51 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 305.9 MiB (scratch space shared across 2 tasks(s)) = 306.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:41:51 WARN BlockManager: Persisting block rdd_12_8 to disk instead.
21/01/31 13:42:01 INFO MemoryStore: Will not store rdd_12_8
21/01/31 13:42:01 WARN MemoryStore: Not enough space to cache rdd_12_8 in memory! (computed 136.3 MiB so far)
21/01/31 13:42:01 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 305.9 MiB (scratch space shared across 2 tasks(s)) = 306.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:01 INFO CodeGenerator: Code generated in 13.262412 ms
21/01/31 13:42:01 INFO CodeGenerator: Code generated in 69.914532 ms
21/01/31 13:42:01 INFO CodeGenerator: Code generated in 19.468381 ms
21/01/31 13:42:01 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2108 bytes result sent to driver
21/01/31 13:42:01 INFO CoarseGrainedExecutorBackend: Got assigned task 39
21/01/31 13:42:01 INFO Executor: Running task 38.0 in stage 1.0 (TID 39)
21/01/31 13:42:01 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 5100273664-5234491392, partition values: [empty row]
21/01/31 13:42:02 INFO MemoryStore: Block rdd_12_18 stored as values in memory (estimated size 187.2 MiB, free 178.7 MiB)
21/01/31 13:42:02 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2108 bytes result sent to driver
21/01/31 13:42:02 INFO CoarseGrainedExecutorBackend: Got assigned task 40
21/01/31 13:42:02 INFO Executor: Running task 39.0 in stage 1.0 (TID 40)
21/01/31 13:42:02 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 5234491392-5368709120, partition values: [empty row]
21/01/31 13:42:14 INFO MemoryStore: Will not store rdd_12_39
21/01/31 13:42:14 WARN MemoryStore: Not enough space to cache rdd_12_39 in memory! (computed 69.1 MiB so far)
21/01/31 13:42:14 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 157.0 MiB (scratch space shared across 2 tasks(s)) = 344.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:14 WARN BlockManager: Persisting block rdd_12_39 to disk instead.
21/01/31 13:42:26 INFO MemoryStore: Will not store rdd_12_38
21/01/31 13:42:26 WARN MemoryStore: Not enough space to cache rdd_12_38 in memory! (computed 136.8 MiB so far)
21/01/31 13:42:26 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 103.8 MiB (scratch space shared across 1 tasks(s)) = 291.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:26 WARN BlockManager: Persisting block rdd_12_38 to disk instead.
21/01/31 13:42:33 INFO MemoryStore: Will not store rdd_12_38
21/01/31 13:42:33 WARN MemoryStore: Not enough space to cache rdd_12_38 in memory! (computed 136.8 MiB so far)
21/01/31 13:42:33 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 103.8 MiB (scratch space shared across 1 tasks(s)) = 291.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:33 INFO Executor: Finished task 38.0 in stage 1.0 (TID 39). 2108 bytes result sent to driver
21/01/31 13:42:33 INFO CoarseGrainedExecutorBackend: Got assigned task 50
21/01/31 13:42:33 INFO Executor: Running task 49.0 in stage 1.0 (TID 50)
21/01/31 13:42:33 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 6576668672-6710886400, partition values: [empty row]
21/01/31 13:42:34 INFO MemoryStore: Will not store rdd_12_39
21/01/31 13:42:34 WARN MemoryStore: Not enough space to cache rdd_12_39 in memory! (computed 136.1 MiB so far)
21/01/31 13:42:34 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 106.8 MiB (scratch space shared across 2 tasks(s)) = 294.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:34 INFO Executor: Finished task 39.0 in stage 1.0 (TID 40). 2108 bytes result sent to driver
21/01/31 13:42:34 INFO CoarseGrainedExecutorBackend: Got assigned task 55
21/01/31 13:42:34 INFO Executor: Running task 54.0 in stage 1.0 (TID 55)
21/01/31 13:42:34 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 7247757312-7381975040, partition values: [empty row]
21/01/31 13:42:47 INFO MemoryStore: Will not store rdd_12_54
21/01/31 13:42:47 WARN MemoryStore: Not enough space to cache rdd_12_54 in memory! (computed 69.0 MiB so far)
21/01/31 13:42:47 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 157.4 MiB (scratch space shared across 2 tasks(s)) = 345.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:47 WARN BlockManager: Persisting block rdd_12_54 to disk instead.
21/01/31 13:42:58 INFO MemoryStore: Will not store rdd_12_49
21/01/31 13:42:58 WARN MemoryStore: Not enough space to cache rdd_12_49 in memory! (computed 137.0 MiB so far)
21/01/31 13:42:58 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 104.3 MiB (scratch space shared across 1 tasks(s)) = 291.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:58 WARN BlockManager: Persisting block rdd_12_49 to disk instead.
21/01/31 13:43:04 INFO MemoryStore: Will not store rdd_12_49
21/01/31 13:43:04 WARN MemoryStore: Not enough space to cache rdd_12_49 in memory! (computed 137.0 MiB so far)
21/01/31 13:43:04 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 104.3 MiB (scratch space shared across 1 tasks(s)) = 291.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:04 INFO Executor: Finished task 49.0 in stage 1.0 (TID 50). 2108 bytes result sent to driver
21/01/31 13:43:04 INFO CoarseGrainedExecutorBackend: Got assigned task 69
21/01/31 13:43:04 INFO Executor: Running task 68.0 in stage 1.0 (TID 69)
21/01/31 13:43:04 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 9126805504-9261023232, partition values: [empty row]
21/01/31 13:43:05 INFO MemoryStore: Will not store rdd_12_54
21/01/31 13:43:05 WARN MemoryStore: Not enough space to cache rdd_12_54 in memory! (computed 136.3 MiB so far)
21/01/31 13:43:05 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 106.7 MiB (scratch space shared across 2 tasks(s)) = 294.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:05 INFO Executor: Finished task 54.0 in stage 1.0 (TID 55). 2108 bytes result sent to driver
21/01/31 13:43:05 INFO CoarseGrainedExecutorBackend: Got assigned task 73
21/01/31 13:43:05 INFO Executor: Running task 72.0 in stage 1.0 (TID 73)
21/01/31 13:43:06 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 9663676416-9797894144, partition values: [empty row]
21/01/31 13:43:18 INFO MemoryStore: Will not store rdd_12_72
21/01/31 13:43:18 WARN MemoryStore: Not enough space to cache rdd_12_72 in memory! (computed 70.5 MiB so far)
21/01/31 13:43:18 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 157.8 MiB (scratch space shared across 2 tasks(s)) = 345.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:18 WARN BlockManager: Persisting block rdd_12_72 to disk instead.
21/01/31 13:43:30 INFO MemoryStore: Will not store rdd_12_68
21/01/31 13:43:30 WARN MemoryStore: Not enough space to cache rdd_12_68 in memory! (computed 135.6 MiB so far)
21/01/31 13:43:30 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 103.5 MiB (scratch space shared across 1 tasks(s)) = 291.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:30 WARN BlockManager: Persisting block rdd_12_68 to disk instead.
21/01/31 13:43:36 INFO MemoryStore: Will not store rdd_12_68
21/01/31 13:43:36 WARN MemoryStore: Not enough space to cache rdd_12_68 in memory! (computed 135.6 MiB so far)
21/01/31 13:43:36 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 103.5 MiB (scratch space shared across 1 tasks(s)) = 291.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:36 INFO Executor: Finished task 68.0 in stage 1.0 (TID 69). 2108 bytes result sent to driver
21/01/31 13:43:36 INFO CoarseGrainedExecutorBackend: Got assigned task 89
21/01/31 13:43:36 INFO Executor: Running task 88.0 in stage 1.0 (TID 89)
21/01/31 13:43:36 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 11811160064-11945377792, partition values: [empty row]
21/01/31 13:43:36 INFO MemoryStore: Will not store rdd_12_72
21/01/31 13:43:36 WARN MemoryStore: Not enough space to cache rdd_12_72 in memory! (computed 136.9 MiB so far)
21/01/31 13:43:36 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 106.7 MiB (scratch space shared across 2 tasks(s)) = 294.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:37 INFO Executor: Finished task 72.0 in stage 1.0 (TID 73). 2108 bytes result sent to driver
21/01/31 13:43:37 INFO CoarseGrainedExecutorBackend: Got assigned task 92
21/01/31 13:43:37 INFO Executor: Running task 91.0 in stage 1.0 (TID 92)
21/01/31 13:43:37 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 12213813248-12348030976, partition values: [empty row]
21/01/31 13:43:49 INFO MemoryStore: Will not store rdd_12_91
21/01/31 13:43:49 WARN MemoryStore: Not enough space to cache rdd_12_91 in memory! (computed 68.0 MiB so far)
21/01/31 13:43:49 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 152.1 MiB (scratch space shared across 2 tasks(s)) = 339.8 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:49 WARN BlockManager: Persisting block rdd_12_91 to disk instead.
21/01/31 13:44:01 INFO MemoryStore: Will not store rdd_12_88
21/01/31 13:44:01 WARN MemoryStore: Not enough space to cache rdd_12_88 in memory! (computed 133.7 MiB so far)
21/01/31 13:44:01 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 100.1 MiB (scratch space shared across 1 tasks(s)) = 287.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:01 WARN BlockManager: Persisting block rdd_12_88 to disk instead.
21/01/31 13:44:06 INFO MemoryStore: Will not store rdd_12_88
21/01/31 13:44:06 WARN MemoryStore: Not enough space to cache rdd_12_88 in memory! (computed 133.7 MiB so far)
21/01/31 13:44:06 INFO MemoryStore: Memory use = 187.6 MiB (blocks) + 100.1 MiB (scratch space shared across 1 tasks(s)) = 287.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:06 INFO Executor: Finished task 88.0 in stage 1.0 (TID 89). 2108 bytes result sent to driver
21/01/31 13:44:06 INFO CoarseGrainedExecutorBackend: Got assigned task 108
21/01/31 13:44:06 INFO Executor: Running task 107.0 in stage 1.0 (TID 108)
21/01/31 13:44:06 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 14361296896-14495514624, partition values: [empty row]
21/01/31 13:44:07 INFO MemoryStore: Block rdd_12_91 stored as values in memory (estimated size 156.3 MiB, free 22.4 MiB)
21/01/31 13:44:07 INFO Executor: Finished task 91.0 in stage 1.0 (TID 92). 2108 bytes result sent to driver
21/01/31 13:44:07 INFO CoarseGrainedExecutorBackend: Got assigned task 110
21/01/31 13:44:07 INFO Executor: Running task 109.0 in stage 1.0 (TID 110)
21/01/31 13:44:07 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 14629732352-14763950080, partition values: [empty row]
21/01/31 13:44:13 INFO MemoryStore: Will not store rdd_12_107
21/01/31 13:44:13 WARN MemoryStore: Not enough space to cache rdd_12_107 in memory! (computed 35.5 MiB so far)
21/01/31 13:44:13 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 5.9 MiB (scratch space shared across 2 tasks(s)) = 349.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:13 WARN BlockManager: Persisting block rdd_12_107 to disk instead.
21/01/31 13:44:14 INFO MemoryStore: Will not store rdd_12_109
21/01/31 13:44:14 WARN MemoryStore: Not enough space to cache rdd_12_109 in memory! (computed 34.3 MiB so far)
21/01/31 13:44:14 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 2.9 MiB (scratch space shared across 1 tasks(s)) = 346.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:14 WARN BlockManager: Persisting block rdd_12_109 to disk instead.
21/01/31 13:44:38 INFO MemoryStore: Will not store rdd_12_107
21/01/31 13:44:38 WARN MemoryStore: Not enough space to cache rdd_12_107 in memory! (computed 35.5 MiB so far)
21/01/31 13:44:38 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.0 MiB (scratch space shared across 1 tasks(s)) = 347.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:38 INFO Executor: Finished task 107.0 in stage 1.0 (TID 108). 2108 bytes result sent to driver
21/01/31 13:44:38 INFO CoarseGrainedExecutorBackend: Got assigned task 129
21/01/31 13:44:38 INFO Executor: Running task 128.0 in stage 1.0 (TID 129)
21/01/31 13:44:38 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 17179869184-17314086912, partition values: [empty row]
21/01/31 13:44:39 INFO MemoryStore: Will not store rdd_12_109
21/01/31 13:44:39 WARN MemoryStore: Not enough space to cache rdd_12_109 in memory! (computed 34.3 MiB so far)
21/01/31 13:44:39 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 350.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:39 INFO Executor: Finished task 109.0 in stage 1.0 (TID 110). 2108 bytes result sent to driver
21/01/31 13:44:39 INFO CoarseGrainedExecutorBackend: Got assigned task 134
21/01/31 13:44:39 INFO Executor: Running task 133.0 in stage 1.0 (TID 134)
21/01/31 13:44:39 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 17850957824-17985175552, partition values: [empty row]
21/01/31 13:44:45 INFO MemoryStore: Will not store rdd_12_128
21/01/31 13:44:45 WARN MemoryStore: Not enough space to cache rdd_12_128 in memory! (computed 35.2 MiB so far)
21/01/31 13:44:45 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 350.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:45 WARN BlockManager: Persisting block rdd_12_128 to disk instead.
21/01/31 13:44:46 INFO MemoryStore: Will not store rdd_12_133
21/01/31 13:44:46 WARN MemoryStore: Not enough space to cache rdd_12_133 in memory! (computed 35.7 MiB so far)
21/01/31 13:44:46 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 347.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:46 WARN BlockManager: Persisting block rdd_12_133 to disk instead.
21/01/31 13:45:09 INFO MemoryStore: Will not store rdd_12_128
21/01/31 13:45:09 WARN MemoryStore: Not enough space to cache rdd_12_128 in memory! (computed 35.2 MiB so far)
21/01/31 13:45:09 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:09 INFO Executor: Finished task 128.0 in stage 1.0 (TID 129). 2108 bytes result sent to driver
21/01/31 13:45:09 INFO CoarseGrainedExecutorBackend: Got assigned task 150
21/01/31 13:45:09 INFO Executor: Running task 149.0 in stage 1.0 (TID 150)
21/01/31 13:45:09 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 19998441472-20132659200, partition values: [empty row]
21/01/31 13:45:10 INFO MemoryStore: Will not store rdd_12_133
21/01/31 13:45:10 WARN MemoryStore: Not enough space to cache rdd_12_133 in memory! (computed 35.7 MiB so far)
21/01/31 13:45:10 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 350.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:11 INFO Executor: Finished task 133.0 in stage 1.0 (TID 134). 2108 bytes result sent to driver
21/01/31 13:45:11 INFO CoarseGrainedExecutorBackend: Got assigned task 153
21/01/31 13:45:11 INFO Executor: Running task 152.0 in stage 1.0 (TID 153)
21/01/31 13:45:11 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 20401094656-20535312384, partition values: [empty row]
21/01/31 13:45:17 INFO MemoryStore: Will not store rdd_12_149
21/01/31 13:45:17 WARN MemoryStore: Not enough space to cache rdd_12_149 in memory! (computed 36.7 MiB so far)
21/01/31 13:45:17 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 350.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:17 WARN BlockManager: Persisting block rdd_12_149 to disk instead.
21/01/31 13:45:18 INFO MemoryStore: Will not store rdd_12_152
21/01/31 13:45:18 WARN MemoryStore: Not enough space to cache rdd_12_152 in memory! (computed 37.4 MiB so far)
21/01/31 13:45:18 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 347.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:18 WARN BlockManager: Persisting block rdd_12_152 to disk instead.
21/01/31 13:45:39 INFO MemoryStore: Will not store rdd_12_149
21/01/31 13:45:39 WARN MemoryStore: Not enough space to cache rdd_12_149 in memory! (computed 36.7 MiB so far)
21/01/31 13:45:39 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 347.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:40 INFO Executor: Finished task 149.0 in stage 1.0 (TID 150). 2108 bytes result sent to driver
21/01/31 13:45:40 INFO CoarseGrainedExecutorBackend: Got assigned task 167
21/01/31 13:45:40 INFO Executor: Running task 166.0 in stage 1.0 (TID 167)
21/01/31 13:45:40 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22280142848-22414360576, partition values: [empty row]
21/01/31 13:45:40 INFO MemoryStore: Will not store rdd_12_152
21/01/31 13:45:40 WARN MemoryStore: Not enough space to cache rdd_12_152 in memory! (computed 37.4 MiB so far)
21/01/31 13:45:40 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 6.7 MiB (scratch space shared across 2 tasks(s)) = 350.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:41 INFO Executor: Finished task 152.0 in stage 1.0 (TID 153). 2108 bytes result sent to driver
21/01/31 13:45:41 INFO CoarseGrainedExecutorBackend: Got assigned task 172
21/01/31 13:45:41 INFO Executor: Running task 171.0 in stage 1.0 (TID 172)
21/01/31 13:45:41 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22951231488-23085449216, partition values: [empty row]
21/01/31 13:45:46 INFO MemoryStore: Will not store rdd_12_166
21/01/31 13:45:46 WARN MemoryStore: Not enough space to cache rdd_12_166 in memory! (computed 37.1 MiB so far)
21/01/31 13:45:46 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 350.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:46 WARN BlockManager: Persisting block rdd_12_166 to disk instead.
21/01/31 13:45:47 INFO MemoryStore: Will not store rdd_12_171
21/01/31 13:45:47 WARN MemoryStore: Not enough space to cache rdd_12_171 in memory! (computed 37.7 MiB so far)
21/01/31 13:45:47 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:47 WARN BlockManager: Persisting block rdd_12_171 to disk instead.
21/01/31 13:46:02 INFO MemoryStore: Will not store rdd_12_166
21/01/31 13:46:02 WARN MemoryStore: Not enough space to cache rdd_12_166 in memory! (computed 37.1 MiB so far)
21/01/31 13:46:02 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.4 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:02 INFO Executor: Finished task 166.0 in stage 1.0 (TID 167). 2108 bytes result sent to driver
21/01/31 13:46:03 INFO MemoryStore: Will not store rdd_12_171
21/01/31 13:46:03 WARN MemoryStore: Not enough space to cache rdd_12_171 in memory! (computed 37.7 MiB so far)
21/01/31 13:46:03 INFO MemoryStore: Memory use = 343.9 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:03 INFO Executor: Finished task 171.0 in stage 1.0 (TID 172). 2108 bytes result sent to driver
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 185
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 195
21/01/31 13:46:04 INFO Executor: Running task 8.0 in stage 3.0 (TID 185)
21/01/31 13:46:04 INFO Executor: Running task 18.0 in stage 3.0 (TID 195)
21/01/31 13:46:04 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
21/01/31 13:46:04 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:46:04 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51036 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:46:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 69.4 KiB, free 22.3 MiB)
21/01/31 13:46:04 INFO TorrentBroadcast: Reading broadcast variable 6 took 23 ms
21/01/31 13:46:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 194.6 KiB, free 22.1 MiB)
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_18 locally
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 242.320435 ms
21/01/31 13:46:05 INFO MemoryStore: Will not store rdd_12_8
21/01/31 13:46:05 WARN MemoryStore: Not enough space to cache rdd_12_8 in memory! (computed 35.7 MiB so far)
21/01/31 13:46:05 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_8 locally
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 222.008219 ms
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 41135870
21/01/31 13:46:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 42950673
21/01/31 13:46:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000008_185' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000008
21/01/31 13:46:20 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000008_185: Committed
21/01/31 13:46:20 INFO Executor: Finished task 8.0 in stage 3.0 (TID 185). 2504 bytes result sent to driver
21/01/31 13:46:20 INFO CoarseGrainedExecutorBackend: Got assigned task 216
21/01/31 13:46:20 INFO Executor: Running task 38.0 in stage 3.0 (TID 216)
21/01/31 13:46:20 INFO MemoryStore: Will not store rdd_12_38
21/01/31 13:46:20 WARN MemoryStore: Not enough space to cache rdd_12_38 in memory! (computed 35.5 MiB so far)
21/01/31 13:46:20 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:20 INFO BlockManager: Found block rdd_12_38 locally
21/01/31 13:46:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:20 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:20 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:21 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000018_195' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000018
21/01/31 13:46:21 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000018_195: Committed
21/01/31 13:46:21 INFO Executor: Finished task 18.0 in stage 3.0 (TID 195). 2461 bytes result sent to driver
21/01/31 13:46:21 INFO CoarseGrainedExecutorBackend: Got assigned task 217
21/01/31 13:46:21 INFO Executor: Running task 39.0 in stage 3.0 (TID 217)
21/01/31 13:46:21 INFO MemoryStore: Will not store rdd_12_39
21/01/31 13:46:21 WARN MemoryStore: Not enough space to cache rdd_12_39 in memory! (computed 35.4 MiB so far)
21/01/31 13:46:21 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 350.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:21 INFO BlockManager: Found block rdd_12_39 locally
21/01/31 13:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:21 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:21 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:21 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:21 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:21 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:21 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:21 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:21 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:21 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:21 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:21 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:49 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:75)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1939)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1975)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:753)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1036/1918857709.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$971/1708922260.apply(Unknown Source)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$427/996017622.apply(Unknown Source)
21/01/31 13:47:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37596441
21/01/31 13:47:39 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.addSlab(CapacityByteArrayOutputStream.java:184)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.write(CapacityByteArrayOutputStream.java:214)
	at org.apache.parquet.bytes.LittleEndianDataOutputStream.write(LittleEndianDataOutputStream.java:76)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.writeTo(Binary.java:297)
	at org.apache.parquet.column.values.plain.PlainValuesWriter.writeBytes(PlainValuesWriter.java:55)
	at org.apache.parquet.column.values.fallback.FallbackValuesWriter.writeBytes(FallbackValuesWriter.java:168)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:198)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1004/262744881.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1040/856366332.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1039/1594814971.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1036/1918857709.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
21/01/31 13:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39322223
21/01/31 13:47:41 WARN Utils: Suppressing exception in catch: Java heap space
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:41 ERROR Executor: Exception in task 39.0 in stage 3.0 (TID 217)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.lang.reflect.Array.newInstance(Array.java:75)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1939)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1975)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:753)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1036/1918857709.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$971/1708922260.apply(Unknown Source)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$427/996017622.apply(Unknown Source)
21/01/31 13:47:41 INFO CoarseGrainedExecutorBackend: Got assigned task 288
21/01/31 13:47:41 INFO Executor: Running task 49.0 in stage 3.0 (TID 288)
21/01/31 13:47:42 INFO MemoryStore: Will not store rdd_12_49
21/01/31 13:47:42 WARN MemoryStore: Not enough space to cache rdd_12_49 in memory! (computed 35.8 MiB so far)
21/01/31 13:47:42 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:42 INFO BlockManager: Found block rdd_12_49 locally
21/01/31 13:47:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:42 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:42 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:42 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:42 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:42 ERROR FileFormatWriter: Job job_20210131134604_0003 aborted.
21/01/31 13:47:42 ERROR Executor: Exception in task 38.0 in stage 3.0 (TID 216)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.addSlab(CapacityByteArrayOutputStream.java:184)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.write(CapacityByteArrayOutputStream.java:214)
	at org.apache.parquet.bytes.LittleEndianDataOutputStream.write(LittleEndianDataOutputStream.java:76)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.writeTo(Binary.java:297)
	at org.apache.parquet.column.values.plain.PlainValuesWriter.writeBytes(PlainValuesWriter.java:55)
	at org.apache.parquet.column.values.fallback.FallbackValuesWriter.writeBytes(FallbackValuesWriter.java:168)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:198)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1004/262744881.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1040/856366332.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1039/1594814971.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1036/1918857709.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
21/01/31 13:47:42 INFO CoarseGrainedExecutorBackend: Got assigned task 289
21/01/31 13:47:42 INFO Executor: Running task 39.1 in stage 3.0 (TID 289)
21/01/31 13:47:42 INFO MemoryStore: Will not store rdd_12_39
21/01/31 13:47:42 WARN MemoryStore: Not enough space to cache rdd_12_39 in memory! (computed 35.4 MiB so far)
21/01/31 13:47:42 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 350.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:42 INFO BlockManager: Found block rdd_12_39 locally
21/01/31 13:47:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:42 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:42 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:42 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:42 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38242605
21/01/31 13:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38952202
21/01/31 13:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000049_288' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000049
21/01/31 13:47:57 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000049_288: Committed
21/01/31 13:47:57 INFO Executor: Finished task 49.0 in stage 3.0 (TID 288). 2504 bytes result sent to driver
21/01/31 13:47:57 INFO CoarseGrainedExecutorBackend: Got assigned task 306
21/01/31 13:47:57 INFO Executor: Running task 38.1 in stage 3.0 (TID 306)
21/01/31 13:47:57 INFO MemoryStore: Will not store rdd_12_38
21/01/31 13:47:57 WARN MemoryStore: Not enough space to cache rdd_12_38 in memory! (computed 35.5 MiB so far)
21/01/31 13:47:57 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:57 INFO BlockManager: Found block rdd_12_38 locally
21/01/31 13:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:57 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:57 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:57 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:57 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000039_289' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000039
21/01/31 13:47:58 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000039_289: Committed
21/01/31 13:47:58 INFO Executor: Finished task 39.1 in stage 3.0 (TID 289). 2461 bytes result sent to driver
21/01/31 13:47:58 INFO CoarseGrainedExecutorBackend: Got assigned task 307
21/01/31 13:47:58 INFO Executor: Running task 54.0 in stage 3.0 (TID 307)
21/01/31 13:47:58 INFO MemoryStore: Will not store rdd_12_54
21/01/31 13:47:58 WARN MemoryStore: Not enough space to cache rdd_12_54 in memory! (computed 35.4 MiB so far)
21/01/31 13:47:58 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 6.1 MiB (scratch space shared across 2 tasks(s)) = 350.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:58 INFO BlockManager: Found block rdd_12_54 locally
21/01/31 13:47:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:58 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:58 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:58 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:58 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:58 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:58 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:58 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:58 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:58 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:51:45 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:51:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37228962
21/01/31 13:51:52 ERROR Executor: Exception in task 54.0 in stage 3.0 (TID 307)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
21/01/31 13:51:52 INFO CoarseGrainedExecutorBackend: Got assigned task 373
21/01/31 13:51:52 INFO Executor: Running task 54.1 in stage 3.0 (TID 373)
21/01/31 13:51:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38621455
21/01/31 13:51:52 INFO MemoryStore: Will not store rdd_12_54
21/01/31 13:51:52 WARN MemoryStore: Not enough space to cache rdd_12_54 in memory! (computed 35.4 MiB so far)
21/01/31 13:51:52 INFO MemoryStore: Memory use = 344.1 MiB (blocks) + 2.9 MiB (scratch space shared across 1 tasks(s)) = 347.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:51:52 INFO BlockManager: Found block rdd_12_54 locally
21/01/31 13:51:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:51:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:51:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:51:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:51:52 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:51:52 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:51:52 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:51:52 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:51:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:51:52 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:51:52 INFO ParquetOutputFormat: Validation is off
21/01/31 13:51:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:51:52 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:51:52 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:51:52 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:51:52 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:51:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:51:52 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000038_306' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000038
21/01/31 13:51:52 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000038_306: Committed
21/01/31 13:51:52 INFO Executor: Finished task 38.1 in stage 3.0 (TID 306). 2504 bytes result sent to driver
21/01/31 13:51:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37779638
21/01/31 13:51:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000054_373' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000054
21/01/31 13:51:55 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000054_373: Committed
21/01/31 13:51:55 INFO Executor: Finished task 54.1 in stage 3.0 (TID 373). 2461 bytes result sent to driver
21/01/31 13:51:55 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
