Spark Executor Command: "/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/bin/java" "-cp" "/Users/kinsho/workspace/spark-3.0.1/conf/:/Users/kinsho/workspace/spark-3.0.1/assembly/target/scala-2.12/jars/*" "-Xmx1024M" "-Dspark.driver.port=51030" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.11.7:51030" "--executor-id" "3" "--hostname" "192.168.11.7" "--cores" "2" "--app-id" "app-20210131134107-0030" "--worker-url" "spark://Worker@192.168.11.7:63901"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/31 13:41:09 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 27466@ST000000035
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for TERM
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for HUP
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for INT
21/01/31 13:41:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/01/31 13:41:10 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 111 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 5 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO DiskBlockManager: Created local directory at /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-102929ca-e5d8-4f9d-8f5f-261f7d02820c/executor-361f7767-900f-4583-8f8a-4988bdafdad2/blockmgr-92ab6998-f2d6-4b88-83ab-8ecf2da244d5
21/01/31 13:41:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/01/31 13:41:11 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.11.7:51030
21/01/31 13:41:11 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.11.7:63901
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:63901 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO ResourceUtils: ==============================================================
21/01/31 13:41:11 INFO ResourceUtils: Resources for spark.executor:

21/01/31 13:41:11 INFO ResourceUtils: ==============================================================
21/01/31 13:41:11 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.11.7:63901
21/01/31 13:41:11 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
21/01/31 13:41:11 INFO Executor: Starting executor ID 3 on host 192.168.11.7
21/01/31 13:41:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51085.
21/01/31 13:41:12 INFO NettyBlockTransferService: Server created on 192.168.11.7:51085
21/01/31 13:41:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/31 13:41:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(3, 192.168.11.7, 51085, None)
21/01/31 13:41:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(3, 192.168.11.7, 51085, None)
21/01/31 13:41:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(3, 192.168.11.7, 51085, None)
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 8
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 18
21/01/31 13:41:16 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
21/01/31 13:41:16 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
21/01/31 13:41:16 INFO Executor: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar with timestamp 1612068067341
21/01/31 13:41:16 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 30 ms (0 ms spent in bootstraps)
21/01/31 13:41:16 INFO Utils: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar to /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-102929ca-e5d8-4f9d-8f5f-261f7d02820c/executor-361f7767-900f-4583-8f8a-4988bdafdad2/spark-b6e21840-eb4e-42a3-ba2a-f1e66cf4cb8a/fetchFileTemp5004183628566206123.tmp
21/01/31 13:41:16 INFO Utils: Copying /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-102929ca-e5d8-4f9d-8f5f-261f7d02820c/executor-361f7767-900f-4583-8f8a-4988bdafdad2/spark-b6e21840-eb4e-42a3-ba2a-f1e66cf4cb8a/16839229251612068067341_cache to /Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/3/./simple-project_2.12-1.0.jar
21/01/31 13:41:16 INFO Executor: Adding file:/Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/3/./simple-project_2.12-1.0.jar to class loader
21/01/31 13:41:16 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:17 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51036 after 9 ms (0 ms spent in bootstraps)
21/01/31 13:41:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 366.3 MiB)
21/01/31 13:41:17 INFO TorrentBroadcast: Reading broadcast variable 4 took 335 ms
21/01/31 13:41:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 43.3 KiB, free 366.2 MiB)
21/01/31 13:41:19 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 939524096-1073741824, partition values: [empty row]
21/01/31 13:41:19 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 2281701376-2415919104, partition values: [empty row]
21/01/31 13:41:21 INFO CodeGenerator: Code generated in 1116.737612 ms
21/01/31 13:41:21 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.2 MiB)
21/01/31 13:41:21 INFO TorrentBroadcast: Reading broadcast variable 3 took 10 ms
21/01/31 13:41:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 336.8 KiB, free 365.9 MiB)
21/01/31 13:41:49 INFO MemoryStore: Will not store rdd_12_7
21/01/31 13:41:49 WARN MemoryStore: Not enough space to cache rdd_12_7 in memory! (computed 137.6 MiB so far)
21/01/31 13:41:49 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 306.9 MiB (scratch space shared across 2 tasks(s)) = 307.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:41:49 WARN BlockManager: Persisting block rdd_12_7 to disk instead.
21/01/31 13:41:57 INFO MemoryStore: Will not store rdd_12_7
21/01/31 13:41:57 WARN MemoryStore: Not enough space to cache rdd_12_7 in memory! (computed 137.6 MiB so far)
21/01/31 13:41:57 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 306.9 MiB (scratch space shared across 2 tasks(s)) = 307.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:41:57 INFO CodeGenerator: Code generated in 8.517881 ms
21/01/31 13:41:57 INFO CodeGenerator: Code generated in 53.414013 ms
21/01/31 13:41:57 INFO CodeGenerator: Code generated in 24.63005 ms
21/01/31 13:41:57 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2151 bytes result sent to driver
21/01/31 13:41:57 INFO CoarseGrainedExecutorBackend: Got assigned task 27
21/01/31 13:41:57 INFO Executor: Running task 26.0 in stage 1.0 (TID 27)
21/01/31 13:41:57 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 3489660928-3623878656, partition values: [empty row]
21/01/31 13:41:59 INFO MemoryStore: Block rdd_12_17 stored as values in memory (estimated size 185.6 MiB, free 180.3 MiB)
21/01/31 13:41:59 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2108 bytes result sent to driver
21/01/31 13:41:59 INFO CoarseGrainedExecutorBackend: Got assigned task 31
21/01/31 13:41:59 INFO Executor: Running task 30.0 in stage 1.0 (TID 31)
21/01/31 13:41:59 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 4026531840-4160749568, partition values: [empty row]
21/01/31 13:42:12 INFO MemoryStore: Will not store rdd_12_30
21/01/31 13:42:12 WARN MemoryStore: Not enough space to cache rdd_12_30 in memory! (computed 69.2 MiB so far)
21/01/31 13:42:12 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 156.8 MiB (scratch space shared across 2 tasks(s)) = 342.8 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:12 WARN BlockManager: Persisting block rdd_12_30 to disk instead.
21/01/31 13:42:22 INFO MemoryStore: Will not store rdd_12_26
21/01/31 13:42:22 WARN MemoryStore: Not enough space to cache rdd_12_26 in memory! (computed 135.6 MiB so far)
21/01/31 13:42:22 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 103.7 MiB (scratch space shared across 1 tasks(s)) = 289.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:22 WARN BlockManager: Persisting block rdd_12_26 to disk instead.
21/01/31 13:42:32 INFO MemoryStore: Will not store rdd_12_26
21/01/31 13:42:32 WARN MemoryStore: Not enough space to cache rdd_12_26 in memory! (computed 135.6 MiB so far)
21/01/31 13:42:32 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 103.7 MiB (scratch space shared across 1 tasks(s)) = 289.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:32 INFO Executor: Finished task 26.0 in stage 1.0 (TID 27). 2108 bytes result sent to driver
21/01/31 13:42:32 INFO CoarseGrainedExecutorBackend: Got assigned task 47
21/01/31 13:42:32 INFO Executor: Running task 46.0 in stage 1.0 (TID 47)
21/01/31 13:42:32 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 6174015488-6308233216, partition values: [empty row]
21/01/31 13:42:34 INFO MemoryStore: Will not store rdd_12_30
21/01/31 13:42:34 WARN MemoryStore: Not enough space to cache rdd_12_30 in memory! (computed 136.1 MiB so far)
21/01/31 13:42:34 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 106.8 MiB (scratch space shared across 2 tasks(s)) = 292.8 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:34 INFO Executor: Finished task 30.0 in stage 1.0 (TID 31). 2108 bytes result sent to driver
21/01/31 13:42:34 INFO CoarseGrainedExecutorBackend: Got assigned task 54
21/01/31 13:42:34 INFO Executor: Running task 53.0 in stage 1.0 (TID 54)
21/01/31 13:42:34 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 7113539584-7247757312, partition values: [empty row]
21/01/31 13:42:47 INFO MemoryStore: Will not store rdd_12_53
21/01/31 13:42:47 WARN MemoryStore: Not enough space to cache rdd_12_53 in memory! (computed 70.0 MiB so far)
21/01/31 13:42:47 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 157.3 MiB (scratch space shared across 2 tasks(s)) = 343.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:47 WARN BlockManager: Persisting block rdd_12_53 to disk instead.
21/01/31 13:42:57 INFO MemoryStore: Will not store rdd_12_46
21/01/31 13:42:57 WARN MemoryStore: Not enough space to cache rdd_12_46 in memory! (computed 136.5 MiB so far)
21/01/31 13:42:57 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 102.9 MiB (scratch space shared across 1 tasks(s)) = 288.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:57 WARN BlockManager: Persisting block rdd_12_46 to disk instead.
21/01/31 13:43:03 INFO MemoryStore: Will not store rdd_12_46
21/01/31 13:43:03 WARN MemoryStore: Not enough space to cache rdd_12_46 in memory! (computed 136.5 MiB so far)
21/01/31 13:43:03 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 102.9 MiB (scratch space shared across 1 tasks(s)) = 288.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:03 INFO Executor: Finished task 46.0 in stage 1.0 (TID 47). 2108 bytes result sent to driver
21/01/31 13:43:03 INFO CoarseGrainedExecutorBackend: Got assigned task 65
21/01/31 13:43:03 INFO Executor: Running task 64.0 in stage 1.0 (TID 65)
21/01/31 13:43:03 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 8589934592-8724152320, partition values: [empty row]
21/01/31 13:43:06 INFO MemoryStore: Will not store rdd_12_53
21/01/31 13:43:06 WARN MemoryStore: Not enough space to cache rdd_12_53 in memory! (computed 137.2 MiB so far)
21/01/31 13:43:06 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 108.2 MiB (scratch space shared across 2 tasks(s)) = 294.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:06 INFO Executor: Finished task 53.0 in stage 1.0 (TID 54). 2108 bytes result sent to driver
21/01/31 13:43:06 INFO CoarseGrainedExecutorBackend: Got assigned task 75
21/01/31 13:43:06 INFO Executor: Running task 74.0 in stage 1.0 (TID 75)
21/01/31 13:43:06 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 9932111872-10066329600, partition values: [empty row]
21/01/31 13:43:19 INFO MemoryStore: Will not store rdd_12_74
21/01/31 13:43:19 WARN MemoryStore: Not enough space to cache rdd_12_74 in memory! (computed 68.3 MiB so far)
21/01/31 13:43:19 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 158.2 MiB (scratch space shared across 2 tasks(s)) = 344.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:19 WARN BlockManager: Persisting block rdd_12_74 to disk instead.
21/01/31 13:43:28 INFO MemoryStore: Will not store rdd_12_64
21/01/31 13:43:28 WARN MemoryStore: Not enough space to cache rdd_12_64 in memory! (computed 137.9 MiB so far)
21/01/31 13:43:28 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 104.7 MiB (scratch space shared across 1 tasks(s)) = 290.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:28 WARN BlockManager: Persisting block rdd_12_64 to disk instead.
21/01/31 13:43:34 INFO MemoryStore: Will not store rdd_12_64
21/01/31 13:43:34 WARN MemoryStore: Not enough space to cache rdd_12_64 in memory! (computed 137.9 MiB so far)
21/01/31 13:43:34 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 104.7 MiB (scratch space shared across 1 tasks(s)) = 290.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:34 INFO Executor: Finished task 64.0 in stage 1.0 (TID 65). 2108 bytes result sent to driver
21/01/31 13:43:34 INFO CoarseGrainedExecutorBackend: Got assigned task 85
21/01/31 13:43:34 INFO Executor: Running task 84.0 in stage 1.0 (TID 85)
21/01/31 13:43:34 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 11274289152-11408506880, partition values: [empty row]
21/01/31 13:43:37 INFO MemoryStore: Will not store rdd_12_74
21/01/31 13:43:37 WARN MemoryStore: Not enough space to cache rdd_12_74 in memory! (computed 135.7 MiB so far)
21/01/31 13:43:37 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 105.5 MiB (scratch space shared across 2 tasks(s)) = 291.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:38 INFO Executor: Finished task 74.0 in stage 1.0 (TID 75). 2108 bytes result sent to driver
21/01/31 13:43:38 INFO CoarseGrainedExecutorBackend: Got assigned task 96
21/01/31 13:43:38 INFO Executor: Running task 95.0 in stage 1.0 (TID 96)
21/01/31 13:43:38 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 12750684160-12884901888, partition values: [empty row]
21/01/31 13:43:50 INFO MemoryStore: Will not store rdd_12_95
21/01/31 13:43:50 WARN MemoryStore: Not enough space to cache rdd_12_95 in memory! (computed 68.0 MiB so far)
21/01/31 13:43:50 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 154.0 MiB (scratch space shared across 2 tasks(s)) = 340.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:50 WARN BlockManager: Persisting block rdd_12_95 to disk instead.
21/01/31 13:43:59 INFO MemoryStore: Will not store rdd_12_84
21/01/31 13:43:59 WARN MemoryStore: Not enough space to cache rdd_12_84 in memory! (computed 134.0 MiB so far)
21/01/31 13:43:59 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 101.1 MiB (scratch space shared across 1 tasks(s)) = 287.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:59 WARN BlockManager: Persisting block rdd_12_84 to disk instead.
21/01/31 13:44:05 INFO MemoryStore: Will not store rdd_12_84
21/01/31 13:44:05 WARN MemoryStore: Not enough space to cache rdd_12_84 in memory! (computed 134.0 MiB so far)
21/01/31 13:44:05 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 101.1 MiB (scratch space shared across 1 tasks(s)) = 287.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:05 INFO Executor: Finished task 84.0 in stage 1.0 (TID 85). 2108 bytes result sent to driver
21/01/31 13:44:05 INFO CoarseGrainedExecutorBackend: Got assigned task 105
21/01/31 13:44:05 INFO Executor: Running task 104.0 in stage 1.0 (TID 105)
21/01/31 13:44:05 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 13958643712-14092861440, partition values: [empty row]
21/01/31 13:44:09 INFO MemoryStore: Will not store rdd_12_95
21/01/31 13:44:09 WARN MemoryStore: Not enough space to cache rdd_12_95 in memory! (computed 132.3 MiB so far)
21/01/31 13:44:09 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 105.0 MiB (scratch space shared across 2 tasks(s)) = 291.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:09 INFO Executor: Finished task 95.0 in stage 1.0 (TID 96). 2108 bytes result sent to driver
21/01/31 13:44:09 INFO CoarseGrainedExecutorBackend: Got assigned task 116
21/01/31 13:44:09 INFO Executor: Running task 115.0 in stage 1.0 (TID 116)
21/01/31 13:44:09 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 15435038720-15569256448, partition values: [empty row]
21/01/31 13:44:22 INFO MemoryStore: Will not store rdd_12_115
21/01/31 13:44:22 WARN MemoryStore: Not enough space to cache rdd_12_115 in memory! (computed 68.3 MiB so far)
21/01/31 13:44:22 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 154.2 MiB (scratch space shared across 2 tasks(s)) = 340.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:22 WARN BlockManager: Persisting block rdd_12_115 to disk instead.
21/01/31 13:44:31 INFO MemoryStore: Will not store rdd_12_104
21/01/31 13:44:31 WARN MemoryStore: Not enough space to cache rdd_12_104 in memory! (computed 133.9 MiB so far)
21/01/31 13:44:31 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 101.6 MiB (scratch space shared across 1 tasks(s)) = 287.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:31 WARN BlockManager: Persisting block rdd_12_104 to disk instead.
21/01/31 13:44:36 INFO MemoryStore: Will not store rdd_12_104
21/01/31 13:44:36 WARN MemoryStore: Not enough space to cache rdd_12_104 in memory! (computed 133.9 MiB so far)
21/01/31 13:44:36 INFO MemoryStore: Memory use = 186.0 MiB (blocks) + 101.6 MiB (scratch space shared across 1 tasks(s)) = 287.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:36 INFO Executor: Finished task 104.0 in stage 1.0 (TID 105). 2108 bytes result sent to driver
21/01/31 13:44:36 INFO CoarseGrainedExecutorBackend: Got assigned task 124
21/01/31 13:44:36 INFO Executor: Running task 123.0 in stage 1.0 (TID 124)
21/01/31 13:44:36 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 16508780544-16642998272, partition values: [empty row]
21/01/31 13:44:39 INFO MemoryStore: Block rdd_12_115 stored as values in memory (estimated size 152.2 MiB, free 28.2 MiB)
21/01/31 13:44:39 INFO Executor: Finished task 115.0 in stage 1.0 (TID 116). 2108 bytes result sent to driver
21/01/31 13:44:39 INFO CoarseGrainedExecutorBackend: Got assigned task 133
21/01/31 13:44:39 INFO Executor: Running task 132.0 in stage 1.0 (TID 133)
21/01/31 13:44:39 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 17716740096-17850957824, partition values: [empty row]
21/01/31 13:44:43 INFO MemoryStore: Will not store rdd_12_123
21/01/31 13:44:43 WARN MemoryStore: Not enough space to cache rdd_12_123 in memory! (computed 35.1 MiB so far)
21/01/31 13:44:43 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 344.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:43 WARN BlockManager: Persisting block rdd_12_123 to disk instead.
21/01/31 13:44:46 INFO MemoryStore: Will not store rdd_12_132
21/01/31 13:44:46 WARN MemoryStore: Not enough space to cache rdd_12_132 in memory! (computed 36.1 MiB so far)
21/01/31 13:44:46 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:46 WARN BlockManager: Persisting block rdd_12_132 to disk instead.
21/01/31 13:45:07 INFO MemoryStore: Will not store rdd_12_123
21/01/31 13:45:07 WARN MemoryStore: Not enough space to cache rdd_12_123 in memory! (computed 35.1 MiB so far)
21/01/31 13:45:07 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 341.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:07 INFO Executor: Finished task 123.0 in stage 1.0 (TID 124). 2108 bytes result sent to driver
21/01/31 13:45:07 INFO CoarseGrainedExecutorBackend: Got assigned task 145
21/01/31 13:45:07 INFO Executor: Running task 144.0 in stage 1.0 (TID 145)
21/01/31 13:45:07 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 19327352832-19461570560, partition values: [empty row]
21/01/31 13:45:11 INFO MemoryStore: Will not store rdd_12_132
21/01/31 13:45:11 WARN MemoryStore: Not enough space to cache rdd_12_132 in memory! (computed 36.1 MiB so far)
21/01/31 13:45:11 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 344.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:11 INFO Executor: Finished task 132.0 in stage 1.0 (TID 133). 2108 bytes result sent to driver
21/01/31 13:45:11 INFO CoarseGrainedExecutorBackend: Got assigned task 154
21/01/31 13:45:11 INFO Executor: Running task 153.0 in stage 1.0 (TID 154)
21/01/31 13:45:11 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 20535312384-20669530112, partition values: [empty row]
21/01/31 13:45:15 INFO MemoryStore: Will not store rdd_12_144
21/01/31 13:45:15 WARN MemoryStore: Not enough space to cache rdd_12_144 in memory! (computed 36.1 MiB so far)
21/01/31 13:45:15 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 344.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:15 WARN BlockManager: Persisting block rdd_12_144 to disk instead.
21/01/31 13:45:18 INFO MemoryStore: Will not store rdd_12_153
21/01/31 13:45:18 WARN MemoryStore: Not enough space to cache rdd_12_153 in memory! (computed 37.1 MiB so far)
21/01/31 13:45:18 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:18 WARN BlockManager: Persisting block rdd_12_153 to disk instead.
21/01/31 13:45:40 INFO MemoryStore: Will not store rdd_12_144
21/01/31 13:45:40 WARN MemoryStore: Not enough space to cache rdd_12_144 in memory! (computed 36.1 MiB so far)
21/01/31 13:45:40 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 341.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:40 INFO Executor: Finished task 144.0 in stage 1.0 (TID 145). 2108 bytes result sent to driver
21/01/31 13:45:40 INFO CoarseGrainedExecutorBackend: Got assigned task 169
21/01/31 13:45:40 INFO Executor: Running task 168.0 in stage 1.0 (TID 169)
21/01/31 13:45:40 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22548578304-22682796032, partition values: [empty row]
21/01/31 13:45:41 INFO MemoryStore: Will not store rdd_12_153
21/01/31 13:45:41 WARN MemoryStore: Not enough space to cache rdd_12_153 in memory! (computed 37.1 MiB so far)
21/01/31 13:45:41 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 6.7 MiB (scratch space shared across 2 tasks(s)) = 344.8 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:41 INFO Executor: Finished task 153.0 in stage 1.0 (TID 154). 2108 bytes result sent to driver
21/01/31 13:45:41 INFO CoarseGrainedExecutorBackend: Got assigned task 174
21/01/31 13:45:41 INFO Executor: Running task 173.0 in stage 1.0 (TID 174)
21/01/31 13:45:41 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 23219666944-23353884672, partition values: [empty row]
21/01/31 13:45:47 INFO MemoryStore: Will not store rdd_12_168
21/01/31 13:45:47 WARN MemoryStore: Not enough space to cache rdd_12_168 in memory! (computed 37.5 MiB so far)
21/01/31 13:45:47 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 6.7 MiB (scratch space shared across 2 tasks(s)) = 344.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:47 WARN BlockManager: Persisting block rdd_12_168 to disk instead.
21/01/31 13:45:48 INFO MemoryStore: Will not store rdd_12_173
21/01/31 13:45:48 WARN MemoryStore: Not enough space to cache rdd_12_173 in memory! (computed 37.6 MiB so far)
21/01/31 13:45:48 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:48 WARN BlockManager: Persisting block rdd_12_173 to disk instead.
21/01/31 13:46:03 INFO MemoryStore: Will not store rdd_12_168
21/01/31 13:46:03 WARN MemoryStore: Not enough space to cache rdd_12_168 in memory! (computed 37.5 MiB so far)
21/01/31 13:46:03 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 3.4 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:03 INFO Executor: Finished task 168.0 in stage 1.0 (TID 169). 2108 bytes result sent to driver
21/01/31 13:46:03 INFO MemoryStore: Will not store rdd_12_173
21/01/31 13:46:03 WARN MemoryStore: Not enough space to cache rdd_12_173 in memory! (computed 37.6 MiB so far)
21/01/31 13:46:03 INFO MemoryStore: Memory use = 338.1 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:03 INFO Executor: Finished task 173.0 in stage 1.0 (TID 174). 2108 bytes result sent to driver
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 187
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 197
21/01/31 13:46:04 INFO Executor: Running task 7.0 in stage 3.0 (TID 187)
21/01/31 13:46:04 INFO Executor: Running task 17.0 in stage 3.0 (TID 197)
21/01/31 13:46:04 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
21/01/31 13:46:04 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:46:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 69.4 KiB, free 28.1 MiB)
21/01/31 13:46:04 INFO TorrentBroadcast: Reading broadcast variable 6 took 24 ms
21/01/31 13:46:05 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 194.6 KiB, free 27.9 MiB)
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_17 locally
21/01/31 13:46:05 INFO MemoryStore: Will not store rdd_12_7
21/01/31 13:46:05 WARN MemoryStore: Not enough space to cache rdd_12_7 in memory! (computed 36.2 MiB so far)
21/01/31 13:46:05 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 341.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_7 locally
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 205.951931 ms
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 209.49093 ms
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39131394
21/01/31 13:46:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 42911418
21/01/31 13:46:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000007_187' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000007
21/01/31 13:46:17 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000007_187: Committed
21/01/31 13:46:17 INFO Executor: Finished task 7.0 in stage 3.0 (TID 187). 2504 bytes result sent to driver
21/01/31 13:46:17 INFO CoarseGrainedExecutorBackend: Got assigned task 200
21/01/31 13:46:17 INFO Executor: Running task 26.0 in stage 3.0 (TID 200)
21/01/31 13:46:17 INFO MemoryStore: Will not store rdd_12_26
21/01/31 13:46:17 WARN MemoryStore: Not enough space to cache rdd_12_26 in memory! (computed 35.3 MiB so far)
21/01/31 13:46:17 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:17 INFO BlockManager: Found block rdd_12_26 locally
21/01/31 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:17 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:17 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:17 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:17 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:17 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:17 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000017_197' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000017
21/01/31 13:46:17 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000017_197: Committed
21/01/31 13:46:17 INFO Executor: Finished task 17.0 in stage 3.0 (TID 197). 2461 bytes result sent to driver
21/01/31 13:46:17 INFO CoarseGrainedExecutorBackend: Got assigned task 201
21/01/31 13:46:17 INFO Executor: Running task 30.0 in stage 3.0 (TID 201)
21/01/31 13:46:17 INFO MemoryStore: Will not store rdd_12_30
21/01/31 13:46:17 WARN MemoryStore: Not enough space to cache rdd_12_30 in memory! (computed 35.4 MiB so far)
21/01/31 13:46:17 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 344.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:17 INFO BlockManager: Found block rdd_12_30 locally
21/01/31 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:17 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:17 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:17 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:17 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:17 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:18 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:46 WARN Utils: Suppressing exception in catch: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:46:46 ERROR Utils: Uncaught exception in thread executor-heartbeater
java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:46:46 ERROR Executor: Exception in task 26.0 in stage 3.0 (TID 200)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.unsafe.types.UTF8String.fromBytes(UTF8String.java:126)
	at org.apache.spark.sql.execution.columnar.STRING$.extract(ColumnType.scala:447)
	at org.apache.spark.sql.execution.columnar.STRING$.extract(ColumnType.scala:431)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder.$anonfun$new$3(compressionSchemes.scala:478)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder$$Lambda$969/2053609293.apply$mcVI$sp(Unknown Source)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$Decoder.<init>(compressionSchemes.scala:477)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:369)
	at org.apache.spark.sql.execution.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:361)
	at org.apache.spark.sql.execution.columnar.compression.CompressibleColumnAccessor.initialize(CompressibleColumnAccessor.scala:32)
	at org.apache.spark.sql.execution.columnar.compression.CompressibleColumnAccessor.initialize$(CompressibleColumnAccessor.scala:30)
	at org.apache.spark.sql.execution.columnar.NativeColumnAccessor.initialize(ColumnAccessor.scala:73)
	at org.apache.spark.sql.execution.columnar.ColumnAccessor.$init$(ColumnAccessor.scala:38)
	at org.apache.spark.sql.execution.columnar.BasicColumnAccessor.<init>(ColumnAccessor.scala:52)
	at org.apache.spark.sql.execution.columnar.NativeColumnAccessor.<init>(ColumnAccessor.scala:76)
	at org.apache.spark.sql.execution.columnar.StringColumnAccessor.<init>(ColumnAccessor.scala:102)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1025/33810262.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$963/1740031907.apply(Unknown Source)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$423/535358765.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
21/01/31 13:46:46 ERROR Executor: Exception in task 30.0 in stage 3.0 (TID 201)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.BootstrapMethodError: call site initialization exception
	at java.lang.invoke.CallSite.makeSite(CallSite.java:341)
	at java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:307)
	at java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:297)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1427)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	... 9 more
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:46:47 INFO CoarseGrainedExecutorBackend: Got assigned task 230
21/01/31 13:46:47 INFO Executor: Running task 46.0 in stage 3.0 (TID 230)
21/01/31 13:46:47 INFO CoarseGrainedExecutorBackend: Got assigned task 231
21/01/31 13:46:47 INFO Executor: Running task 53.0 in stage 3.0 (TID 231)
21/01/31 13:46:47 INFO MemoryStore: Will not store rdd_12_46
21/01/31 13:46:47 WARN MemoryStore: Not enough space to cache rdd_12_46 in memory! (computed 34.8 MiB so far)
21/01/31 13:46:47 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 344.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:47 INFO BlockManager: Found block rdd_12_46 locally
21/01/31 13:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:47 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:47 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:47 INFO MemoryStore: Will not store rdd_12_53
21/01/31 13:46:47 WARN MemoryStore: Not enough space to cache rdd_12_53 in memory! (computed 36.3 MiB so far)
21/01/31 13:46:47 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 344.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:47 INFO BlockManager: Found block rdd_12_53 locally
21/01/31 13:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:47 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:47 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:47 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:47 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:47 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:48 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:47:06 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:47:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28342546
21/01/31 13:47:09 ERROR Executor: Exception in task 53.0 in stage 3.0 (TID 231)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:47:09 INFO CoarseGrainedExecutorBackend: Got assigned task 249
21/01/31 13:47:09 INFO Executor: Running task 26.1 in stage 3.0 (TID 249)
21/01/31 13:47:09 INFO MemoryStore: Will not store rdd_12_26
21/01/31 13:47:09 WARN MemoryStore: Not enough space to cache rdd_12_26 in memory! (computed 35.3 MiB so far)
21/01/31 13:47:09 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:09 INFO BlockManager: Found block rdd_12_26 locally
21/01/31 13:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:09 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:09 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37635492
21/01/31 13:47:12 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000046_230' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000046
21/01/31 13:47:12 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000046_230: Committed
21/01/31 13:47:12 INFO Executor: Finished task 46.0 in stage 3.0 (TID 230). 2504 bytes result sent to driver
21/01/31 13:47:12 INFO CoarseGrainedExecutorBackend: Got assigned task 259
21/01/31 13:47:12 INFO Executor: Running task 53.1 in stage 3.0 (TID 259)
21/01/31 13:47:13 INFO MemoryStore: Will not store rdd_12_53
21/01/31 13:47:13 WARN MemoryStore: Not enough space to cache rdd_12_53 in memory! (computed 36.3 MiB so far)
21/01/31 13:47:13 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 341.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:13 INFO BlockManager: Found block rdd_12_53 locally
21/01/31 13:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:13 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:13 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:13 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:13 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:13 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:13 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:13 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:13 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:13 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:25 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 23399291
21/01/31 13:47:29 WARN Utils: Suppressing exception in catch: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
21/01/31 13:47:29 ERROR Executor: Exception in task 53.1 in stage 3.0 (TID 259)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:29 INFO CoarseGrainedExecutorBackend: Got assigned task 274
21/01/31 13:47:29 INFO Executor: Running task 30.1 in stage 3.0 (TID 274)
21/01/31 13:47:30 INFO MemoryStore: Will not store rdd_12_30
21/01/31 13:47:30 WARN MemoryStore: Not enough space to cache rdd_12_30 in memory! (computed 35.4 MiB so far)
21/01/31 13:47:30 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:30 INFO BlockManager: Found block rdd_12_30 locally
21/01/31 13:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:30 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:30 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 40946882
21/01/31 13:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000026_249' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000026
21/01/31 13:47:32 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000026_249: Committed
21/01/31 13:47:32 INFO Executor: Finished task 26.1 in stage 3.0 (TID 249). 2504 bytes result sent to driver
21/01/31 13:47:32 INFO CoarseGrainedExecutorBackend: Got assigned task 275
21/01/31 13:47:32 INFO Executor: Running task 53.2 in stage 3.0 (TID 275)
21/01/31 13:47:32 INFO MemoryStore: Will not store rdd_12_53
21/01/31 13:47:32 WARN MemoryStore: Not enough space to cache rdd_12_53 in memory! (computed 36.3 MiB so far)
21/01/31 13:47:32 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 341.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:32 INFO BlockManager: Found block rdd_12_53 locally
21/01/31 13:47:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:32 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:32 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:32 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:32 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:32 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:32 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:32 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:49 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34085283
21/01/31 13:47:58 ERROR Executor: Exception in task 30.1 in stage 3.0 (TID 274)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
21/01/31 13:47:58 INFO CoarseGrainedExecutorBackend: Got assigned task 308
21/01/31 13:47:58 INFO Executor: Running task 64.0 in stage 3.0 (TID 308)
21/01/31 13:47:59 INFO MemoryStore: Will not store rdd_12_64
21/01/31 13:47:59 WARN MemoryStore: Not enough space to cache rdd_12_64 in memory! (computed 35.8 MiB so far)
21/01/31 13:47:59 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 341.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:59 INFO BlockManager: Found block rdd_12_64 locally
21/01/31 13:47:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:59 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:59 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:59 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:59 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:59 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:59 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:59 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:59 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:59 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38347548
21/01/31 13:48:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000053_275' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000053
21/01/31 13:48:03 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000053_275: Committed
21/01/31 13:48:03 INFO Executor: Finished task 53.2 in stage 3.0 (TID 275). 2504 bytes result sent to driver
21/01/31 13:48:03 INFO CoarseGrainedExecutorBackend: Got assigned task 314
21/01/31 13:48:03 INFO Executor: Running task 30.2 in stage 3.0 (TID 314)
21/01/31 13:48:04 INFO MemoryStore: Will not store rdd_12_30
21/01/31 13:48:04 WARN MemoryStore: Not enough space to cache rdd_12_30 in memory! (computed 35.4 MiB so far)
21/01/31 13:48:04 INFO MemoryStore: Memory use = 338.3 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 341.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:04 INFO BlockManager: Found block rdd_12_30 locally
21/01/31 13:48:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:04 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:04 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:04 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:04 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:04 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:04 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:04 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:04 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:04 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:11 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:48:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37012505
21/01/31 13:49:01 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 13:49:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 20071976
