Spark Executor Command: "/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/bin/java" "-cp" "/Users/kinsho/workspace/spark-3.0.1/conf/:/Users/kinsho/workspace/spark-3.0.1/assembly/target/scala-2.12/jars/*" "-Xmx1024M" "-Dspark.driver.port=51030" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.11.7:51030" "--executor-id" "7" "--hostname" "192.168.11.7" "--cores" "2" "--app-id" "app-20210131134107-0030" "--worker-url" "spark://Worker@192.168.11.7:63861"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/31 13:41:09 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 27472@ST000000035
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for TERM
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for HUP
21/01/31 13:41:09 INFO SignalUtils: Registered signal handler for INT
21/01/31 13:41:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/01/31 13:41:10 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:10 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 129 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 13:41:11 INFO SecurityManager: Changing view acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 13:41:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 13:41:11 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:41:11 INFO DiskBlockManager: Created local directory at /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-25de37d2-ff0b-4e7a-8da1-628ca4e62b74/executor-57808d34-5a4e-409a-acf9-8edee28926c2/blockmgr-6d0d822d-e608-437e-9f1f-119a1dda7c74
21/01/31 13:41:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/01/31 13:41:12 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.11.7:51030
21/01/31 13:41:12 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.11.7:63861
21/01/31 13:41:12 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:63861 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:41:12 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.11.7:63861
21/01/31 13:41:12 INFO ResourceUtils: ==============================================================
21/01/31 13:41:12 INFO ResourceUtils: Resources for spark.executor:

21/01/31 13:41:12 INFO ResourceUtils: ==============================================================
21/01/31 13:41:12 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
21/01/31 13:41:12 INFO Executor: Starting executor ID 7 on host 192.168.11.7
21/01/31 13:41:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51094.
21/01/31 13:41:12 INFO NettyBlockTransferService: Server created on 192.168.11.7:51094
21/01/31 13:41:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/31 13:41:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(7, 192.168.11.7, 51094, None)
21/01/31 13:41:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(7, 192.168.11.7, 51094, None)
21/01/31 13:41:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(7, 192.168.11.7, 51094, None)
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 6
21/01/31 13:41:16 INFO CoarseGrainedExecutorBackend: Got assigned task 16
21/01/31 13:41:16 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
21/01/31 13:41:16 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
21/01/31 13:41:16 INFO Executor: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar with timestamp 1612068067341
21/01/31 13:41:16 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51030 after 6 ms (0 ms spent in bootstraps)
21/01/31 13:41:16 INFO Utils: Fetching spark://192.168.11.7:51030/jars/simple-project_2.12-1.0.jar to /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-25de37d2-ff0b-4e7a-8da1-628ca4e62b74/executor-57808d34-5a4e-409a-acf9-8edee28926c2/spark-4eaeca66-419a-42d2-9c27-444fb6b31ead/fetchFileTemp557693953667640930.tmp
21/01/31 13:41:16 INFO Utils: Copying /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-25de37d2-ff0b-4e7a-8da1-628ca4e62b74/executor-57808d34-5a4e-409a-acf9-8edee28926c2/spark-4eaeca66-419a-42d2-9c27-444fb6b31ead/16839229251612068067341_cache to /Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/7/./simple-project_2.12-1.0.jar
21/01/31 13:41:16 INFO Executor: Adding file:/Users/kinsho/workspace/spark-3.0.1/work/app-20210131134107-0030/7/./simple-project_2.12-1.0.jar to class loader
21/01/31 13:41:17 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:17 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51096 after 5 ms (0 ms spent in bootstraps)
21/01/31 13:41:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 366.3 MiB)
21/01/31 13:41:17 INFO TorrentBroadcast: Reading broadcast variable 4 took 381 ms
21/01/31 13:41:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 43.3 KiB, free 366.2 MiB)
21/01/31 13:41:19 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 671088640-805306368, partition values: [empty row]
21/01/31 13:41:19 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 2013265920-2147483648, partition values: [empty row]
21/01/31 13:41:21 INFO CodeGenerator: Code generated in 1219.531992 ms
21/01/31 13:41:21 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:41:22 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51093 after 3 ms (0 ms spent in bootstraps)
21/01/31 13:41:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.2 MiB)
21/01/31 13:41:22 INFO TorrentBroadcast: Reading broadcast variable 3 took 69 ms
21/01/31 13:41:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 336.8 KiB, free 365.9 MiB)
21/01/31 13:41:35 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51222
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:35 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51223
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:35 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51225
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:37 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51230
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:37 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51231
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:37 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51232
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:38 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51242
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:38 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51243
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:38 WARN TransportChannelHandler: Exception in connection from /192.168.11.7:51244
java.lang.IllegalArgumentException: Too large frame: 5135603447292250188
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 13:41:50 INFO MemoryStore: Will not store rdd_12_15
21/01/31 13:41:50 WARN MemoryStore: Not enough space to cache rdd_12_15 in memory! (computed 135.7 MiB so far)
21/01/31 13:41:50 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 309.8 MiB (scratch space shared across 2 tasks(s)) = 310.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:41:50 WARN BlockManager: Persisting block rdd_12_15 to disk instead.
21/01/31 13:41:55 INFO MemoryStore: Block rdd_12_5 stored as values in memory (estimated size 164.8 MiB, free 201.1 MiB)
21/01/31 13:41:55 INFO CodeGenerator: Code generated in 8.988918 ms
21/01/31 13:41:55 INFO CodeGenerator: Code generated in 66.954522 ms
21/01/31 13:41:55 INFO CodeGenerator: Code generated in 21.136693 ms
21/01/31 13:41:55 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2151 bytes result sent to driver
21/01/31 13:41:55 INFO CoarseGrainedExecutorBackend: Got assigned task 26
21/01/31 13:41:55 INFO Executor: Running task 25.0 in stage 1.0 (TID 26)
21/01/31 13:41:55 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 3355443200-3489660928, partition values: [empty row]
21/01/31 13:42:00 INFO MemoryStore: Will not store rdd_12_15
21/01/31 13:42:00 WARN MemoryStore: Not enough space to cache rdd_12_15 in memory! (computed 135.7 MiB so far)
21/01/31 13:42:00 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 105.8 MiB (scratch space shared across 2 tasks(s)) = 271.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:01 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2108 bytes result sent to driver
21/01/31 13:42:01 INFO CoarseGrainedExecutorBackend: Got assigned task 36
21/01/31 13:42:01 INFO Executor: Running task 35.0 in stage 1.0 (TID 36)
21/01/31 13:42:01 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 4697620480-4831838208, partition values: [empty row]
21/01/31 13:42:13 INFO MemoryStore: Will not store rdd_12_35
21/01/31 13:42:13 WARN MemoryStore: Not enough space to cache rdd_12_35 in memory! (computed 69.2 MiB so far)
21/01/31 13:42:13 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 156.3 MiB (scratch space shared across 2 tasks(s)) = 321.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:13 WARN BlockManager: Persisting block rdd_12_35 to disk instead.
21/01/31 13:42:21 INFO MemoryStore: Will not store rdd_12_25
21/01/31 13:42:21 WARN MemoryStore: Not enough space to cache rdd_12_25 in memory! (computed 135.9 MiB so far)
21/01/31 13:42:21 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 103.2 MiB (scratch space shared across 1 tasks(s)) = 268.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:21 WARN BlockManager: Persisting block rdd_12_25 to disk instead.
21/01/31 13:42:30 INFO MemoryStore: Will not store rdd_12_25
21/01/31 13:42:30 WARN MemoryStore: Not enough space to cache rdd_12_25 in memory! (computed 135.9 MiB so far)
21/01/31 13:42:30 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 103.2 MiB (scratch space shared across 1 tasks(s)) = 268.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:30 INFO Executor: Finished task 25.0 in stage 1.0 (TID 26). 2108 bytes result sent to driver
21/01/31 13:42:30 INFO CoarseGrainedExecutorBackend: Got assigned task 44
21/01/31 13:42:30 INFO Executor: Running task 43.0 in stage 1.0 (TID 44)
21/01/31 13:42:30 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 5771362304-5905580032, partition values: [empty row]
21/01/31 13:42:34 INFO MemoryStore: Will not store rdd_12_35
21/01/31 13:42:34 WARN MemoryStore: Not enough space to cache rdd_12_35 in memory! (computed 136.5 MiB so far)
21/01/31 13:42:34 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 107.0 MiB (scratch space shared across 2 tasks(s)) = 272.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:34 INFO Executor: Finished task 35.0 in stage 1.0 (TID 36). 2108 bytes result sent to driver
21/01/31 13:42:34 INFO CoarseGrainedExecutorBackend: Got assigned task 52
21/01/31 13:42:34 INFO Executor: Running task 51.0 in stage 1.0 (TID 52)
21/01/31 13:42:34 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 6845104128-6979321856, partition values: [empty row]
21/01/31 13:42:46 INFO MemoryStore: Will not store rdd_12_51
21/01/31 13:42:46 WARN MemoryStore: Not enough space to cache rdd_12_51 in memory! (computed 69.7 MiB so far)
21/01/31 13:42:46 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 157.4 MiB (scratch space shared across 2 tasks(s)) = 322.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:46 WARN BlockManager: Persisting block rdd_12_51 to disk instead.
21/01/31 13:42:55 INFO MemoryStore: Will not store rdd_12_43
21/01/31 13:42:55 WARN MemoryStore: Not enough space to cache rdd_12_43 in memory! (computed 136.8 MiB so far)
21/01/31 13:42:55 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 103.3 MiB (scratch space shared across 1 tasks(s)) = 268.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:42:55 WARN BlockManager: Persisting block rdd_12_43 to disk instead.
21/01/31 13:43:01 INFO MemoryStore: Will not store rdd_12_43
21/01/31 13:43:01 WARN MemoryStore: Not enough space to cache rdd_12_43 in memory! (computed 136.8 MiB so far)
21/01/31 13:43:01 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 103.3 MiB (scratch space shared across 1 tasks(s)) = 268.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:01 INFO Executor: Finished task 43.0 in stage 1.0 (TID 44). 2108 bytes result sent to driver
21/01/31 13:43:01 INFO CoarseGrainedExecutorBackend: Got assigned task 63
21/01/31 13:43:01 INFO Executor: Running task 62.0 in stage 1.0 (TID 63)
21/01/31 13:43:01 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 8321499136-8455716864, partition values: [empty row]
21/01/31 13:43:04 INFO MemoryStore: Will not store rdd_12_51
21/01/31 13:43:04 WARN MemoryStore: Not enough space to cache rdd_12_51 in memory! (computed 137.2 MiB so far)
21/01/31 13:43:04 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 107.8 MiB (scratch space shared across 2 tasks(s)) = 273.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:04 INFO Executor: Finished task 51.0 in stage 1.0 (TID 52). 2108 bytes result sent to driver
21/01/31 13:43:04 INFO CoarseGrainedExecutorBackend: Got assigned task 70
21/01/31 13:43:04 INFO Executor: Running task 69.0 in stage 1.0 (TID 70)
21/01/31 13:43:04 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 9261023232-9395240960, partition values: [empty row]
21/01/31 13:43:17 INFO MemoryStore: Will not store rdd_12_69
21/01/31 13:43:17 WARN MemoryStore: Not enough space to cache rdd_12_69 in memory! (computed 70.2 MiB so far)
21/01/31 13:43:17 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 157.1 MiB (scratch space shared across 2 tasks(s)) = 322.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:17 WARN BlockManager: Persisting block rdd_12_69 to disk instead.
21/01/31 13:43:26 INFO MemoryStore: Will not store rdd_12_62
21/01/31 13:43:26 WARN MemoryStore: Not enough space to cache rdd_12_62 in memory! (computed 135.6 MiB so far)
21/01/31 13:43:26 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 102.8 MiB (scratch space shared across 1 tasks(s)) = 268.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:26 WARN BlockManager: Persisting block rdd_12_62 to disk instead.
21/01/31 13:43:32 INFO MemoryStore: Will not store rdd_12_62
21/01/31 13:43:32 WARN MemoryStore: Not enough space to cache rdd_12_62 in memory! (computed 135.6 MiB so far)
21/01/31 13:43:32 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 102.8 MiB (scratch space shared across 1 tasks(s)) = 268.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:32 INFO Executor: Finished task 62.0 in stage 1.0 (TID 63). 2108 bytes result sent to driver
21/01/31 13:43:32 INFO CoarseGrainedExecutorBackend: Got assigned task 82
21/01/31 13:43:32 INFO Executor: Running task 81.0 in stage 1.0 (TID 82)
21/01/31 13:43:32 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 10871635968-11005853696, partition values: [empty row]
21/01/31 13:43:35 INFO MemoryStore: Will not store rdd_12_69
21/01/31 13:43:35 WARN MemoryStore: Not enough space to cache rdd_12_69 in memory! (computed 138.7 MiB so far)
21/01/31 13:43:35 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 108.2 MiB (scratch space shared across 2 tasks(s)) = 273.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:35 INFO Executor: Finished task 69.0 in stage 1.0 (TID 70). 2108 bytes result sent to driver
21/01/31 13:43:35 INFO CoarseGrainedExecutorBackend: Got assigned task 86
21/01/31 13:43:35 INFO Executor: Running task 85.0 in stage 1.0 (TID 86)
21/01/31 13:43:35 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 11408506880-11542724608, partition values: [empty row]
21/01/31 13:43:48 INFO MemoryStore: Will not store rdd_12_85
21/01/31 13:43:48 WARN MemoryStore: Not enough space to cache rdd_12_85 in memory! (computed 67.2 MiB so far)
21/01/31 13:43:48 INFO MemoryStore: Memory use = 165.2 MiB (blocks) + 153.8 MiB (scratch space shared across 2 tasks(s)) = 319.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:43:48 WARN BlockManager: Persisting block rdd_12_85 to disk instead.
21/01/31 13:44:02 INFO MemoryStore: Block rdd_12_81 stored as values in memory (estimated size 158.9 MiB, free 42.3 MiB)
21/01/31 13:44:02 INFO Executor: Finished task 81.0 in stage 1.0 (TID 82). 2108 bytes result sent to driver
21/01/31 13:44:02 INFO CoarseGrainedExecutorBackend: Got assigned task 102
21/01/31 13:44:02 INFO Executor: Running task 101.0 in stage 1.0 (TID 102)
21/01/31 13:44:02 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 13555990528-13690208256, partition values: [empty row]
21/01/31 13:44:06 INFO MemoryStore: Will not store rdd_12_85
21/01/31 13:44:06 WARN MemoryStore: Not enough space to cache rdd_12_85 in memory! (computed 34.9 MiB so far)
21/01/31 13:44:06 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 5.9 MiB (scratch space shared across 2 tasks(s)) = 329.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:06 INFO Executor: Finished task 85.0 in stage 1.0 (TID 86). 2108 bytes result sent to driver
21/01/31 13:44:06 INFO CoarseGrainedExecutorBackend: Got assigned task 107
21/01/31 13:44:06 INFO Executor: Running task 106.0 in stage 1.0 (TID 107)
21/01/31 13:44:06 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 14227079168-14361296896, partition values: [empty row]
21/01/31 13:44:09 INFO MemoryStore: Will not store rdd_12_101
21/01/31 13:44:09 WARN MemoryStore: Not enough space to cache rdd_12_101 in memory! (computed 33.1 MiB so far)
21/01/31 13:44:09 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 5.8 MiB (scratch space shared across 2 tasks(s)) = 329.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:09 WARN BlockManager: Persisting block rdd_12_101 to disk instead.
21/01/31 13:44:13 INFO MemoryStore: Will not store rdd_12_106
21/01/31 13:44:13 WARN MemoryStore: Not enough space to cache rdd_12_106 in memory! (computed 33.6 MiB so far)
21/01/31 13:44:13 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.0 MiB (scratch space shared across 1 tasks(s)) = 327.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:13 WARN BlockManager: Persisting block rdd_12_106 to disk instead.
21/01/31 13:44:33 INFO MemoryStore: Will not store rdd_12_101
21/01/31 13:44:33 WARN MemoryStore: Not enough space to cache rdd_12_101 in memory! (computed 33.1 MiB so far)
21/01/31 13:44:33 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 2.8 MiB (scratch space shared across 1 tasks(s)) = 326.8 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:33 INFO Executor: Finished task 101.0 in stage 1.0 (TID 102). 2108 bytes result sent to driver
21/01/31 13:44:33 INFO CoarseGrainedExecutorBackend: Got assigned task 122
21/01/31 13:44:33 INFO Executor: Running task 121.0 in stage 1.0 (TID 122)
21/01/31 13:44:33 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 16240345088-16374562816, partition values: [empty row]
21/01/31 13:44:38 INFO MemoryStore: Will not store rdd_12_106
21/01/31 13:44:38 WARN MemoryStore: Not enough space to cache rdd_12_106 in memory! (computed 33.6 MiB so far)
21/01/31 13:44:38 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 6.1 MiB (scratch space shared across 2 tasks(s)) = 330.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:38 INFO Executor: Finished task 106.0 in stage 1.0 (TID 107). 2108 bytes result sent to driver
21/01/31 13:44:38 INFO CoarseGrainedExecutorBackend: Got assigned task 130
21/01/31 13:44:38 INFO Executor: Running task 129.0 in stage 1.0 (TID 130)
21/01/31 13:44:38 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 17314086912-17448304640, partition values: [empty row]
21/01/31 13:44:40 INFO MemoryStore: Will not store rdd_12_121
21/01/31 13:44:40 WARN MemoryStore: Not enough space to cache rdd_12_121 in memory! (computed 35.2 MiB so far)
21/01/31 13:44:40 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 330.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:40 WARN BlockManager: Persisting block rdd_12_121 to disk instead.
21/01/31 13:44:45 INFO MemoryStore: Will not store rdd_12_129
21/01/31 13:44:45 WARN MemoryStore: Not enough space to cache rdd_12_129 in memory! (computed 35.5 MiB so far)
21/01/31 13:44:45 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:44:45 WARN BlockManager: Persisting block rdd_12_129 to disk instead.
21/01/31 13:45:04 INFO MemoryStore: Will not store rdd_12_121
21/01/31 13:45:04 WARN MemoryStore: Not enough space to cache rdd_12_121 in memory! (computed 35.2 MiB so far)
21/01/31 13:45:04 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:04 INFO Executor: Finished task 121.0 in stage 1.0 (TID 122). 2108 bytes result sent to driver
21/01/31 13:45:04 INFO CoarseGrainedExecutorBackend: Got assigned task 142
21/01/31 13:45:04 INFO Executor: Running task 141.0 in stage 1.0 (TID 142)
21/01/31 13:45:04 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 18924699648-19058917376, partition values: [empty row]
21/01/31 13:45:09 INFO MemoryStore: Will not store rdd_12_129
21/01/31 13:45:09 WARN MemoryStore: Not enough space to cache rdd_12_129 in memory! (computed 35.5 MiB so far)
21/01/31 13:45:09 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 6.1 MiB (scratch space shared across 2 tasks(s)) = 330.2 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:09 INFO Executor: Finished task 129.0 in stage 1.0 (TID 130). 2108 bytes result sent to driver
21/01/31 13:45:09 INFO CoarseGrainedExecutorBackend: Got assigned task 149
21/01/31 13:45:09 INFO Executor: Running task 148.0 in stage 1.0 (TID 149)
21/01/31 13:45:09 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 19864223744-19998441472, partition values: [empty row]
21/01/31 13:45:12 INFO MemoryStore: Will not store rdd_12_141
21/01/31 13:45:12 WARN MemoryStore: Not enough space to cache rdd_12_141 in memory! (computed 36.3 MiB so far)
21/01/31 13:45:12 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 330.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:12 WARN BlockManager: Persisting block rdd_12_141 to disk instead.
21/01/31 13:45:16 INFO MemoryStore: Will not store rdd_12_148
21/01/31 13:45:16 WARN MemoryStore: Not enough space to cache rdd_12_148 in memory! (computed 36.4 MiB so far)
21/01/31 13:45:16 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 327.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:16 WARN BlockManager: Persisting block rdd_12_148 to disk instead.
21/01/31 13:45:35 INFO MemoryStore: Will not store rdd_12_141
21/01/31 13:45:35 WARN MemoryStore: Not enough space to cache rdd_12_141 in memory! (computed 36.3 MiB so far)
21/01/31 13:45:35 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:36 INFO Executor: Finished task 141.0 in stage 1.0 (TID 142). 2108 bytes result sent to driver
21/01/31 13:45:36 INFO CoarseGrainedExecutorBackend: Got assigned task 162
21/01/31 13:45:36 INFO Executor: Running task 161.0 in stage 1.0 (TID 162)
21/01/31 13:45:36 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 21609054208-21743271936, partition values: [empty row]
21/01/31 13:45:40 INFO MemoryStore: Will not store rdd_12_148
21/01/31 13:45:40 WARN MemoryStore: Not enough space to cache rdd_12_148 in memory! (computed 36.4 MiB so far)
21/01/31 13:45:40 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 330.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:40 INFO Executor: Finished task 148.0 in stage 1.0 (TID 149). 2108 bytes result sent to driver
21/01/31 13:45:40 INFO CoarseGrainedExecutorBackend: Got assigned task 170
21/01/31 13:45:40 INFO Executor: Running task 169.0 in stage 1.0 (TID 170)
21/01/31 13:45:40 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22682796032-22817013760, partition values: [empty row]
21/01/31 13:45:43 INFO MemoryStore: Will not store rdd_12_161
21/01/31 13:45:43 WARN MemoryStore: Not enough space to cache rdd_12_161 in memory! (computed 37.9 MiB so far)
21/01/31 13:45:43 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 330.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:43 WARN BlockManager: Persisting block rdd_12_161 to disk instead.
21/01/31 13:45:47 INFO MemoryStore: Will not store rdd_12_169
21/01/31 13:45:47 WARN MemoryStore: Not enough space to cache rdd_12_169 in memory! (computed 37.7 MiB so far)
21/01/31 13:45:47 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 327.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:45:47 WARN BlockManager: Persisting block rdd_12_169 to disk instead.
21/01/31 13:46:00 INFO MemoryStore: Will not store rdd_12_161
21/01/31 13:46:00 WARN MemoryStore: Not enough space to cache rdd_12_161 in memory! (computed 37.9 MiB so far)
21/01/31 13:46:00 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 327.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:00 INFO Executor: Finished task 161.0 in stage 1.0 (TID 162). 2108 bytes result sent to driver
21/01/31 13:46:03 INFO MemoryStore: Will not store rdd_12_169
21/01/31 13:46:03 WARN MemoryStore: Not enough space to cache rdd_12_169 in memory! (computed 37.7 MiB so far)
21/01/31 13:46:03 INFO MemoryStore: Memory use = 324.0 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 327.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:03 INFO Executor: Finished task 169.0 in stage 1.0 (TID 170). 2108 bytes result sent to driver
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 183
21/01/31 13:46:04 INFO CoarseGrainedExecutorBackend: Got assigned task 193
21/01/31 13:46:04 INFO Executor: Running task 5.0 in stage 3.0 (TID 183)
21/01/31 13:46:04 INFO Executor: Running task 15.0 in stage 3.0 (TID 193)
21/01/31 13:46:04 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
21/01/31 13:46:04 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 13:46:04 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:51036 after 2 ms (0 ms spent in bootstraps)
21/01/31 13:46:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 69.4 KiB, free 42.2 MiB)
21/01/31 13:46:04 INFO TorrentBroadcast: Reading broadcast variable 6 took 33 ms
21/01/31 13:46:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 194.6 KiB, free 42.0 MiB)
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_5 locally
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 314.147557 ms
21/01/31 13:46:05 INFO MemoryStore: Will not store rdd_12_15
21/01/31 13:46:05 WARN MemoryStore: Not enough space to cache rdd_12_15 in memory! (computed 35.2 MiB so far)
21/01/31 13:46:05 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:05 INFO BlockManager: Found block rdd_12_15 locally
21/01/31 13:46:05 INFO CodeGenerator: Code generated in 41.363904 ms
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:06 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 13:46:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37953958
21/01/31 13:46:18 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000005_183' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000005
21/01/31 13:46:18 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000005_183: Committed
21/01/31 13:46:18 INFO Executor: Finished task 5.0 in stage 3.0 (TID 183). 2504 bytes result sent to driver
21/01/31 13:46:18 INFO CoarseGrainedExecutorBackend: Got assigned task 205
21/01/31 13:46:18 INFO Executor: Running task 25.0 in stage 3.0 (TID 205)
21/01/31 13:46:18 INFO MemoryStore: Will not store rdd_12_25
21/01/31 13:46:18 WARN MemoryStore: Not enough space to cache rdd_12_25 in memory! (computed 35.2 MiB so far)
21/01/31 13:46:18 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:18 INFO BlockManager: Found block rdd_12_25 locally
21/01/31 13:46:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:18 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:18 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:18 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:18 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:18 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:18 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:18 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:18 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:18 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 41158218
21/01/31 13:46:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000015_193' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000015
21/01/31 13:46:20 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000015_193: Committed
21/01/31 13:46:20 INFO Executor: Finished task 15.0 in stage 3.0 (TID 193). 2461 bytes result sent to driver
21/01/31 13:46:20 INFO CoarseGrainedExecutorBackend: Got assigned task 215
21/01/31 13:46:20 INFO Executor: Running task 35.0 in stage 3.0 (TID 215)
21/01/31 13:46:20 INFO MemoryStore: Will not store rdd_12_35
21/01/31 13:46:20 WARN MemoryStore: Not enough space to cache rdd_12_35 in memory! (computed 35.4 MiB so far)
21/01/31 13:46:20 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 6.1 MiB (scratch space shared across 2 tasks(s)) = 330.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:20 INFO BlockManager: Found block rdd_12_35 locally
21/01/31 13:46:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:20 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:20 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 40610475
21/01/31 13:46:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000025_205' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000025
21/01/31 13:46:38 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000025_205: Committed
21/01/31 13:46:38 INFO Executor: Finished task 25.0 in stage 3.0 (TID 205). 2461 bytes result sent to driver
21/01/31 13:46:38 INFO CoarseGrainedExecutorBackend: Got assigned task 228
21/01/31 13:46:38 INFO Executor: Running task 43.0 in stage 3.0 (TID 228)
21/01/31 13:46:38 INFO MemoryStore: Will not store rdd_12_43
21/01/31 13:46:38 WARN MemoryStore: Not enough space to cache rdd_12_43 in memory! (computed 35.8 MiB so far)
21/01/31 13:46:38 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:38 INFO BlockManager: Found block rdd_12_43 locally
21/01/31 13:46:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39097802
21/01/31 13:46:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:38 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:38 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:38 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:38 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:38 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:38 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:38 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:38 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000035_215' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000035
21/01/31 13:46:38 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000035_215: Committed
21/01/31 13:46:38 INFO Executor: Finished task 35.0 in stage 3.0 (TID 215). 2461 bytes result sent to driver
21/01/31 13:46:38 INFO CoarseGrainedExecutorBackend: Got assigned task 229
21/01/31 13:46:38 INFO Executor: Running task 51.0 in stage 3.0 (TID 229)
21/01/31 13:46:39 INFO MemoryStore: Will not store rdd_12_51
21/01/31 13:46:39 WARN MemoryStore: Not enough space to cache rdd_12_51 in memory! (computed 36.1 MiB so far)
21/01/31 13:46:39 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 330.5 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:39 INFO BlockManager: Found block rdd_12_51 locally
21/01/31 13:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:39 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:39 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:39 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:39 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:39 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:39 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:39 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38175253
21/01/31 13:46:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000043_228' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000043
21/01/31 13:46:54 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000043_228: Committed
21/01/31 13:46:54 INFO Executor: Finished task 43.0 in stage 3.0 (TID 228). 2461 bytes result sent to driver
21/01/31 13:46:54 INFO CoarseGrainedExecutorBackend: Got assigned task 242
21/01/31 13:46:54 INFO Executor: Running task 62.0 in stage 3.0 (TID 242)
21/01/31 13:46:54 INFO MemoryStore: Will not store rdd_12_62
21/01/31 13:46:54 WARN MemoryStore: Not enough space to cache rdd_12_62 in memory! (computed 36.0 MiB so far)
21/01/31 13:46:54 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 327.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:54 INFO BlockManager: Found block rdd_12_62 locally
21/01/31 13:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:54 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:54 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:54 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:54 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:54 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:54 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:54 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:54 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:54 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:46:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37629367
21/01/31 13:46:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000051_229' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000051
21/01/31 13:46:55 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000051_229: Committed
21/01/31 13:46:55 INFO Executor: Finished task 51.0 in stage 3.0 (TID 229). 2461 bytes result sent to driver
21/01/31 13:46:55 INFO CoarseGrainedExecutorBackend: Got assigned task 245
21/01/31 13:46:55 INFO Executor: Running task 69.0 in stage 3.0 (TID 245)
21/01/31 13:46:55 INFO MemoryStore: Will not store rdd_12_69
21/01/31 13:46:55 WARN MemoryStore: Not enough space to cache rdd_12_69 in memory! (computed 36.2 MiB so far)
21/01/31 13:46:55 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 330.7 MiB. Storage limit = 366.3 MiB.
21/01/31 13:46:55 INFO BlockManager: Found block rdd_12_69 locally
21/01/31 13:46:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:46:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:46:55 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:55 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:46:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:46:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:46:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:46:55 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:46:55 INFO ParquetOutputFormat: Validation is off
21/01/31 13:46:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:46:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:46:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:46:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:46:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:46:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37284984
21/01/31 13:47:10 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000062_242' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000062
21/01/31 13:47:10 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000062_242: Committed
21/01/31 13:47:10 INFO Executor: Finished task 62.0 in stage 3.0 (TID 242). 2461 bytes result sent to driver
21/01/31 13:47:10 INFO CoarseGrainedExecutorBackend: Got assigned task 253
21/01/31 13:47:10 INFO Executor: Running task 81.0 in stage 3.0 (TID 253)
21/01/31 13:47:10 INFO BlockManager: Found block rdd_12_81 locally
21/01/31 13:47:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:10 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:10 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:10 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:10 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:10 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:10 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:10 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37090210
21/01/31 13:47:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000069_245' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000069
21/01/31 13:47:11 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000069_245: Committed
21/01/31 13:47:11 INFO Executor: Finished task 69.0 in stage 3.0 (TID 245). 2461 bytes result sent to driver
21/01/31 13:47:11 INFO CoarseGrainedExecutorBackend: Got assigned task 257
21/01/31 13:47:11 INFO Executor: Running task 85.0 in stage 3.0 (TID 257)
21/01/31 13:47:11 INFO MemoryStore: Will not store rdd_12_85
21/01/31 13:47:11 WARN MemoryStore: Not enough space to cache rdd_12_85 in memory! (computed 34.9 MiB so far)
21/01/31 13:47:11 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:11 INFO BlockManager: Found block rdd_12_85 locally
21/01/31 13:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:11 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:11 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:11 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:11 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:11 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:11 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:11 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:11 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:11 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:11 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36427733
21/01/31 13:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000081_253' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000081
21/01/31 13:47:23 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000081_253: Committed
21/01/31 13:47:23 INFO Executor: Finished task 81.0 in stage 3.0 (TID 253). 2461 bytes result sent to driver
21/01/31 13:47:23 INFO CoarseGrainedExecutorBackend: Got assigned task 262
21/01/31 13:47:23 INFO Executor: Running task 101.0 in stage 3.0 (TID 262)
21/01/31 13:47:23 INFO MemoryStore: Will not store rdd_12_101
21/01/31 13:47:23 WARN MemoryStore: Not enough space to cache rdd_12_101 in memory! (computed 33.1 MiB so far)
21/01/31 13:47:23 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 2.8 MiB (scratch space shared across 1 tasks(s)) = 327.0 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:23 INFO BlockManager: Found block rdd_12_101 locally
21/01/31 13:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:23 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:23 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:23 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:23 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:23 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:23 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36033742
21/01/31 13:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000085_257' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000085
21/01/31 13:47:25 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000085_257: Committed
21/01/31 13:47:25 INFO Executor: Finished task 85.0 in stage 3.0 (TID 257). 2461 bytes result sent to driver
21/01/31 13:47:25 INFO CoarseGrainedExecutorBackend: Got assigned task 267
21/01/31 13:47:25 INFO Executor: Running task 106.0 in stage 3.0 (TID 267)
21/01/31 13:47:25 INFO MemoryStore: Will not store rdd_12_106
21/01/31 13:47:25 WARN MemoryStore: Not enough space to cache rdd_12_106 in memory! (computed 33.6 MiB so far)
21/01/31 13:47:25 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 5.8 MiB (scratch space shared across 2 tasks(s)) = 330.1 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:25 INFO BlockManager: Found block rdd_12_106 locally
21/01/31 13:47:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:25 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:25 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:25 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:25 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:25 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34998704
21/01/31 13:47:35 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000101_262' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000101
21/01/31 13:47:35 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000101_262: Committed
21/01/31 13:47:35 INFO Executor: Finished task 101.0 in stage 3.0 (TID 262). 2461 bytes result sent to driver
21/01/31 13:47:35 INFO CoarseGrainedExecutorBackend: Got assigned task 278
21/01/31 13:47:35 INFO Executor: Running task 121.0 in stage 3.0 (TID 278)
21/01/31 13:47:35 INFO MemoryStore: Will not store rdd_12_121
21/01/31 13:47:35 WARN MemoryStore: Not enough space to cache rdd_12_121 in memory! (computed 35.2 MiB so far)
21/01/31 13:47:35 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:35 INFO BlockManager: Found block rdd_12_121 locally
21/01/31 13:47:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:36 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:36 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:36 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:36 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:36 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:36 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:36 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:36 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:36 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:36 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:36 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34669200
21/01/31 13:47:37 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000106_267' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000106
21/01/31 13:47:37 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000106_267: Committed
21/01/31 13:47:37 INFO Executor: Finished task 106.0 in stage 3.0 (TID 267). 2461 bytes result sent to driver
21/01/31 13:47:37 INFO CoarseGrainedExecutorBackend: Got assigned task 279
21/01/31 13:47:37 INFO Executor: Running task 129.0 in stage 3.0 (TID 279)
21/01/31 13:47:37 INFO MemoryStore: Will not store rdd_12_129
21/01/31 13:47:37 WARN MemoryStore: Not enough space to cache rdd_12_129 in memory! (computed 35.5 MiB so far)
21/01/31 13:47:37 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 330.4 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:37 INFO BlockManager: Found block rdd_12_129 locally
21/01/31 13:47:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:37 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:37 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:37 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:37 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:37 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:37 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:37 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:37 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:37 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35465607
21/01/31 13:47:48 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000121_278' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000121
21/01/31 13:47:48 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000121_278: Committed
21/01/31 13:47:48 INFO Executor: Finished task 121.0 in stage 3.0 (TID 278). 2461 bytes result sent to driver
21/01/31 13:47:48 INFO CoarseGrainedExecutorBackend: Got assigned task 295
21/01/31 13:47:48 INFO Executor: Running task 141.0 in stage 3.0 (TID 295)
21/01/31 13:47:48 INFO MemoryStore: Will not store rdd_12_141
21/01/31 13:47:48 WARN MemoryStore: Not enough space to cache rdd_12_141 in memory! (computed 36.3 MiB so far)
21/01/31 13:47:48 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 327.3 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:48 INFO BlockManager: Found block rdd_12_141 locally
21/01/31 13:47:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:48 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:48 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:48 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:48 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:48 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:48 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:48 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:48 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:48 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:48 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35417947
21/01/31 13:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000129_279' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000129
21/01/31 13:47:50 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000129_279: Committed
21/01/31 13:47:50 INFO Executor: Finished task 129.0 in stage 3.0 (TID 279). 2461 bytes result sent to driver
21/01/31 13:47:50 INFO CoarseGrainedExecutorBackend: Got assigned task 297
21/01/31 13:47:50 INFO Executor: Running task 148.0 in stage 3.0 (TID 297)
21/01/31 13:47:50 INFO MemoryStore: Will not store rdd_12_148
21/01/31 13:47:50 WARN MemoryStore: Not enough space to cache rdd_12_148 in memory! (computed 36.4 MiB so far)
21/01/31 13:47:50 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 330.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:47:50 INFO BlockManager: Found block rdd_12_148 locally
21/01/31 13:47:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:47:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:47:50 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:50 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:47:50 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:47:50 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:47:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:47:50 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:47:50 INFO ParquetOutputFormat: Validation is off
21/01/31 13:47:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:47:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:47:50 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:47:50 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:47:50 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:47:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35744831
21/01/31 13:48:01 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000141_295' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000141
21/01/31 13:48:01 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000141_295: Committed
21/01/31 13:48:01 INFO Executor: Finished task 141.0 in stage 3.0 (TID 295). 2461 bytes result sent to driver
21/01/31 13:48:01 INFO CoarseGrainedExecutorBackend: Got assigned task 312
21/01/31 13:48:01 INFO Executor: Running task 161.0 in stage 3.0 (TID 312)
21/01/31 13:48:01 INFO MemoryStore: Will not store rdd_12_161
21/01/31 13:48:01 WARN MemoryStore: Not enough space to cache rdd_12_161 in memory! (computed 37.9 MiB so far)
21/01/31 13:48:01 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 327.6 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:01 INFO BlockManager: Found block rdd_12_161 locally
21/01/31 13:48:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:01 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:01 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:01 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:01 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:01 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:01 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:01 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:01 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:01 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35552734
21/01/31 13:48:03 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000148_297' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000148
21/01/31 13:48:03 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000148_297: Committed
21/01/31 13:48:03 INFO Executor: Finished task 148.0 in stage 3.0 (TID 297). 2461 bytes result sent to driver
21/01/31 13:48:03 INFO CoarseGrainedExecutorBackend: Got assigned task 313
21/01/31 13:48:03 INFO Executor: Running task 169.0 in stage 3.0 (TID 313)
21/01/31 13:48:03 INFO MemoryStore: Will not store rdd_12_169
21/01/31 13:48:03 WARN MemoryStore: Not enough space to cache rdd_12_169 in memory! (computed 37.7 MiB so far)
21/01/31 13:48:03 INFO MemoryStore: Memory use = 324.2 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 330.9 MiB. Storage limit = 366.3 MiB.
21/01/31 13:48:03 INFO BlockManager: Found block rdd_12_169 locally
21/01/31 13:48:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:03 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:03 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:03 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:03 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:03 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:03 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:03 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35765160
21/01/31 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000161_312' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000161
21/01/31 13:48:11 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000161_312: Committed
21/01/31 13:48:11 INFO Executor: Finished task 161.0 in stage 3.0 (TID 312). 2461 bytes result sent to driver
21/01/31 13:48:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35128386
21/01/31 13:48:13 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000169_313' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000169
21/01/31 13:48:13 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000169_313: Committed
21/01/31 13:48:13 INFO Executor: Finished task 169.0 in stage 3.0 (TID 313). 2461 bytes result sent to driver
21/01/31 13:48:15 INFO CoarseGrainedExecutorBackend: Got assigned task 327
21/01/31 13:48:15 INFO Executor: Running task 72.0 in stage 3.0 (TID 327)
21/01/31 13:48:15 INFO CoarseGrainedExecutorBackend: Got assigned task 328
21/01/31 13:48:15 INFO Executor: Running task 74.0 in stage 3.0 (TID 328)
21/01/31 13:48:15 INFO BlockManager: Read rdd_12_74 from the disk of a same host executor is successful.
21/01/31 13:48:15 INFO BlockManager: Read rdd_12_72 from the disk of a same host executor is successful.
21/01/31 13:48:15 INFO BlockManager: Found block rdd_12_74 remotely
21/01/31 13:48:15 INFO BlockManager: Found block rdd_12_72 remotely
21/01/31 13:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:15 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:15 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:15 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:15 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36642278
21/01/31 13:48:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36691457
21/01/31 13:48:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000072_327' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000072
21/01/31 13:48:30 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000072_327: Committed
21/01/31 13:48:30 INFO Executor: Finished task 72.0 in stage 3.0 (TID 327). 2461 bytes result sent to driver
21/01/31 13:48:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000074_328' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000074
21/01/31 13:48:30 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000074_328: Committed
21/01/31 13:48:30 INFO Executor: Finished task 74.0 in stage 3.0 (TID 328). 2461 bytes result sent to driver
21/01/31 13:48:31 INFO CoarseGrainedExecutorBackend: Got assigned task 344
21/01/31 13:48:31 INFO Executor: Running task 116.0 in stage 3.0 (TID 344)
21/01/31 13:48:31 INFO CoarseGrainedExecutorBackend: Got assigned task 345
21/01/31 13:48:31 INFO Executor: Running task 118.0 in stage 3.0 (TID 345)
21/01/31 13:48:31 INFO BlockManager: Read rdd_12_118 from the disk of a same host executor is successful.
21/01/31 13:48:31 INFO BlockManager: Found block rdd_12_118 remotely
21/01/31 13:48:31 INFO BlockManager: Read rdd_12_116 from the disk of a same host executor is successful.
21/01/31 13:48:31 INFO BlockManager: Found block rdd_12_116 remotely
21/01/31 13:48:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:31 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:31 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:31 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:31 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:31 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:31 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:31 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:31 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35264638
21/01/31 13:48:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35910165
21/01/31 13:48:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000116_344' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000116
21/01/31 13:48:43 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000116_344: Committed
21/01/31 13:48:43 INFO Executor: Finished task 116.0 in stage 3.0 (TID 344). 2461 bytes result sent to driver
21/01/31 13:48:43 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000118_345' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000118
21/01/31 13:48:43 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000118_345: Committed
21/01/31 13:48:43 INFO Executor: Finished task 118.0 in stage 3.0 (TID 345). 2461 bytes result sent to driver
21/01/31 13:48:46 INFO CoarseGrainedExecutorBackend: Got assigned task 362
21/01/31 13:48:46 INFO Executor: Running task 153.0 in stage 3.0 (TID 362)
21/01/31 13:48:46 INFO CoarseGrainedExecutorBackend: Got assigned task 364
21/01/31 13:48:46 INFO Executor: Running task 163.0 in stage 3.0 (TID 364)
21/01/31 13:48:46 INFO BlockManager: Read rdd_12_163 from the disk of a same host executor is successful.
21/01/31 13:48:46 INFO BlockManager: Found block rdd_12_163 remotely
21/01/31 13:48:46 INFO BlockManager: Read rdd_12_153 from the disk of a same host executor is successful.
21/01/31 13:48:46 INFO BlockManager: Found block rdd_12_153 remotely
21/01/31 13:48:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 13:48:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 13:48:46 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:46 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:46 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:46 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:46 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:46 INFO CodecConfig: Compression: SNAPPY
21/01/31 13:48:46 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 13:48:46 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 13:48:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 13:48:46 INFO ParquetOutputFormat: Dictionary is on
21/01/31 13:48:46 INFO ParquetOutputFormat: Validation is off
21/01/31 13:48:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 13:48:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 13:48:46 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 13:48:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 13:48:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 13:48:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 13:48:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35772534
21/01/31 13:48:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36257615
21/01/31 13:48:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000163_364' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000163
21/01/31 13:48:57 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000163_364: Committed
21/01/31 13:48:57 INFO Executor: Finished task 163.0 in stage 3.0 (TID 364). 2461 bytes result sent to driver
21/01/31 13:48:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210131134604_0003_m_000153_362' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131134604_0003_m_000153
21/01/31 13:48:57 INFO SparkHadoopMapRedUtil: attempt_20210131134604_0003_m_000153_362: Committed
21/01/31 13:48:57 INFO Executor: Finished task 153.0 in stage 3.0 (TID 362). 2461 bytes result sent to driver
21/01/31 13:51:55 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
