Spark Executor Command: "/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/bin/java" "-cp" "/Users/kinsho/workspace/spark-3.0.1/conf/:/Users/kinsho/workspace/spark-3.0.1/assembly/target/scala-2.12/jars/*" "-Xmx1024M" "-Dspark.driver.port=61200" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.11.7:61200" "--executor-id" "2" "--hostname" "192.168.11.7" "--cores" "2" "--app-id" "app-20210131124043-0029" "--worker-url" "spark://Worker@192.168.11.7:63876"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/31 12:40:45 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 26423@ST000000035
21/01/31 12:40:45 INFO SignalUtils: Registered signal handler for TERM
21/01/31 12:40:45 INFO SignalUtils: Registered signal handler for HUP
21/01/31 12:40:45 INFO SignalUtils: Registered signal handler for INT
21/01/31 12:40:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/01/31 12:40:46 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 12:40:46 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 12:40:46 INFO SecurityManager: Changing view acls groups to: 
21/01/31 12:40:46 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 12:40:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 12:40:47 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61200 after 111 ms (0 ms spent in bootstraps)
21/01/31 12:40:47 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 12:40:47 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 12:40:47 INFO SecurityManager: Changing view acls groups to: 
21/01/31 12:40:47 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 12:40:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 12:40:47 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61200 after 2 ms (0 ms spent in bootstraps)
21/01/31 12:40:47 INFO DiskBlockManager: Created local directory at /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-ed26b2ee-040f-4b9b-80d0-8d360fa07eb5/executor-894d2637-b5b0-4428-b314-3bf9396f3bc4/blockmgr-64de6f5b-2e10-4d44-9e4a-539e45dee073
21/01/31 12:40:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/01/31 12:40:48 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.11.7:61200
21/01/31 12:40:48 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.11.7:63876
21/01/31 12:40:48 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:63876 after 3 ms (0 ms spent in bootstraps)
21/01/31 12:40:48 INFO ResourceUtils: ==============================================================
21/01/31 12:40:48 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.11.7:63876
21/01/31 12:40:48 INFO ResourceUtils: Resources for spark.executor:

21/01/31 12:40:48 INFO ResourceUtils: ==============================================================
21/01/31 12:40:48 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
21/01/31 12:40:48 INFO Executor: Starting executor ID 2 on host 192.168.11.7
21/01/31 12:40:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61266.
21/01/31 12:40:48 INFO NettyBlockTransferService: Server created on 192.168.11.7:61266
21/01/31 12:40:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/31 12:40:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, 192.168.11.7, 61266, None)
21/01/31 12:40:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, 192.168.11.7, 61266, None)
21/01/31 12:40:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, 192.168.11.7, 61266, None)
21/01/31 12:40:52 INFO CoarseGrainedExecutorBackend: Got assigned task 3
21/01/31 12:40:52 INFO CoarseGrainedExecutorBackend: Got assigned task 13
21/01/31 12:40:52 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
21/01/31 12:40:52 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
21/01/31 12:40:52 INFO Executor: Fetching spark://192.168.11.7:61200/jars/simple-project_2.12-1.0.jar with timestamp 1612064442939
21/01/31 12:40:52 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61200 after 16 ms (0 ms spent in bootstraps)
21/01/31 12:40:52 INFO Utils: Fetching spark://192.168.11.7:61200/jars/simple-project_2.12-1.0.jar to /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-ed26b2ee-040f-4b9b-80d0-8d360fa07eb5/executor-894d2637-b5b0-4428-b314-3bf9396f3bc4/spark-46845e65-64a2-444b-bf14-535b5e15622b/fetchFileTemp4781211930344198914.tmp
21/01/31 12:40:52 INFO Utils: Copying /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-ed26b2ee-040f-4b9b-80d0-8d360fa07eb5/executor-894d2637-b5b0-4428-b314-3bf9396f3bc4/spark-46845e65-64a2-444b-bf14-535b5e15622b/20408847431612064442939_cache to /Users/kinsho/workspace/spark-3.0.1/work/app-20210131124043-0029/2/./simple-project_2.12-1.0.jar
21/01/31 12:40:52 INFO Executor: Adding file:/Users/kinsho/workspace/spark-3.0.1/work/app-20210131124043-0029/2/./simple-project_2.12-1.0.jar to class loader
21/01/31 12:40:52 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 12:40:53 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61206 after 10 ms (0 ms spent in bootstraps)
21/01/31 12:40:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 366.3 MiB)
21/01/31 12:40:53 INFO TorrentBroadcast: Reading broadcast variable 4 took 498 ms
21/01/31 12:40:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 43.3 KiB, free 366.2 MiB)
21/01/31 12:40:55 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 268435456-402653184, partition values: [empty row]
21/01/31 12:40:55 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 1610612736-1744830464, partition values: [empty row]
21/01/31 12:40:57 INFO CodeGenerator: Code generated in 1240.178982 ms
21/01/31 12:40:57 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 12:40:57 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61256 after 2 ms (0 ms spent in bootstraps)
21/01/31 12:40:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.2 MiB)
21/01/31 12:40:57 INFO TorrentBroadcast: Reading broadcast variable 3 took 13 ms
21/01/31 12:40:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 336.8 KiB, free 365.9 MiB)
21/01/31 12:41:25 INFO MemoryStore: Will not store rdd_12_12
21/01/31 12:41:25 WARN MemoryStore: Not enough space to cache rdd_12_12 in memory! (computed 136.1 MiB so far)
21/01/31 12:41:25 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 311.0 MiB (scratch space shared across 2 tasks(s)) = 311.5 MiB. Storage limit = 366.3 MiB.
21/01/31 12:41:25 WARN BlockManager: Persisting block rdd_12_12 to disk instead.
21/01/31 12:41:30 INFO MemoryStore: Block rdd_12_2 stored as values in memory (estimated size 163.4 MiB, free 202.5 MiB)
21/01/31 12:41:30 INFO CodeGenerator: Code generated in 10.293916 ms
21/01/31 12:41:30 INFO CodeGenerator: Code generated in 45.590319 ms
21/01/31 12:41:30 INFO CodeGenerator: Code generated in 37.726275 ms
21/01/31 12:41:30 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2108 bytes result sent to driver
21/01/31 12:41:30 INFO CoarseGrainedExecutorBackend: Got assigned task 23
21/01/31 12:41:30 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
21/01/31 12:41:30 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 2952790016-3087007744, partition values: [empty row]
21/01/31 12:41:35 INFO MemoryStore: Will not store rdd_12_12
21/01/31 12:41:35 WARN MemoryStore: Not enough space to cache rdd_12_12 in memory! (computed 136.1 MiB so far)
21/01/31 12:41:35 INFO MemoryStore: Memory use = 163.8 MiB (blocks) + 106.4 MiB (scratch space shared across 2 tasks(s)) = 270.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:41:36 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2108 bytes result sent to driver
21/01/31 12:41:36 INFO CoarseGrainedExecutorBackend: Got assigned task 38
21/01/31 12:41:36 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
21/01/31 12:41:36 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 4966055936-5100273664, partition values: [empty row]
21/01/31 12:41:49 INFO MemoryStore: Will not store rdd_12_37
21/01/31 12:41:49 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 69.7 MiB so far)
21/01/31 12:41:49 INFO MemoryStore: Memory use = 163.8 MiB (blocks) + 156.2 MiB (scratch space shared across 2 tasks(s)) = 320.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:41:49 WARN BlockManager: Persisting block rdd_12_37 to disk instead.
21/01/31 12:42:05 INFO MemoryStore: Block rdd_12_22 stored as values in memory (estimated size 184.6 MiB, free 17.8 MiB)
21/01/31 12:42:05 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2108 bytes result sent to driver
21/01/31 12:42:05 INFO CoarseGrainedExecutorBackend: Got assigned task 43
21/01/31 12:42:05 INFO Executor: Running task 42.0 in stage 1.0 (TID 43)
21/01/31 12:42:05 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 5637144576-5771362304, partition values: [empty row]
21/01/31 12:42:08 INFO MemoryStore: Will not store rdd_12_37
21/01/31 12:42:08 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 35.9 MiB so far)
21/01/31 12:42:08 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 354.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:08 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2108 bytes result sent to driver
21/01/31 12:42:08 INFO CoarseGrainedExecutorBackend: Got assigned task 52
21/01/31 12:42:08 INFO Executor: Running task 51.0 in stage 1.0 (TID 52)
21/01/31 12:42:08 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 6845104128-6979321856, partition values: [empty row]
21/01/31 12:42:12 INFO MemoryStore: Will not store rdd_12_42
21/01/31 12:42:12 WARN MemoryStore: Not enough space to cache rdd_12_42 in memory! (computed 35.9 MiB so far)
21/01/31 12:42:12 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 354.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:12 WARN BlockManager: Persisting block rdd_12_42 to disk instead.
21/01/31 12:42:15 INFO MemoryStore: Will not store rdd_12_51
21/01/31 12:42:15 WARN MemoryStore: Not enough space to cache rdd_12_51 in memory! (computed 36.1 MiB so far)
21/01/31 12:42:15 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 351.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:15 WARN BlockManager: Persisting block rdd_12_51 to disk instead.
21/01/31 12:42:37 INFO MemoryStore: Will not store rdd_12_42
21/01/31 12:42:37 WARN MemoryStore: Not enough space to cache rdd_12_42 in memory! (computed 35.9 MiB so far)
21/01/31 12:42:37 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 351.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:38 INFO Executor: Finished task 42.0 in stage 1.0 (TID 43). 2108 bytes result sent to driver
21/01/31 12:42:38 INFO CoarseGrainedExecutorBackend: Got assigned task 65
21/01/31 12:42:38 INFO Executor: Running task 64.0 in stage 1.0 (TID 65)
21/01/31 12:42:38 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 8589934592-8724152320, partition values: [empty row]
21/01/31 12:42:40 INFO MemoryStore: Will not store rdd_12_51
21/01/31 12:42:40 WARN MemoryStore: Not enough space to cache rdd_12_51 in memory! (computed 36.1 MiB so far)
21/01/31 12:42:40 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 354.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:40 INFO Executor: Finished task 51.0 in stage 1.0 (TID 52). 2108 bytes result sent to driver
21/01/31 12:42:40 INFO CoarseGrainedExecutorBackend: Got assigned task 73
21/01/31 12:42:40 INFO Executor: Running task 72.0 in stage 1.0 (TID 73)
21/01/31 12:42:40 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 9663676416-9797894144, partition values: [empty row]
21/01/31 12:42:45 INFO MemoryStore: Will not store rdd_12_64
21/01/31 12:42:45 WARN MemoryStore: Not enough space to cache rdd_12_64 in memory! (computed 35.8 MiB so far)
21/01/31 12:42:45 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 354.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:45 WARN BlockManager: Persisting block rdd_12_64 to disk instead.
21/01/31 12:42:47 INFO MemoryStore: Will not store rdd_12_72
21/01/31 12:42:47 WARN MemoryStore: Not enough space to cache rdd_12_72 in memory! (computed 36.2 MiB so far)
21/01/31 12:42:47 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 351.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:47 WARN BlockManager: Persisting block rdd_12_72 to disk instead.
21/01/31 12:43:10 INFO MemoryStore: Will not store rdd_12_64
21/01/31 12:43:10 WARN MemoryStore: Not enough space to cache rdd_12_64 in memory! (computed 35.8 MiB so far)
21/01/31 12:43:10 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 351.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:10 INFO Executor: Finished task 64.0 in stage 1.0 (TID 65). 2108 bytes result sent to driver
21/01/31 12:43:10 INFO CoarseGrainedExecutorBackend: Got assigned task 86
21/01/31 12:43:10 INFO Executor: Running task 85.0 in stage 1.0 (TID 86)
21/01/31 12:43:10 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 11408506880-11542724608, partition values: [empty row]
21/01/31 12:43:12 INFO MemoryStore: Will not store rdd_12_72
21/01/31 12:43:12 WARN MemoryStore: Not enough space to cache rdd_12_72 in memory! (computed 36.2 MiB so far)
21/01/31 12:43:12 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 354.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:12 INFO Executor: Finished task 72.0 in stage 1.0 (TID 73). 2108 bytes result sent to driver
21/01/31 12:43:12 INFO CoarseGrainedExecutorBackend: Got assigned task 93
21/01/31 12:43:12 INFO Executor: Running task 92.0 in stage 1.0 (TID 93)
21/01/31 12:43:12 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 12348030976-12482248704, partition values: [empty row]
21/01/31 12:43:17 INFO MemoryStore: Will not store rdd_12_85
21/01/31 12:43:17 WARN MemoryStore: Not enough space to cache rdd_12_85 in memory! (computed 34.9 MiB so far)
21/01/31 12:43:17 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 354.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:17 WARN BlockManager: Persisting block rdd_12_85 to disk instead.
21/01/31 12:43:19 INFO MemoryStore: Will not store rdd_12_92
21/01/31 12:43:19 WARN MemoryStore: Not enough space to cache rdd_12_92 in memory! (computed 34.7 MiB so far)
21/01/31 12:43:19 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 351.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:19 WARN BlockManager: Persisting block rdd_12_92 to disk instead.
21/01/31 12:43:42 INFO MemoryStore: Will not store rdd_12_85
21/01/31 12:43:42 WARN MemoryStore: Not enough space to cache rdd_12_85 in memory! (computed 34.9 MiB so far)
21/01/31 12:43:42 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 351.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:42 INFO Executor: Finished task 85.0 in stage 1.0 (TID 86). 2108 bytes result sent to driver
21/01/31 12:43:42 INFO CoarseGrainedExecutorBackend: Got assigned task 108
21/01/31 12:43:42 INFO Executor: Running task 107.0 in stage 1.0 (TID 108)
21/01/31 12:43:42 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 14361296896-14495514624, partition values: [empty row]
21/01/31 12:43:44 INFO MemoryStore: Will not store rdd_12_92
21/01/31 12:43:44 WARN MemoryStore: Not enough space to cache rdd_12_92 in memory! (computed 34.7 MiB so far)
21/01/31 12:43:44 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 354.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:45 INFO Executor: Finished task 92.0 in stage 1.0 (TID 93). 2108 bytes result sent to driver
21/01/31 12:43:45 INFO CoarseGrainedExecutorBackend: Got assigned task 114
21/01/31 12:43:45 INFO Executor: Running task 113.0 in stage 1.0 (TID 114)
21/01/31 12:43:45 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 15166603264-15300820992, partition values: [empty row]
21/01/31 12:43:49 INFO MemoryStore: Will not store rdd_12_107
21/01/31 12:43:49 WARN MemoryStore: Not enough space to cache rdd_12_107 in memory! (computed 35.5 MiB so far)
21/01/31 12:43:49 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 354.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:49 WARN BlockManager: Persisting block rdd_12_107 to disk instead.
21/01/31 12:43:51 INFO MemoryStore: Will not store rdd_12_113
21/01/31 12:43:51 WARN MemoryStore: Not enough space to cache rdd_12_113 in memory! (computed 36.0 MiB so far)
21/01/31 12:43:51 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 351.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:51 WARN BlockManager: Persisting block rdd_12_113 to disk instead.
21/01/31 12:44:13 INFO MemoryStore: Will not store rdd_12_107
21/01/31 12:44:13 WARN MemoryStore: Not enough space to cache rdd_12_107 in memory! (computed 35.5 MiB so far)
21/01/31 12:44:13 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.0 MiB (scratch space shared across 1 tasks(s)) = 351.5 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:14 INFO Executor: Finished task 107.0 in stage 1.0 (TID 108). 2108 bytes result sent to driver
21/01/31 12:44:14 INFO CoarseGrainedExecutorBackend: Got assigned task 127
21/01/31 12:44:14 INFO Executor: Running task 126.0 in stage 1.0 (TID 127)
21/01/31 12:44:14 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 16911433728-17045651456, partition values: [empty row]
21/01/31 12:44:15 INFO MemoryStore: Will not store rdd_12_113
21/01/31 12:44:15 WARN MemoryStore: Not enough space to cache rdd_12_113 in memory! (computed 36.0 MiB so far)
21/01/31 12:44:15 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 354.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:16 INFO Executor: Finished task 113.0 in stage 1.0 (TID 114). 2108 bytes result sent to driver
21/01/31 12:44:16 INFO CoarseGrainedExecutorBackend: Got assigned task 134
21/01/31 12:44:16 INFO Executor: Running task 133.0 in stage 1.0 (TID 134)
21/01/31 12:44:16 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 17850957824-17985175552, partition values: [empty row]
21/01/31 12:44:20 INFO MemoryStore: Will not store rdd_12_126
21/01/31 12:44:20 WARN MemoryStore: Not enough space to cache rdd_12_126 in memory! (computed 35.8 MiB so far)
21/01/31 12:44:20 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 354.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:20 WARN BlockManager: Persisting block rdd_12_126 to disk instead.
21/01/31 12:44:22 INFO MemoryStore: Will not store rdd_12_133
21/01/31 12:44:22 WARN MemoryStore: Not enough space to cache rdd_12_133 in memory! (computed 35.7 MiB so far)
21/01/31 12:44:22 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 351.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:22 WARN BlockManager: Persisting block rdd_12_133 to disk instead.
21/01/31 12:44:44 INFO MemoryStore: Will not store rdd_12_126
21/01/31 12:44:44 WARN MemoryStore: Not enough space to cache rdd_12_126 in memory! (computed 35.8 MiB so far)
21/01/31 12:44:44 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 351.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:45 INFO Executor: Finished task 126.0 in stage 1.0 (TID 127). 2108 bytes result sent to driver
21/01/31 12:44:45 INFO CoarseGrainedExecutorBackend: Got assigned task 148
21/01/31 12:44:45 INFO Executor: Running task 147.0 in stage 1.0 (TID 148)
21/01/31 12:44:45 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 19730006016-19864223744, partition values: [empty row]
21/01/31 12:44:47 INFO MemoryStore: Will not store rdd_12_133
21/01/31 12:44:47 WARN MemoryStore: Not enough space to cache rdd_12_133 in memory! (computed 35.7 MiB so far)
21/01/31 12:44:47 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 355.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:47 INFO Executor: Finished task 133.0 in stage 1.0 (TID 134). 2108 bytes result sent to driver
21/01/31 12:44:47 INFO CoarseGrainedExecutorBackend: Got assigned task 153
21/01/31 12:44:47 INFO Executor: Running task 152.0 in stage 1.0 (TID 153)
21/01/31 12:44:47 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 20401094656-20535312384, partition values: [empty row]
21/01/31 12:44:52 INFO MemoryStore: Will not store rdd_12_147
21/01/31 12:44:52 WARN MemoryStore: Not enough space to cache rdd_12_147 in memory! (computed 36.6 MiB so far)
21/01/31 12:44:52 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 355.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:52 WARN BlockManager: Persisting block rdd_12_147 to disk instead.
21/01/31 12:44:54 INFO MemoryStore: Will not store rdd_12_152
21/01/31 12:44:54 WARN MemoryStore: Not enough space to cache rdd_12_152 in memory! (computed 37.4 MiB so far)
21/01/31 12:44:54 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 351.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:54 WARN BlockManager: Persisting block rdd_12_152 to disk instead.
21/01/31 12:45:16 INFO MemoryStore: Will not store rdd_12_147
21/01/31 12:45:16 WARN MemoryStore: Not enough space to cache rdd_12_147 in memory! (computed 36.6 MiB so far)
21/01/31 12:45:16 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 351.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:16 INFO MemoryStore: Will not store rdd_12_152
21/01/31 12:45:16 WARN MemoryStore: Not enough space to cache rdd_12_152 in memory! (computed 37.4 MiB so far)
21/01/31 12:45:16 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 351.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:16 INFO Executor: Finished task 147.0 in stage 1.0 (TID 148). 2108 bytes result sent to driver
21/01/31 12:45:16 INFO CoarseGrainedExecutorBackend: Got assigned task 168
21/01/31 12:45:16 INFO Executor: Running task 167.0 in stage 1.0 (TID 168)
21/01/31 12:45:16 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22414360576-22548578304, partition values: [empty row]
21/01/31 12:45:16 INFO Executor: Finished task 152.0 in stage 1.0 (TID 153). 2108 bytes result sent to driver
21/01/31 12:45:16 INFO CoarseGrainedExecutorBackend: Got assigned task 169
21/01/31 12:45:16 INFO Executor: Running task 168.0 in stage 1.0 (TID 169)
21/01/31 12:45:16 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22548578304-22682796032, partition values: [empty row]
21/01/31 12:45:23 INFO MemoryStore: Will not store rdd_12_167
21/01/31 12:45:23 WARN MemoryStore: Not enough space to cache rdd_12_167 in memory! (computed 37.4 MiB so far)
21/01/31 12:45:23 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.7 MiB (scratch space shared across 2 tasks(s)) = 355.2 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:23 WARN BlockManager: Persisting block rdd_12_167 to disk instead.
21/01/31 12:45:23 INFO MemoryStore: Will not store rdd_12_168
21/01/31 12:45:23 WARN MemoryStore: Not enough space to cache rdd_12_168 in memory! (computed 37.5 MiB so far)
21/01/31 12:45:23 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 6.7 MiB (scratch space shared across 2 tasks(s)) = 355.2 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:23 WARN BlockManager: Persisting block rdd_12_168 to disk instead.
21/01/31 12:45:38 INFO MemoryStore: Will not store rdd_12_168
21/01/31 12:45:38 WARN MemoryStore: Not enough space to cache rdd_12_168 in memory! (computed 37.5 MiB so far)
21/01/31 12:45:38 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.4 MiB (scratch space shared across 1 tasks(s)) = 351.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:38 INFO MemoryStore: Will not store rdd_12_167
21/01/31 12:45:38 WARN MemoryStore: Not enough space to cache rdd_12_167 in memory! (computed 37.4 MiB so far)
21/01/31 12:45:38 INFO MemoryStore: Memory use = 348.5 MiB (blocks) + 3.3 MiB (scratch space shared across 1 tasks(s)) = 351.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:38 INFO Executor: Finished task 168.0 in stage 1.0 (TID 169). 2108 bytes result sent to driver
21/01/31 12:45:39 INFO Executor: Finished task 167.0 in stage 1.0 (TID 168). 2108 bytes result sent to driver
21/01/31 12:45:41 INFO CoarseGrainedExecutorBackend: Got assigned task 186
21/01/31 12:45:41 INFO CoarseGrainedExecutorBackend: Got assigned task 196
21/01/31 12:45:41 INFO Executor: Running task 12.0 in stage 3.0 (TID 196)
21/01/31 12:45:41 INFO Executor: Running task 2.0 in stage 3.0 (TID 186)
21/01/31 12:45:41 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
21/01/31 12:45:41 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 12:45:41 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 69.4 KiB, free 17.7 MiB)
21/01/31 12:45:41 INFO TorrentBroadcast: Reading broadcast variable 6 took 39 ms
21/01/31 12:45:41 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 194.5 KiB, free 17.5 MiB)
21/01/31 12:45:41 INFO BlockManager: Found block rdd_12_2 locally
21/01/31 12:45:41 INFO CodeGenerator: Code generated in 188.106675 ms
21/01/31 12:45:41 INFO MemoryStore: Will not store rdd_12_12
21/01/31 12:45:41 WARN MemoryStore: Not enough space to cache rdd_12_12 in memory! (computed 35.4 MiB so far)
21/01/31 12:45:41 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 351.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:41 INFO BlockManager: Found block rdd_12_12 locally
21/01/31 12:45:41 INFO CodeGenerator: Code generated in 38.401577 ms
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:42 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:42 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:45:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:45:42 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 12:45:42 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 12:45:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38169442
21/01/31 12:45:53 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000002_186' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000002
21/01/31 12:45:53 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000002_186: Committed
21/01/31 12:45:53 INFO Executor: Finished task 2.0 in stage 3.0 (TID 186). 2504 bytes result sent to driver
21/01/31 12:45:53 INFO CoarseGrainedExecutorBackend: Got assigned task 200
21/01/31 12:45:53 INFO Executor: Running task 22.0 in stage 3.0 (TID 200)
21/01/31 12:45:53 INFO BlockManager: Found block rdd_12_22 locally
21/01/31 12:45:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:53 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:53 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:53 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:53 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:53 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:53 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:53 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:53 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:53 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:53 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:45:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 41092133
21/01/31 12:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000012_196' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000012
21/01/31 12:45:55 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000012_196: Committed
21/01/31 12:45:55 INFO Executor: Finished task 12.0 in stage 3.0 (TID 196). 2461 bytes result sent to driver
21/01/31 12:45:55 INFO CoarseGrainedExecutorBackend: Got assigned task 209
21/01/31 12:45:55 INFO Executor: Running task 37.0 in stage 3.0 (TID 209)
21/01/31 12:45:55 INFO MemoryStore: Will not store rdd_12_37
21/01/31 12:45:55 WARN MemoryStore: Not enough space to cache rdd_12_37 in memory! (computed 35.9 MiB so far)
21/01/31 12:45:55 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 351.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:55 INFO BlockManager: Found block rdd_12_37 locally
21/01/31 12:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:55 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:55 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:55 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:55 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 42398223
21/01/31 12:46:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000022_200' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000022
21/01/31 12:46:05 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000022_200: Committed
21/01/31 12:46:05 INFO Executor: Finished task 22.0 in stage 3.0 (TID 200). 2461 bytes result sent to driver
21/01/31 12:46:05 INFO CoarseGrainedExecutorBackend: Got assigned task 219
21/01/31 12:46:05 INFO Executor: Running task 42.0 in stage 3.0 (TID 219)
21/01/31 12:46:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38899243
21/01/31 12:46:05 INFO MemoryStore: Will not store rdd_12_42
21/01/31 12:46:05 WARN MemoryStore: Not enough space to cache rdd_12_42 in memory! (computed 35.9 MiB so far)
21/01/31 12:46:05 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 351.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:05 INFO BlockManager: Found block rdd_12_42 locally
21/01/31 12:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:05 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000037_209' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000037
21/01/31 12:46:05 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000037_209: Committed
21/01/31 12:46:05 INFO Executor: Finished task 37.0 in stage 3.0 (TID 209). 2461 bytes result sent to driver
21/01/31 12:46:05 INFO CoarseGrainedExecutorBackend: Got assigned task 221
21/01/31 12:46:05 INFO Executor: Running task 51.0 in stage 3.0 (TID 221)
21/01/31 12:46:05 INFO MemoryStore: Will not store rdd_12_51
21/01/31 12:46:05 WARN MemoryStore: Not enough space to cache rdd_12_51 in memory! (computed 36.1 MiB so far)
21/01/31 12:46:05 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 355.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:05 INFO BlockManager: Found block rdd_12_51 locally
21/01/31 12:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:05 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:05 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:05 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:05 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:05 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:05 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:05 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:05 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:27 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:77)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:234)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:232)
	at org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:202)
	at org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33)
	at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:126)
	at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147)
	at org.apache.parquet.column.impl.ColumnWriterV1.accountForValueWritten(ColumnWriterV1.java:106)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:200)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$998/122123343.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1031/698430364.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1030/1285276654.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1027/1254066508.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
21/01/31 12:46:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37482128
21/01/31 12:46:30 WARN Utils: Suppressing exception in catch: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:77)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:234)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:232)
	at org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:202)
	at org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33)
	at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:126)
	at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147)
	at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:235)
	at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:122)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:172)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)
	at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:84)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:278)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1028/308820047.apply$mcV$sp(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1422)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$959/138497015.apply(Unknown Source)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$430/260756260.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/01/31 12:46:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38127032
21/01/31 12:46:30 ERROR Executor: Exception in task 51.0 in stage 3.0 (TID 221)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:77)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:234)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:232)
	at org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:202)
	at org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33)
	at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:126)
	at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147)
	at org.apache.parquet.column.impl.ColumnWriterV1.accountForValueWritten(ColumnWriterV1.java:106)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:200)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$998/122123343.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1031/698430364.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1030/1285276654.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1027/1254066508.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
21/01/31 12:46:30 INFO CoarseGrainedExecutorBackend: Got assigned task 249
21/01/31 12:46:30 INFO Executor: Running task 64.0 in stage 3.0 (TID 249)
21/01/31 12:46:30 INFO MemoryStore: Will not store rdd_12_64
21/01/31 12:46:30 WARN MemoryStore: Not enough space to cache rdd_12_64 in memory! (computed 35.8 MiB so far)
21/01/31 12:46:30 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 351.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:30 INFO BlockManager: Found block rdd_12_64 locally
21/01/31 12:46:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:30 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:30 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000042_219' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000042
21/01/31 12:46:30 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000042_219: Committed
21/01/31 12:46:31 INFO Executor: Finished task 42.0 in stage 3.0 (TID 219). 2504 bytes result sent to driver
21/01/31 12:46:31 INFO CoarseGrainedExecutorBackend: Got assigned task 250
21/01/31 12:46:31 INFO Executor: Running task 51.1 in stage 3.0 (TID 250)
21/01/31 12:46:31 INFO MemoryStore: Will not store rdd_12_51
21/01/31 12:46:31 WARN MemoryStore: Not enough space to cache rdd_12_51 in memory! (computed 36.1 MiB so far)
21/01/31 12:46:31 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.4 MiB (scratch space shared across 2 tasks(s)) = 355.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:31 INFO BlockManager: Found block rdd_12_51 locally
21/01/31 12:46:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:31 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:31 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:31 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:31 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:31 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:31 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:31 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:31 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:31 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37569747
21/01/31 12:46:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37629367
21/01/31 12:46:44 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000064_249' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000064
21/01/31 12:46:44 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000064_249: Committed
21/01/31 12:46:44 INFO Executor: Finished task 64.0 in stage 3.0 (TID 249). 2504 bytes result sent to driver
21/01/31 12:46:44 INFO CoarseGrainedExecutorBackend: Got assigned task 270
21/01/31 12:46:44 INFO Executor: Running task 72.0 in stage 3.0 (TID 270)
21/01/31 12:46:45 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000051_250' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000051
21/01/31 12:46:45 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000051_250: Committed
21/01/31 12:46:45 INFO Executor: Finished task 51.1 in stage 3.0 (TID 250). 2461 bytes result sent to driver
21/01/31 12:46:45 INFO CoarseGrainedExecutorBackend: Got assigned task 271
21/01/31 12:46:45 INFO Executor: Running task 85.0 in stage 3.0 (TID 271)
21/01/31 12:46:45 INFO MemoryStore: Will not store rdd_12_72
21/01/31 12:46:45 WARN MemoryStore: Not enough space to cache rdd_12_72 in memory! (computed 36.2 MiB so far)
21/01/31 12:46:45 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 355.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:45 INFO BlockManager: Found block rdd_12_72 locally
21/01/31 12:46:45 INFO MemoryStore: Will not store rdd_12_85
21/01/31 12:46:45 WARN MemoryStore: Not enough space to cache rdd_12_85 in memory! (computed 34.9 MiB so far)
21/01/31 12:46:45 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 355.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:45 INFO BlockManager: Found block rdd_12_85 locally
21/01/31 12:46:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:45 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:45 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:45 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:45 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:45 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:45 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:45 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:45 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:45 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:45 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:45 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36033742
21/01/31 12:46:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36642278
21/01/31 12:46:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000085_271' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000085
21/01/31 12:46:57 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000085_271: Committed
21/01/31 12:46:57 INFO Executor: Finished task 85.0 in stage 3.0 (TID 271). 2504 bytes result sent to driver
21/01/31 12:46:57 INFO CoarseGrainedExecutorBackend: Got assigned task 286
21/01/31 12:46:57 INFO Executor: Running task 92.0 in stage 3.0 (TID 286)
21/01/31 12:46:57 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000072_270' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000072
21/01/31 12:46:57 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000072_270: Committed
21/01/31 12:46:57 INFO Executor: Finished task 72.0 in stage 3.0 (TID 270). 2461 bytes result sent to driver
21/01/31 12:46:57 INFO CoarseGrainedExecutorBackend: Got assigned task 287
21/01/31 12:46:57 INFO Executor: Running task 107.0 in stage 3.0 (TID 287)
21/01/31 12:46:57 INFO MemoryStore: Will not store rdd_12_92
21/01/31 12:46:57 WARN MemoryStore: Not enough space to cache rdd_12_92 in memory! (computed 34.7 MiB so far)
21/01/31 12:46:57 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 354.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:57 INFO BlockManager: Found block rdd_12_92 locally
21/01/31 12:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:57 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:57 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:57 INFO MemoryStore: Will not store rdd_12_107
21/01/31 12:46:57 WARN MemoryStore: Not enough space to cache rdd_12_107 in memory! (computed 35.5 MiB so far)
21/01/31 12:46:57 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 354.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:46:57 INFO BlockManager: Found block rdd_12_107 locally
21/01/31 12:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:46:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:57 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:46:57 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:46:57 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:46:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:46:57 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:46:57 INFO ParquetOutputFormat: Validation is off
21/01/31 12:46:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:46:57 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:46:57 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:46:57 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:46:57 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:46:57 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35684465
21/01/31 12:47:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35293916
21/01/31 12:47:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000107_287' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000107
21/01/31 12:47:09 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000107_287: Committed
21/01/31 12:47:09 INFO Executor: Finished task 107.0 in stage 3.0 (TID 287). 2504 bytes result sent to driver
21/01/31 12:47:09 INFO CoarseGrainedExecutorBackend: Got assigned task 303
21/01/31 12:47:09 INFO Executor: Running task 113.0 in stage 3.0 (TID 303)
21/01/31 12:47:09 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000092_286' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000092
21/01/31 12:47:09 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000092_286: Committed
21/01/31 12:47:09 INFO Executor: Finished task 92.0 in stage 3.0 (TID 286). 2461 bytes result sent to driver
21/01/31 12:47:09 INFO CoarseGrainedExecutorBackend: Got assigned task 304
21/01/31 12:47:09 INFO Executor: Running task 126.0 in stage 3.0 (TID 304)
21/01/31 12:47:09 INFO MemoryStore: Will not store rdd_12_126
21/01/31 12:47:09 WARN MemoryStore: Not enough space to cache rdd_12_126 in memory! (computed 35.8 MiB so far)
21/01/31 12:47:09 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 355.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:09 INFO BlockManager: Found block rdd_12_126 locally
21/01/31 12:47:09 INFO MemoryStore: Will not store rdd_12_113
21/01/31 12:47:09 WARN MemoryStore: Not enough space to cache rdd_12_113 in memory! (computed 36.0 MiB so far)
21/01/31 12:47:09 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 355.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:09 INFO BlockManager: Found block rdd_12_113 locally
21/01/31 12:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:09 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:09 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:09 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:09 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:09 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:09 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:09 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:09 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:09 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:09 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35418967
21/01/31 12:47:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35460898
21/01/31 12:47:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000113_303' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000113
21/01/31 12:47:20 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000113_303: Committed
21/01/31 12:47:20 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000126_304' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000126
21/01/31 12:47:20 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000126_304: Committed
21/01/31 12:47:20 INFO Executor: Finished task 113.0 in stage 3.0 (TID 303). 2504 bytes result sent to driver
21/01/31 12:47:20 INFO Executor: Finished task 126.0 in stage 3.0 (TID 304). 2504 bytes result sent to driver
21/01/31 12:47:20 INFO CoarseGrainedExecutorBackend: Got assigned task 321
21/01/31 12:47:20 INFO Executor: Running task 133.0 in stage 3.0 (TID 321)
21/01/31 12:47:20 INFO CoarseGrainedExecutorBackend: Got assigned task 322
21/01/31 12:47:20 INFO Executor: Running task 147.0 in stage 3.0 (TID 322)
21/01/31 12:47:20 INFO MemoryStore: Will not store rdd_12_133
21/01/31 12:47:20 WARN MemoryStore: Not enough space to cache rdd_12_133 in memory! (computed 35.7 MiB so far)
21/01/31 12:47:20 INFO MemoryStore: Will not store rdd_12_147
21/01/31 12:47:20 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 355.2 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:20 INFO BlockManager: Found block rdd_12_133 locally
21/01/31 12:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:20 WARN MemoryStore: Not enough space to cache rdd_12_147 in memory! (computed 36.6 MiB so far)
21/01/31 12:47:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:20 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:20 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:20 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 355.2 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:20 INFO BlockManager: Found block rdd_12_147 locally
21/01/31 12:47:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:20 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:20 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:20 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:20 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:20 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:20 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35602206
21/01/31 12:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35097443
21/01/31 12:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000147_322' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000147
21/01/31 12:47:29 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000147_322: Committed
21/01/31 12:47:29 INFO Executor: Finished task 147.0 in stage 3.0 (TID 322). 2504 bytes result sent to driver
21/01/31 12:47:29 INFO CoarseGrainedExecutorBackend: Got assigned task 336
21/01/31 12:47:29 INFO Executor: Running task 152.0 in stage 3.0 (TID 336)
21/01/31 12:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000133_321' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000133
21/01/31 12:47:30 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000133_321: Committed
21/01/31 12:47:30 INFO Executor: Finished task 133.0 in stage 3.0 (TID 321). 2461 bytes result sent to driver
21/01/31 12:47:30 INFO CoarseGrainedExecutorBackend: Got assigned task 337
21/01/31 12:47:30 INFO Executor: Running task 167.0 in stage 3.0 (TID 337)
21/01/31 12:47:30 INFO MemoryStore: Will not store rdd_12_167
21/01/31 12:47:30 WARN MemoryStore: Not enough space to cache rdd_12_167 in memory! (computed 37.4 MiB so far)
21/01/31 12:47:30 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 355.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:30 INFO BlockManager: Found block rdd_12_167 locally
21/01/31 12:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:30 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:30 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:30 INFO MemoryStore: Will not store rdd_12_152
21/01/31 12:47:30 WARN MemoryStore: Not enough space to cache rdd_12_152 in memory! (computed 37.4 MiB so far)
21/01/31 12:47:30 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 6.6 MiB (scratch space shared across 2 tasks(s)) = 355.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:30 INFO BlockManager: Found block rdd_12_152 locally
21/01/31 12:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:30 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:30 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:30 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:30 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35244816
21/01/31 12:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35798564
21/01/31 12:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000167_337' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000167
21/01/31 12:47:40 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000167_337: Committed
21/01/31 12:47:40 INFO Executor: Finished task 167.0 in stage 3.0 (TID 337). 2504 bytes result sent to driver
21/01/31 12:47:40 INFO CoarseGrainedExecutorBackend: Got assigned task 352
21/01/31 12:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000152_336' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000152
21/01/31 12:47:40 INFO Executor: Running task 168.0 in stage 3.0 (TID 352)
21/01/31 12:47:40 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000152_336: Committed
21/01/31 12:47:40 INFO Executor: Finished task 152.0 in stage 3.0 (TID 336). 2461 bytes result sent to driver
21/01/31 12:47:41 INFO MemoryStore: Will not store rdd_12_168
21/01/31 12:47:41 WARN MemoryStore: Not enough space to cache rdd_12_168 in memory! (computed 37.5 MiB so far)
21/01/31 12:47:41 INFO MemoryStore: Memory use = 348.7 MiB (blocks) + 3.4 MiB (scratch space shared across 1 tasks(s)) = 352.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:41 INFO BlockManager: Found block rdd_12_168 locally
21/01/31 12:47:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:41 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:41 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:41 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:41 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:41 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:44 INFO CoarseGrainedExecutorBackend: Got assigned task 354
21/01/31 12:47:44 INFO Executor: Running task 156.0 in stage 3.0 (TID 354)
21/01/31 12:47:44 INFO BlockManager: Read rdd_12_156 from the disk of a same host executor is successful.
21/01/31 12:47:44 INFO BlockManager: Found block rdd_12_156 remotely
21/01/31 12:47:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:44 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:44 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:44 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:44 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:44 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:44 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:44 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34921270
21/01/31 12:47:49 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000168_352' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000168
21/01/31 12:47:49 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000168_352: Committed
21/01/31 12:47:49 INFO Executor: Finished task 168.0 in stage 3.0 (TID 352). 2504 bytes result sent to driver
21/01/31 12:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35925973
21/01/31 12:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000156_354' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000156
21/01/31 12:47:51 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000156_354: Committed
21/01/31 12:47:51 INFO Executor: Finished task 156.0 in stage 3.0 (TID 354). 2461 bytes result sent to driver
21/01/31 12:49:40 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
