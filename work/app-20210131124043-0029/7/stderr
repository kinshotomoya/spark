Spark Executor Command: "/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/bin/java" "-cp" "/Users/kinsho/workspace/spark-3.0.1/conf/:/Users/kinsho/workspace/spark-3.0.1/assembly/target/scala-2.12/jars/*" "-Xmx1024M" "-Dspark.driver.port=61200" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.11.7:61200" "--executor-id" "7" "--hostname" "192.168.11.7" "--cores" "2" "--app-id" "app-20210131124043-0029" "--worker-url" "spark://Worker@192.168.11.7:63861"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/01/31 12:40:45 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 26419@ST000000035
21/01/31 12:40:45 INFO SignalUtils: Registered signal handler for TERM
21/01/31 12:40:45 INFO SignalUtils: Registered signal handler for HUP
21/01/31 12:40:45 INFO SignalUtils: Registered signal handler for INT
21/01/31 12:40:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/01/31 12:40:46 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 12:40:46 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 12:40:46 INFO SecurityManager: Changing view acls groups to: 
21/01/31 12:40:46 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 12:40:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 12:40:47 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61200 after 118 ms (0 ms spent in bootstraps)
21/01/31 12:40:47 INFO SecurityManager: Changing view acls to: kinsho
21/01/31 12:40:47 INFO SecurityManager: Changing modify acls to: kinsho
21/01/31 12:40:47 INFO SecurityManager: Changing view acls groups to: 
21/01/31 12:40:47 INFO SecurityManager: Changing modify acls groups to: 
21/01/31 12:40:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kinsho); groups with view permissions: Set(); users  with modify permissions: Set(kinsho); groups with modify permissions: Set()
21/01/31 12:40:47 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61200 after 2 ms (0 ms spent in bootstraps)
21/01/31 12:40:47 INFO DiskBlockManager: Created local directory at /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-25de37d2-ff0b-4e7a-8da1-628ca4e62b74/executor-1779c9a6-7438-47af-89e3-7c4ebebd9a83/blockmgr-d193555b-d4e9-46d7-8e38-015b86d58f81
21/01/31 12:40:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/01/31 12:40:47 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.11.7:61200
21/01/31 12:40:47 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.11.7:63861
21/01/31 12:40:47 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:63861 after 3 ms (0 ms spent in bootstraps)
21/01/31 12:40:47 INFO ResourceUtils: ==============================================================
21/01/31 12:40:47 INFO ResourceUtils: Resources for spark.executor:

21/01/31 12:40:47 INFO ResourceUtils: ==============================================================
21/01/31 12:40:47 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
21/01/31 12:40:47 INFO Executor: Starting executor ID 7 on host 192.168.11.7
21/01/31 12:40:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61254.
21/01/31 12:40:48 INFO NettyBlockTransferService: Server created on 192.168.11.7:61254
21/01/31 12:40:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/01/31 12:40:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(7, 192.168.11.7, 61254, None)
21/01/31 12:40:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(7, 192.168.11.7, 61254, None)
21/01/31 12:40:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(7, 192.168.11.7, 61254, None)
21/01/31 12:40:52 INFO CoarseGrainedExecutorBackend: Got assigned task 7
21/01/31 12:40:52 INFO CoarseGrainedExecutorBackend: Got assigned task 17
21/01/31 12:40:52 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
21/01/31 12:40:52 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
21/01/31 12:40:52 INFO Executor: Fetching spark://192.168.11.7:61200/jars/simple-project_2.12-1.0.jar with timestamp 1612064442939
21/01/31 12:40:52 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61200 after 3 ms (0 ms spent in bootstraps)
21/01/31 12:40:52 INFO Utils: Fetching spark://192.168.11.7:61200/jars/simple-project_2.12-1.0.jar to /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-25de37d2-ff0b-4e7a-8da1-628ca4e62b74/executor-1779c9a6-7438-47af-89e3-7c4ebebd9a83/spark-5984e9fc-b91f-4a71-82d6-f5b9d9510cf5/fetchFileTemp1713693873223145707.tmp
21/01/31 12:40:52 INFO Utils: Copying /private/var/folders/tm/7v5m6cg144g467bsj7zgsxgc0000gn/T/spark-25de37d2-ff0b-4e7a-8da1-628ca4e62b74/executor-1779c9a6-7438-47af-89e3-7c4ebebd9a83/spark-5984e9fc-b91f-4a71-82d6-f5b9d9510cf5/20408847431612064442939_cache to /Users/kinsho/workspace/spark-3.0.1/work/app-20210131124043-0029/7/./simple-project_2.12-1.0.jar
21/01/31 12:40:52 INFO Executor: Adding file:/Users/kinsho/workspace/spark-3.0.1/work/app-20210131124043-0029/7/./simple-project_2.12-1.0.jar to class loader
21/01/31 12:40:52 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 12:40:53 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61206 after 3 ms (0 ms spent in bootstraps)
21/01/31 12:40:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 366.3 MiB)
21/01/31 12:40:53 INFO TorrentBroadcast: Reading broadcast variable 4 took 435 ms
21/01/31 12:40:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 43.3 KiB, free 366.2 MiB)
21/01/31 12:40:55 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 805306368-939524096, partition values: [empty row]
21/01/31 12:40:55 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 2147483648-2281701376, partition values: [empty row]
21/01/31 12:40:57 INFO CodeGenerator: Code generated in 1191.263602 ms
21/01/31 12:40:57 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 12:40:57 INFO TransportClientFactory: Successfully created connection to /192.168.11.7:61262 after 1 ms (0 ms spent in bootstraps)
21/01/31 12:40:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.2 MiB)
21/01/31 12:40:57 INFO TorrentBroadcast: Reading broadcast variable 3 took 79 ms
21/01/31 12:40:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 336.8 KiB, free 365.9 MiB)
21/01/31 12:41:24 INFO MemoryStore: Will not store rdd_12_6
21/01/31 12:41:24 WARN MemoryStore: Not enough space to cache rdd_12_6 in memory! (computed 138.0 MiB so far)
21/01/31 12:41:24 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 306.7 MiB (scratch space shared across 2 tasks(s)) = 307.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:41:24 WARN BlockManager: Persisting block rdd_12_6 to disk instead.
21/01/31 12:41:31 INFO MemoryStore: Will not store rdd_12_6
21/01/31 12:41:31 WARN MemoryStore: Not enough space to cache rdd_12_6 in memory! (computed 138.0 MiB so far)
21/01/31 12:41:31 INFO MemoryStore: Memory use = 422.1 KiB (blocks) + 306.7 MiB (scratch space shared across 2 tasks(s)) = 307.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:41:31 INFO CodeGenerator: Code generated in 9.465442 ms
21/01/31 12:41:31 INFO CodeGenerator: Code generated in 30.375883 ms
21/01/31 12:41:31 INFO CodeGenerator: Code generated in 49.404117 ms
21/01/31 12:41:31 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2108 bytes result sent to driver
21/01/31 12:41:31 INFO CoarseGrainedExecutorBackend: Got assigned task 24
21/01/31 12:41:31 INFO Executor: Running task 23.0 in stage 1.0 (TID 24)
21/01/31 12:41:31 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 3087007744-3221225472, partition values: [empty row]
21/01/31 12:41:35 INFO MemoryStore: Block rdd_12_16 stored as values in memory (estimated size 187.4 MiB, free 178.5 MiB)
21/01/31 12:41:35 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2108 bytes result sent to driver
21/01/31 12:41:35 INFO CoarseGrainedExecutorBackend: Got assigned task 35
21/01/31 12:41:35 INFO Executor: Running task 34.0 in stage 1.0 (TID 35)
21/01/31 12:41:35 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 4563402752-4697620480, partition values: [empty row]
21/01/31 12:41:48 INFO MemoryStore: Will not store rdd_12_34
21/01/31 12:41:48 WARN MemoryStore: Not enough space to cache rdd_12_34 in memory! (computed 68.8 MiB so far)
21/01/31 12:41:48 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 156.0 MiB (scratch space shared across 2 tasks(s)) = 343.8 MiB. Storage limit = 366.3 MiB.
21/01/31 12:41:48 WARN BlockManager: Persisting block rdd_12_34 to disk instead.
21/01/31 12:41:57 INFO MemoryStore: Will not store rdd_12_23
21/01/31 12:41:57 WARN MemoryStore: Not enough space to cache rdd_12_23 in memory! (computed 134.9 MiB so far)
21/01/31 12:41:57 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 103.1 MiB (scratch space shared across 1 tasks(s)) = 290.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:41:57 WARN BlockManager: Persisting block rdd_12_23 to disk instead.
21/01/31 12:42:06 INFO MemoryStore: Will not store rdd_12_23
21/01/31 12:42:06 WARN MemoryStore: Not enough space to cache rdd_12_23 in memory! (computed 134.9 MiB so far)
21/01/31 12:42:06 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 103.1 MiB (scratch space shared across 1 tasks(s)) = 290.9 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:06 INFO Executor: Finished task 23.0 in stage 1.0 (TID 24). 2108 bytes result sent to driver
21/01/31 12:42:06 INFO CoarseGrainedExecutorBackend: Got assigned task 45
21/01/31 12:42:06 INFO Executor: Running task 44.0 in stage 1.0 (TID 45)
21/01/31 12:42:06 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 5905580032-6039797760, partition values: [empty row]
21/01/31 12:42:08 INFO MemoryStore: Will not store rdd_12_34
21/01/31 12:42:08 WARN MemoryStore: Not enough space to cache rdd_12_34 in memory! (computed 136.3 MiB so far)
21/01/31 12:42:08 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 106.5 MiB (scratch space shared across 2 tasks(s)) = 294.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:08 INFO Executor: Finished task 34.0 in stage 1.0 (TID 35). 2108 bytes result sent to driver
21/01/31 12:42:08 INFO CoarseGrainedExecutorBackend: Got assigned task 51
21/01/31 12:42:08 INFO Executor: Running task 50.0 in stage 1.0 (TID 51)
21/01/31 12:42:08 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 6710886400-6845104128, partition values: [empty row]
21/01/31 12:42:21 INFO MemoryStore: Will not store rdd_12_50
21/01/31 12:42:21 WARN MemoryStore: Not enough space to cache rdd_12_50 in memory! (computed 69.2 MiB so far)
21/01/31 12:42:21 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 158.3 MiB (scratch space shared across 2 tasks(s)) = 346.2 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:21 WARN BlockManager: Persisting block rdd_12_50 to disk instead.
21/01/31 12:42:31 INFO MemoryStore: Will not store rdd_12_44
21/01/31 12:42:31 WARN MemoryStore: Not enough space to cache rdd_12_44 in memory! (computed 137.4 MiB so far)
21/01/31 12:42:31 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 105.2 MiB (scratch space shared across 1 tasks(s)) = 293.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:31 WARN BlockManager: Persisting block rdd_12_44 to disk instead.
21/01/31 12:42:38 INFO MemoryStore: Will not store rdd_12_44
21/01/31 12:42:38 WARN MemoryStore: Not enough space to cache rdd_12_44 in memory! (computed 137.4 MiB so far)
21/01/31 12:42:38 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 105.2 MiB (scratch space shared across 1 tasks(s)) = 293.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:38 INFO Executor: Finished task 44.0 in stage 1.0 (TID 45). 2108 bytes result sent to driver
21/01/31 12:42:38 INFO CoarseGrainedExecutorBackend: Got assigned task 66
21/01/31 12:42:38 INFO Executor: Running task 65.0 in stage 1.0 (TID 66)
21/01/31 12:42:38 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 8724152320-8858370048, partition values: [empty row]
21/01/31 12:42:40 INFO MemoryStore: Will not store rdd_12_50
21/01/31 12:42:40 WARN MemoryStore: Not enough space to cache rdd_12_50 in memory! (computed 136.0 MiB so far)
21/01/31 12:42:40 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 106.6 MiB (scratch space shared across 2 tasks(s)) = 294.5 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:40 INFO Executor: Finished task 50.0 in stage 1.0 (TID 51). 2108 bytes result sent to driver
21/01/31 12:42:40 INFO CoarseGrainedExecutorBackend: Got assigned task 71
21/01/31 12:42:40 INFO Executor: Running task 70.0 in stage 1.0 (TID 71)
21/01/31 12:42:40 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 9395240960-9529458688, partition values: [empty row]
21/01/31 12:42:53 INFO MemoryStore: Will not store rdd_12_70
21/01/31 12:42:53 WARN MemoryStore: Not enough space to cache rdd_12_70 in memory! (computed 70.2 MiB so far)
21/01/31 12:42:53 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 156.5 MiB (scratch space shared across 2 tasks(s)) = 344.4 MiB. Storage limit = 366.3 MiB.
21/01/31 12:42:53 WARN BlockManager: Persisting block rdd_12_70 to disk instead.
21/01/31 12:43:04 INFO MemoryStore: Will not store rdd_12_65
21/01/31 12:43:04 WARN MemoryStore: Not enough space to cache rdd_12_65 in memory! (computed 135.8 MiB so far)
21/01/31 12:43:04 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 102.5 MiB (scratch space shared across 1 tasks(s)) = 290.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:04 WARN BlockManager: Persisting block rdd_12_65 to disk instead.
21/01/31 12:43:10 INFO MemoryStore: Will not store rdd_12_65
21/01/31 12:43:10 WARN MemoryStore: Not enough space to cache rdd_12_65 in memory! (computed 135.8 MiB so far)
21/01/31 12:43:10 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 102.5 MiB (scratch space shared across 1 tasks(s)) = 290.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:10 INFO Executor: Finished task 65.0 in stage 1.0 (TID 66). 2108 bytes result sent to driver
21/01/31 12:43:10 INFO CoarseGrainedExecutorBackend: Got assigned task 85
21/01/31 12:43:10 INFO Executor: Running task 84.0 in stage 1.0 (TID 85)
21/01/31 12:43:10 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 11274289152-11408506880, partition values: [empty row]
21/01/31 12:43:12 INFO MemoryStore: Will not store rdd_12_70
21/01/31 12:43:12 WARN MemoryStore: Not enough space to cache rdd_12_70 in memory! (computed 136.9 MiB so far)
21/01/31 12:43:12 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 108.4 MiB (scratch space shared across 2 tasks(s)) = 296.2 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:12 INFO Executor: Finished task 70.0 in stage 1.0 (TID 71). 2108 bytes result sent to driver
21/01/31 12:43:12 INFO CoarseGrainedExecutorBackend: Got assigned task 92
21/01/31 12:43:12 INFO Executor: Running task 91.0 in stage 1.0 (TID 92)
21/01/31 12:43:12 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 12213813248-12348030976, partition values: [empty row]
21/01/31 12:43:24 INFO MemoryStore: Will not store rdd_12_91
21/01/31 12:43:24 WARN MemoryStore: Not enough space to cache rdd_12_91 in memory! (computed 68.0 MiB so far)
21/01/31 12:43:24 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 153.2 MiB (scratch space shared across 2 tasks(s)) = 341.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:24 WARN BlockManager: Persisting block rdd_12_91 to disk instead.
21/01/31 12:43:36 INFO MemoryStore: Will not store rdd_12_84
21/01/31 12:43:36 WARN MemoryStore: Not enough space to cache rdd_12_84 in memory! (computed 134.0 MiB so far)
21/01/31 12:43:36 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 101.1 MiB (scratch space shared across 1 tasks(s)) = 289.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:36 WARN BlockManager: Persisting block rdd_12_84 to disk instead.
21/01/31 12:43:41 INFO MemoryStore: Will not store rdd_12_84
21/01/31 12:43:41 WARN MemoryStore: Not enough space to cache rdd_12_84 in memory! (computed 134.0 MiB so far)
21/01/31 12:43:41 INFO MemoryStore: Memory use = 187.8 MiB (blocks) + 101.1 MiB (scratch space shared across 1 tasks(s)) = 289.0 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:42 INFO Executor: Finished task 84.0 in stage 1.0 (TID 85). 2108 bytes result sent to driver
21/01/31 12:43:42 INFO CoarseGrainedExecutorBackend: Got assigned task 106
21/01/31 12:43:42 INFO Executor: Running task 105.0 in stage 1.0 (TID 106)
21/01/31 12:43:42 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 14092861440-14227079168, partition values: [empty row]
21/01/31 12:43:43 INFO MemoryStore: Block rdd_12_91 stored as values in memory (estimated size 156.3 MiB, free 22.1 MiB)
21/01/31 12:43:43 INFO Executor: Finished task 91.0 in stage 1.0 (TID 92). 2108 bytes result sent to driver
21/01/31 12:43:43 INFO CoarseGrainedExecutorBackend: Got assigned task 110
21/01/31 12:43:43 INFO Executor: Running task 109.0 in stage 1.0 (TID 110)
21/01/31 12:43:43 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 14629732352-14763950080, partition values: [empty row]
21/01/31 12:43:49 INFO MemoryStore: Will not store rdd_12_105
21/01/31 12:43:49 WARN MemoryStore: Not enough space to cache rdd_12_105 in memory! (computed 35.4 MiB so far)
21/01/31 12:43:49 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.1 MiB (scratch space shared across 2 tasks(s)) = 350.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:49 WARN BlockManager: Persisting block rdd_12_105 to disk instead.
21/01/31 12:43:50 INFO MemoryStore: Will not store rdd_12_109
21/01/31 12:43:50 WARN MemoryStore: Not enough space to cache rdd_12_109 in memory! (computed 34.3 MiB so far)
21/01/31 12:43:50 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 2.9 MiB (scratch space shared across 1 tasks(s)) = 347.1 MiB. Storage limit = 366.3 MiB.
21/01/31 12:43:50 WARN BlockManager: Persisting block rdd_12_109 to disk instead.
21/01/31 12:44:13 INFO MemoryStore: Will not store rdd_12_105
21/01/31 12:44:13 WARN MemoryStore: Not enough space to cache rdd_12_105 in memory! (computed 35.4 MiB so far)
21/01/31 12:44:13 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.4 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:13 INFO Executor: Finished task 105.0 in stage 1.0 (TID 106). 2108 bytes result sent to driver
21/01/31 12:44:13 INFO CoarseGrainedExecutorBackend: Got assigned task 126
21/01/31 12:44:13 INFO Executor: Running task 125.0 in stage 1.0 (TID 126)
21/01/31 12:44:13 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 16777216000-16911433728, partition values: [empty row]
21/01/31 12:44:15 INFO MemoryStore: Will not store rdd_12_109
21/01/31 12:44:15 WARN MemoryStore: Not enough space to cache rdd_12_109 in memory! (computed 34.3 MiB so far)
21/01/31 12:44:15 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 350.5 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:15 INFO Executor: Finished task 109.0 in stage 1.0 (TID 110). 2108 bytes result sent to driver
21/01/31 12:44:15 INFO CoarseGrainedExecutorBackend: Got assigned task 132
21/01/31 12:44:15 INFO Executor: Running task 131.0 in stage 1.0 (TID 132)
21/01/31 12:44:15 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 17582522368-17716740096, partition values: [empty row]
21/01/31 12:44:20 INFO MemoryStore: Will not store rdd_12_125
21/01/31 12:44:20 WARN MemoryStore: Not enough space to cache rdd_12_125 in memory! (computed 35.6 MiB so far)
21/01/31 12:44:20 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 350.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:20 WARN BlockManager: Persisting block rdd_12_125 to disk instead.
21/01/31 12:44:22 INFO MemoryStore: Will not store rdd_12_131
21/01/31 12:44:22 WARN MemoryStore: Not enough space to cache rdd_12_131 in memory! (computed 36.9 MiB so far)
21/01/31 12:44:22 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:22 WARN BlockManager: Persisting block rdd_12_131 to disk instead.
21/01/31 12:44:44 INFO MemoryStore: Will not store rdd_12_125
21/01/31 12:44:44 WARN MemoryStore: Not enough space to cache rdd_12_125 in memory! (computed 35.6 MiB so far)
21/01/31 12:44:44 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 3.4 MiB (scratch space shared across 1 tasks(s)) = 347.5 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:44 INFO Executor: Finished task 125.0 in stage 1.0 (TID 126). 2108 bytes result sent to driver
21/01/31 12:44:44 INFO CoarseGrainedExecutorBackend: Got assigned task 147
21/01/31 12:44:44 INFO Executor: Running task 146.0 in stage 1.0 (TID 147)
21/01/31 12:44:44 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 19595788288-19730006016, partition values: [empty row]
21/01/31 12:44:46 INFO MemoryStore: Will not store rdd_12_131
21/01/31 12:44:46 WARN MemoryStore: Not enough space to cache rdd_12_131 in memory! (computed 36.9 MiB so far)
21/01/31 12:44:46 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 350.4 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:46 INFO Executor: Finished task 131.0 in stage 1.0 (TID 132). 2108 bytes result sent to driver
21/01/31 12:44:46 INFO CoarseGrainedExecutorBackend: Got assigned task 150
21/01/31 12:44:46 INFO Executor: Running task 149.0 in stage 1.0 (TID 150)
21/01/31 12:44:46 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 19998441472-20132659200, partition values: [empty row]
21/01/31 12:44:51 INFO MemoryStore: Will not store rdd_12_146
21/01/31 12:44:51 WARN MemoryStore: Not enough space to cache rdd_12_146 in memory! (computed 36.4 MiB so far)
21/01/31 12:44:51 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 350.5 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:51 WARN BlockManager: Persisting block rdd_12_146 to disk instead.
21/01/31 12:44:53 INFO MemoryStore: Will not store rdd_12_149
21/01/31 12:44:53 WARN MemoryStore: Not enough space to cache rdd_12_149 in memory! (computed 36.7 MiB so far)
21/01/31 12:44:53 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:44:53 WARN BlockManager: Persisting block rdd_12_149 to disk instead.
21/01/31 12:45:16 INFO MemoryStore: Will not store rdd_12_146
21/01/31 12:45:16 WARN MemoryStore: Not enough space to cache rdd_12_146 in memory! (computed 36.4 MiB so far)
21/01/31 12:45:16 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.4 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:16 INFO MemoryStore: Will not store rdd_12_149
21/01/31 12:45:16 WARN MemoryStore: Not enough space to cache rdd_12_149 in memory! (computed 36.7 MiB so far)
21/01/31 12:45:16 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 3.1 MiB (scratch space shared across 1 tasks(s)) = 347.3 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:16 INFO Executor: Finished task 146.0 in stage 1.0 (TID 147). 2108 bytes result sent to driver
21/01/31 12:45:16 INFO CoarseGrainedExecutorBackend: Got assigned task 170
21/01/31 12:45:16 INFO Executor: Running task 169.0 in stage 1.0 (TID 170)
21/01/31 12:45:16 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22682796032-22817013760, partition values: [empty row]
21/01/31 12:45:16 INFO Executor: Finished task 149.0 in stage 1.0 (TID 150). 2108 bytes result sent to driver
21/01/31 12:45:16 INFO CoarseGrainedExecutorBackend: Got assigned task 172
21/01/31 12:45:16 INFO Executor: Running task 171.0 in stage 1.0 (TID 172)
21/01/31 12:45:17 INFO FileScanRDD: Reading File path: file:///Users/kinsho/Desktop/impressionLog.csv, range: 22951231488-23085449216, partition values: [empty row]
21/01/31 12:45:23 INFO MemoryStore: Will not store rdd_12_169
21/01/31 12:45:23 WARN MemoryStore: Not enough space to cache rdd_12_169 in memory! (computed 37.7 MiB so far)
21/01/31 12:45:23 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 350.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:23 WARN BlockManager: Persisting block rdd_12_169 to disk instead.
21/01/31 12:45:23 INFO MemoryStore: Will not store rdd_12_171
21/01/31 12:45:23 WARN MemoryStore: Not enough space to cache rdd_12_171 in memory! (computed 37.7 MiB so far)
21/01/31 12:45:23 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 350.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:23 WARN BlockManager: Persisting block rdd_12_171 to disk instead.
21/01/31 12:45:39 INFO MemoryStore: Will not store rdd_12_171
21/01/31 12:45:39 INFO MemoryStore: Will not store rdd_12_169
21/01/31 12:45:39 WARN MemoryStore: Not enough space to cache rdd_12_169 in memory! (computed 37.7 MiB so far)
21/01/31 12:45:39 WARN MemoryStore: Not enough space to cache rdd_12_171 in memory! (computed 37.7 MiB so far)
21/01/31 12:45:39 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 350.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:39 INFO MemoryStore: Memory use = 344.2 MiB (blocks) + 6.5 MiB (scratch space shared across 2 tasks(s)) = 350.7 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:39 INFO Executor: Finished task 171.0 in stage 1.0 (TID 172). 2108 bytes result sent to driver
21/01/31 12:45:39 INFO Executor: Finished task 169.0 in stage 1.0 (TID 170). 2108 bytes result sent to driver
21/01/31 12:45:41 INFO CoarseGrainedExecutorBackend: Got assigned task 184
21/01/31 12:45:41 INFO CoarseGrainedExecutorBackend: Got assigned task 194
21/01/31 12:45:41 INFO Executor: Running task 6.0 in stage 3.0 (TID 184)
21/01/31 12:45:41 INFO Executor: Running task 16.0 in stage 3.0 (TID 194)
21/01/31 12:45:41 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
21/01/31 12:45:41 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
21/01/31 12:45:41 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 69.4 KiB, free 22.1 MiB)
21/01/31 12:45:41 INFO TorrentBroadcast: Reading broadcast variable 6 took 27 ms
21/01/31 12:45:41 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 194.5 KiB, free 21.9 MiB)
21/01/31 12:45:41 INFO BlockManager: Found block rdd_12_16 locally
21/01/31 12:45:41 INFO CodeGenerator: Code generated in 88.293573 ms
21/01/31 12:45:41 INFO MemoryStore: Will not store rdd_12_6
21/01/31 12:45:41 WARN MemoryStore: Not enough space to cache rdd_12_6 in memory! (computed 36.2 MiB so far)
21/01/31 12:45:41 INFO MemoryStore: Memory use = 344.4 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.5 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:41 INFO BlockManager: Found block rdd_12_6 locally
21/01/31 12:45:41 INFO CodeGenerator: Code generated in 334.81232 ms
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:41 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:42 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:42 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:42 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:42 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:42 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:42 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:45:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:45:42 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 12:45:42 INFO CodecPool: Got brand-new compressor [.snappy]
21/01/31 12:45:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38170915
21/01/31 12:45:54 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000006_184' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000006
21/01/31 12:45:54 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000006_184: Committed
21/01/31 12:45:55 INFO Executor: Finished task 6.0 in stage 3.0 (TID 184). 2504 bytes result sent to driver
21/01/31 12:45:55 INFO CoarseGrainedExecutorBackend: Got assigned task 206
21/01/31 12:45:55 INFO Executor: Running task 23.0 in stage 3.0 (TID 206)
21/01/31 12:45:55 INFO MemoryStore: Will not store rdd_12_23
21/01/31 12:45:55 WARN MemoryStore: Not enough space to cache rdd_12_23 in memory! (computed 35.5 MiB so far)
21/01/31 12:45:55 INFO MemoryStore: Memory use = 344.4 MiB (blocks) + 3.0 MiB (scratch space shared across 1 tasks(s)) = 347.4 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:55 INFO BlockManager: Found block rdd_12_23 locally
21/01/31 12:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:55 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:55 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:55 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:55 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:55 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:55 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:55 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:55 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:55 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:45:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 42854867
21/01/31 12:45:56 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000016_194' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000016
21/01/31 12:45:56 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000016_194: Committed
21/01/31 12:45:56 INFO Executor: Finished task 16.0 in stage 3.0 (TID 194). 2461 bytes result sent to driver
21/01/31 12:45:56 INFO CoarseGrainedExecutorBackend: Got assigned task 214
21/01/31 12:45:56 INFO Executor: Running task 34.0 in stage 3.0 (TID 214)
21/01/31 12:45:56 INFO MemoryStore: Will not store rdd_12_34
21/01/31 12:45:56 WARN MemoryStore: Not enough space to cache rdd_12_34 in memory! (computed 35.3 MiB so far)
21/01/31 12:45:56 INFO MemoryStore: Memory use = 344.4 MiB (blocks) + 6.2 MiB (scratch space shared across 2 tasks(s)) = 350.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:45:56 INFO BlockManager: Found block rdd_12_34 locally
21/01/31 12:45:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:45:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:45:56 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:56 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:45:56 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:45:56 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:45:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:45:56 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:45:56 INFO ParquetOutputFormat: Validation is off
21/01/31 12:45:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:45:56 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:45:56 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:45:56 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:45:56 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:45:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:46:27 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
	at java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:77)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:234)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:232)
	at org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:202)
	at org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33)
	at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:126)
	at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147)
	at org.apache.parquet.column.impl.ColumnWriterV1.accountForValueWritten(ColumnWriterV1.java:106)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:200)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$991/1394912730.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1024/42824093.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1023/403025231.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1020/968421472.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
21/01/31 12:47:05 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.addSlab(CapacityByteArrayOutputStream.java:184)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.write(CapacityByteArrayOutputStream.java:214)
	at org.apache.parquet.bytes.LittleEndianDataOutputStream.write(LittleEndianDataOutputStream.java:76)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.writeTo(Binary.java:297)
	at org.apache.parquet.column.values.plain.PlainValuesWriter.writeBytes(PlainValuesWriter.java:55)
	at org.apache.parquet.column.values.fallback.FallbackValuesWriter.writeBytes(FallbackValuesWriter.java:168)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:198)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$991/1394912730.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1024/42824093.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1023/403025231.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1020/968421472.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
21/01/31 12:47:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38310139
21/01/31 12:47:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 36818633
21/01/31 12:47:15 WARN Utils: Suppressing exception in catch: Java heap space
java.lang.OutOfMemoryError: Java heap space
21/01/31 12:47:15 WARN Utils: Suppressing exception in catch: Java heap space
java.lang.OutOfMemoryError: Java heap space
21/01/31 12:47:15 ERROR Executor: Exception in task 23.0 in stage 3.0 (TID 206)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:77)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:234)
	at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:232)
	at org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:202)
	at org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33)
	at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:126)
	at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147)
	at org.apache.parquet.column.impl.ColumnWriterV1.accountForValueWritten(ColumnWriterV1.java:106)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:200)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$991/1394912730.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1024/42824093.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1023/403025231.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1020/968421472.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
21/01/31 12:47:15 ERROR Executor: Exception in task 34.0 in stage 3.0 (TID 214)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.addSlab(CapacityByteArrayOutputStream.java:184)
	at org.apache.parquet.bytes.CapacityByteArrayOutputStream.write(CapacityByteArrayOutputStream.java:214)
	at org.apache.parquet.bytes.LittleEndianDataOutputStream.write(LittleEndianDataOutputStream.java:76)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.writeTo(Binary.java:297)
	at org.apache.parquet.column.values.plain.PlainValuesWriter.writeBytes(PlainValuesWriter.java:55)
	at org.apache.parquet.column.values.fallback.FallbackValuesWriter.writeBytes(FallbackValuesWriter.java:168)
	at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:198)
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:469)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9(ParquetWriteSupport.scala:190)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$9$adapted(ParquetWriteSupport.scala:188)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$991/1394912730.apply(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1024/42824093.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1023/403025231.apply$mcV$sp(Unknown Source)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1020/968421472.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
21/01/31 12:47:15 INFO CoarseGrainedExecutorBackend: Got assigned task 312
21/01/31 12:47:15 INFO Executor: Running task 44.0 in stage 3.0 (TID 312)
21/01/31 12:47:15 INFO CoarseGrainedExecutorBackend: Got assigned task 313
21/01/31 12:47:15 INFO Executor: Running task 23.1 in stage 3.0 (TID 313)
21/01/31 12:47:15 INFO MemoryStore: Will not store rdd_12_23
21/01/31 12:47:15 WARN MemoryStore: Not enough space to cache rdd_12_23 in memory! (computed 35.5 MiB so far)
21/01/31 12:47:15 INFO MemoryStore: Memory use = 344.4 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 350.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:15 INFO BlockManager: Found block rdd_12_23 locally
21/01/31 12:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:15 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:15 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:47:15 INFO MemoryStore: Will not store rdd_12_44
21/01/31 12:47:15 WARN MemoryStore: Not enough space to cache rdd_12_44 in memory! (computed 36.3 MiB so far)
21/01/31 12:47:15 INFO MemoryStore: Memory use = 344.4 MiB (blocks) + 6.3 MiB (scratch space shared across 2 tasks(s)) = 350.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:47:15 INFO BlockManager: Found block rdd_12_44 locally
21/01/31 12:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:47:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:15 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:47:15 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:47:15 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:47:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:47:15 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:47:15 INFO ParquetOutputFormat: Validation is off
21/01/31 12:47:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:47:15 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:47:15 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:47:15 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:47:15 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:47:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:48:24 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
21/01/31 12:48:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37874508
21/01/31 12:48:26 ERROR Executor: Exception in task 44.0 in stage 3.0 (TID 312)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
21/01/31 12:48:26 INFO CoarseGrainedExecutorBackend: Got assigned task 358
21/01/31 12:48:26 INFO Executor: Running task 44.1 in stage 3.0 (TID 358)
21/01/31 12:48:26 INFO MemoryStore: Will not store rdd_12_44
21/01/31 12:48:26 WARN MemoryStore: Not enough space to cache rdd_12_44 in memory! (computed 36.3 MiB so far)
21/01/31 12:48:26 INFO MemoryStore: Memory use = 344.4 MiB (blocks) + 3.2 MiB (scratch space shared across 1 tasks(s)) = 347.6 MiB. Storage limit = 366.3 MiB.
21/01/31 12:48:26 INFO BlockManager: Found block rdd_12_44 locally
21/01/31 12:48:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:48:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:48:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
21/01/31 12:48:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/01/31 12:48:26 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:48:26 INFO CodecConfig: Compression: SNAPPY
21/01/31 12:48:26 INFO ParquetOutputFormat: Parquet block size to 134217728
21/01/31 12:48:26 INFO ParquetOutputFormat: Parquet page size to 1048576
21/01/31 12:48:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
21/01/31 12:48:26 INFO ParquetOutputFormat: Dictionary is on
21/01/31 12:48:26 INFO ParquetOutputFormat: Validation is off
21/01/31 12:48:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
21/01/31 12:48:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/01/31 12:48:26 INFO ParquetOutputFormat: Page size checking is: estimated
21/01/31 12:48:26 INFO ParquetOutputFormat: Min row count for page size check is: 100
21/01/31 12:48:26 INFO ParquetOutputFormat: Max row count for page size check is: 10000
21/01/31 12:48:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "distributionid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "advertiserid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionhourjst",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributiondatetime",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "requestid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uavalue",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "uacategory",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documentid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenturl",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sitecode",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "documenttitle",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaignid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "campaigntype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "publisherchannelid",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "maxcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "bidcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "contractcpc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "finalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "originalscore",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "distributionorder",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "israndom",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jobchanneltype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "buckettype",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "predictctr",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isGsp",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary distributionid (UTF8);
  optional binary advertiserid (UTF8);
  optional binary distributionhourjst (UTF8);
  optional binary distributiondatetime (UTF8);
  optional binary requestid (UTF8);
  optional binary uid (UTF8);
  optional binary uavalue (UTF8);
  optional binary uacategory (UTF8);
  optional binary documentid (UTF8);
  optional binary documenturl (UTF8);
  optional binary sitecode (UTF8);
  optional binary documenttitle (UTF8);
  optional binary campaignid (UTF8);
  optional binary campaigntype (UTF8);
  optional binary publisherid (UTF8);
  optional binary publisherchannelid (UTF8);
  optional binary maxcpc (UTF8);
  optional binary bidcpc (UTF8);
  optional binary contractcpc (UTF8);
  optional binary finalscore (UTF8);
  optional binary originalscore (UTF8);
  optional binary distributionorder (UTF8);
  optional binary israndom (UTF8);
  optional binary jobchanneltype (UTF8);
  optional binary buckettype (UTF8);
  optional binary predictctr (UTF8);
  optional binary dt (UTF8);
  required boolean isGsp;
}

       
21/01/31 12:48:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 42074924
21/01/31 12:48:27 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000023_313' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000023
21/01/31 12:48:27 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000023_313: Committed
21/01/31 12:48:27 INFO Executor: Finished task 23.1 in stage 3.0 (TID 313). 2504 bytes result sent to driver
21/01/31 12:48:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39055310
21/01/31 12:48:29 INFO FileOutputCommitter: Saved output of task 'attempt_20210131124540_0003_m_000044_358' to file:/Users/kinsho/Desktop/log/_temporary/0/task_20210131124540_0003_m_000044
21/01/31 12:48:29 INFO SparkHadoopMapRedUtil: attempt_20210131124540_0003_m_000044_358: Committed
21/01/31 12:48:29 INFO Executor: Finished task 44.1 in stage 3.0 (TID 358). 2461 bytes result sent to driver
21/01/31 12:49:40 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
